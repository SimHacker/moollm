# Data Flow Skill Prototype
# Rooms as processing nodes, exits as edges, objects as messages

skill: data-flow
description: "Build processing pipelines using rooms and thrown objects"

# Core concept
concept: |
  The filesystem IS the data flow network.
  Rooms are nodes. Exits are edges.
  Thrown objects are messages flowing through the graph.

# Room structure for data flow
room_structure:
  directory:
    ROOM.yml: "Room configuration and processor definition"
    inbox/: "Incoming objects queue (FIFO)"
    outbox/: "Staged objects for batch throwing"
    # exits are sub-directories or named links
    
# Commands
commands:
  THROW:
    syntax: "THROW <object> <exit>"
    effect: "Remove object from room, add to exit destination's inbox"
    example: "THROW report.yml door-output"
    
  INBOX:
    syntax: "INBOX"
    effect: "List items waiting in inbox"
    
  NEXT:
    syntax: "NEXT"
    effect: "Get and remove next item from inbox (FIFO)"
    
  PEEK:
    syntax: "PEEK"
    effect: "View next item without removing"
    
  STAGE:
    syntax: "STAGE <object> <exit>"
    effect: "Add object to outbox for specified exit"
    
  FLUSH:
    syntax: "FLUSH [exit]"
    effect: "Throw all staged objects (optionally for specific exit)"

# Processing loop
processing_loop:
  steps:
    - "NEXT from inbox"
    - "Process item (LLM or script)"
    - "STAGE results to outbox"
    - "FLUSH when batch ready"
    - "Repeat until inbox empty"
    
# Processor types
processor_types:
  script:
    description: "Deterministic processing via shell script"
    config:
      type: script
      command: "python process.py ${input}"
      
  llm:
    description: "Semantic processing via LLM prompt"
    config:
      type: llm
      prompt: |
        Analyze this input and produce structured output.
        
  hybrid:
    description: "Script transforms, LLM interprets"
    config:
      type: hybrid
      pre_process: "extract.py ${input}"
      llm_prompt: "Analyze extracted data"
      post_process: "format.py ${output}"

# Example pipeline
example_pipeline:
  uploads:
    type: source
    exits: [door-parser]
    
  parser:
    type: script
    processor: "python parse.py"
    exits: [door-analyzer, door-errors]
    
  analyzer:
    type: llm
    prompt: "Summarize and extract entities"
    exits: [door-output, door-errors]
    
  output:
    type: sink
    # Results accumulate here
    
  errors:
    type: sink
    # Errors routed here for review

# Kilroy mapping
kilroy_mapping:
  room: node
  exit: edge
  throw: message
  inbox: input_queue
  script_processor: deterministic_module
  llm_processor: llm_node
