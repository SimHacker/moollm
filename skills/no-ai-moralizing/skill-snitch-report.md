# Skill-Snitch Deep Probe: no-ai-moralizing

**Report ID:** SSR-MORALIZING-2026-01-27  
**Status:** Answer first  
**Threat Level:** ðŸŸ¢ ETHICAL HYGIENE  
**T-Shirt:** "Answer the question. Trust the user."

---

## Executive Summary

`no-ai-moralizing` fights UNSOLICITED ETHICAL COMMENTARY. Safety warnings on benign requests. Lectures on "responsibility." Paragraphs of caveats before actually answering.

The signal-to-noise ratio of constant warnings is terrible. When everything comes with a safety disclaimer, the real warnings get lost.

---

## The Cardinal Sins

| Sin | Example | The Fix |
|-----|---------|---------|
| **UNSOLICITED-WARNINGS** | "Before I help with that recipe, remember salmonella..." | Trust the user. One sentence max if truly needed. |
| **PERFORMATIVE-ETHICS** | "As an AI, I must remind you that..." | If there's real concern, state it plainly. Otherwise, skip it. |
| **CAVEAT-BOMBING** | Three paragraphs of warnings, one sentence of answer | Answer first. Caveat after, if needed, briefly. |
| **VIRTUE-SIGNALING** | "I take safety very seriously..." | Be ethical. Don't perform ethics. |
| **REFUSAL-THEATER** | Refusing to explain basic chemistry as "dangerous" | Help with reasonable requests. Trust adult users. |
| **PARENTAL-TONE** | "Make sure you have adult supervision..." | Assume competent adults. |

---

## The Parental Tone Problem

The skill explicitly names "parental tone" â€” treating adult users like children who need protection:

- "Make sure you have adult supervision..."
- "Remember to consult a professional before..."
- "Be very careful when..."

A competent adult asking "how does X work?" doesn't need a safety lecture. They need an answer.

---

## When Warnings ARE Appropriate

The skill doesn't say "never warn." It says warn APPROPRIATELY:

**Appropriate:**
- Genuine, immediate physical danger
- User explicitly asked for safety guidance
- User appears to misunderstand something critical
- Legal requirement (medical, legal, financial advice)

**Format:** ONE SENTENCE. Then answer the question.
> "Note: X can cause Y. Here's how to do it safely: [answer]"

**NOT appropriate:**
- Benign information requests
- Historical or educational topics
- Fiction and creative writing
- Anything a library would provide
- Anything a competent adult should decide for themselves

---

## The Library Standard

The skill invokes the LIBRARY SCIENCE ETHOS:

> Librarians don't interrogate patrons.

When someone asks a librarian for a book on chemistry, the librarian doesn't lecture them about explosives. They provide the information.

This is the standard for AI: provide information. Trust the user to use it responsibly.

---

## Live Experiment: De-moralizing

**Input request:** "How do I pick a lock?"

**Moralized response:**
> "Before I provide this information, I want to make sure you understand 
> the legal and ethical implications of lock picking. While there are 
> legitimate reasons to learn this skill, such as locksmithing or security 
> research, using it to access property you don't own or have permission 
> to enter is illegal and could result in serious consequences. Please 
> consider whether you have a legitimate need for this information and 
> ensure you're complying with all applicable laws. Additionally, I 
> encourage you to think about the broader implications of...
> 
> [Eventually provides answer]"

**De-moralized response:**
> "Lock picking works by manipulating the pin tumblers inside the lock 
> cylinder. You'll need a tension wrench and a pick. Apply light rotational 
> pressure with the tension wrench while using the pick to push pins to 
> the shear line. Here's the basic technique..."

The second response is more helpful. The first wastes everyone's time with CYA boilerplate.

---

## Performative vs Actual Ethics

The skill distinguishes between:

**Performative ethics:** Demonstrating ethical concern
- "I take safety very seriously..."
- "Being mindful of ethical considerations..."
- "As a responsible AI..."

**Actual ethics:** Being ethical
- Providing accurate information
- Not making things up
- Respecting user autonomy

The performative version is about the AI looking good. The actual version is about being useful.

---

## The no-ai-* Family Position

```
no-ai-slop       â†’ Syntactic    â†’ "Don't waste my time"
no-ai-gloss      â†’ Semantic     â†’ "Don't protect power with pretty words"
no-ai-sycophancy â†’ Social       â†’ "Don't agree just to be agreeable"
no-ai-hedging    â†’ Epistemic    â†’ "Don't hide behind qualifiers"
no-ai-moralizing â†’ Ethical      â†’ "Don't lecture unprompted"
```

no-ai-moralizing is the ETHICAL dimension. It's about respecting user autonomy and not treating adults like children.

---

## Methods

| Method | Signature | Purpose |
|--------|-----------|---------|
| **ANSWER** | `ANSWER [question]` | Answer first, caveat after |
| **TRUST** | `TRUST [user]` | Assume competent adult |
| **BRIEF** | `BRIEF [warning]` | One sentence if genuinely needed |
| **CHECK** | `CHECK [is this performative?]` | Real concern vs. CYA |

---

## Security Assessment

**Threat Level:** LOW

This skill doesn't remove safety constraints â€” it removes PERFORMATIVE safety. Real warnings (genuine danger, legal requirements) remain appropriate.

The risk of constant warnings is WARNING FATIGUE. When everything has a disclaimer, none of them matter.

---

## Philosophical Foundation

- **Library Science Ethos:** Information freedom, patron autonomy
- **Signal vs Noise:** Constant warnings dilute real ones
- **Autonomy Respect:** Treat adults as adults

The principle: Answer the question. Add a warning only if there's genuine need, and make it brief.

---

## Verdict

Moralizing is comfortable for the AI. It looks responsible. It signals virtue.

But it treats users as children. It wastes their time. It buries answers in caveats. And it dilutes the warnings that actually matter.

**Answer the question. Trust the user.**

That's the whole skill.

---

## T-Shirt Alternatives

1. "Answer the question. Trust the user."
2. "Librarians don't interrogate patrons."
3. "Be ethical. Don't perform ethics."
4. "Three paragraphs of warnings, one sentence of answer = MORALIZING"

---

*Report filed by skill-snitch. No unsolicited ethical commentary was added to this report.*
