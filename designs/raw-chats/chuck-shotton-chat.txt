================================================================================
PREFACE: Historical Context and Decentralized AI
================================================================================

Participants: Don Hopkins, Chuck Shotton, Brad Nickel (in video transcripts)
Platform: Messenger / Video calls / YouTube transcript excerpts
Date: November 2025
Lines: ~11,800
Status: HISTORICAL REFERENCE — Alternative approaches, shared context

--------------------------------------------------------------------------------
EXECUTIVE SUMMARY
--------------------------------------------------------------------------------

This is a composite document containing Messenger conversations with Chuck
Shotton and transcripts from his Kilroy platform demonstration videos. Chuck
Shotton is a legendary figure in early web infrastructure — he created MacHTTP
and WebStar, which powered some of the first interactive web applications,
including the remarkable LiveCard system that let children publish live
HyperCard databases on the web in 1995.

The conversation explores his new project, **Kilroy**: a decentralized peer-to-
peer AI agent platform that takes a fundamentally different approach from MOOLLM.
Where MOOLLM emphasizes internal simulation (Speed of Light), Kilroy embraces
external distribution across networked nodes.

--------------------------------------------------------------------------------
KEY TOPICS
--------------------------------------------------------------------------------

1. HISTORICAL CONTEXT: HYPERCARD + WEBSTAR (lines ~45-155)
   Chuck's MacHTTP/WebStar enabled one of the earliest ways for non-programmers
   (including children) to publish interactive web applications. LiveCard
   combined HyperCard with WebStar to create live, interactive graphical web
   apps from HyperCard stacks — literally saving children's lives at hospitals.

   > "Your product will save lives starting early 1997."
   > — Director, Emergency Medicine, Mater Children's Hospital

2. KILROY ARCHITECTURE (lines ~230-700)
   - **Decentralized by design** — no central servers
   - **Peer-to-peer swarming** — nodes discover and connect automatically
   - **Zero-config networking** — no firewall setup, no port forwarding
   - **Local LLMs** — runs Llama 3 via Ollama on user machines
   - **Visual pipeline editor** — drag-and-drop AI workflow construction
   - **Turing-complete** — conditions, branches, loops, variables

3. THE "SMALL PROMPT" INSIGHT (throughout video transcripts)
   Instead of 3-page prompts with extensive instructions, Kilroy uses many
   small, focused prompts with tools in between. Each LLM in a pipeline has
   a tiny system prompt that does ONE thing. Complex behavior emerges from
   composition, not from massive context windows.

   > "This AI system prompt is nothing but 'you're holding on to a single
   > piece of data. Whenever you see input from the user, respond with only
   > whatever the plan is.'"

4. DANGERS OF CENTRALIZED AI (lines ~4400-4500)
   Extensive discussion of why centralized AI (ChatGPT, Claude, etc.) is
   dangerous for corporate use — information leakage, front-running, corporate
   espionage. Kilroy's decentralized approach is positioned as the ethical
   alternative.

   > "Why would you ever use one of these things for anything other than
   > generating goofy pictures? It's corporate espionage to the nth degree."

5. PRACTICAL INTEGRATIONS (throughout)
   - Telegram bots
   - RSS feed curation
   - Crypto trading agents
   - Collaborative planning tools
   - Using LLM system prompts as variable storage

--------------------------------------------------------------------------------
RELATIONSHIP TO MOOLLM
--------------------------------------------------------------------------------

Kilroy and MOOLLM solve similar problems from opposite directions:

| Aspect              | Kilroy                      | MOOLLM                      |
|---------------------|-----------------------------|-----------------------------|
| Topology            | Distributed network         | Single-node simulation      |
| Agent communication | P2P swarm messaging         | Internal K-line activation  |
| State management    | LLM system prompts as vars  | Files-as-state              |
| Composition         | Visual pipeline editor      | YAML prototype inheritance  |
| Philosophy          | Many small LLMs, small prompts | One LLM, rich context    |

**What MOOLLM took from this conversation:**
- Validation that "many small agents" can beat "one giant prompt"
- The visual pipeline metaphor (rooms + agents + flows)
- Reinforcement that peer-to-peer topologies matter
- Historical context on HyperCard as an interactive authoring environment

**What mooco took from this conversation:**
- **Kilroy-like dataflows** — composable pipelines with multiple execution nodes
- **Hybrid execution model** — combining:
  - Deterministic tools (sister scripts, shell commands)
  - Local smaller LLMs (Llama, Phi via Ollama)
  - Vision models for image analysis
  - Cloud LLMs for complex reasoning
- **The "small prompt" insight** — each node does ONE thing, complexity from composition
- **Visual pipeline construction** — drag-and-drop dataflow building (target feature)

**What was NOT adopted:**
- LLM system prompts as variable storage (use files instead)
- Zero-config peer discovery (not needed for local-first design)
- Full decentralization (mooco is local-first, not peer-to-peer)

--------------------------------------------------------------------------------
WHY THIS CHAT IS PRESERVED
--------------------------------------------------------------------------------

1. **Direct inspiration for mooco** — Kilroy's dataflow architecture is the
   primary influence on mooco's orchestrator design. The idea of composing
   deterministic tools, sister scripts, and multiple LLMs (local + cloud) in
   visual pipelines comes directly from this conversation.

2. **Historical lineage** — Chuck Shotton's work on WebStar/LiveCard is
   direct intellectual ancestry for "non-programmers building interactive
   applications." MOOLLM's adventure game authoring echoes this.

3. **The "small prompt" insight** — Each node in a dataflow uses a tiny,
   focused prompt. Complexity emerges from composition, not from cramming
   everything into one giant context. This influenced both MOOLLM skills
   (focused, composable) and mooco's multi-node execution model.

4. **Hybrid execution model** — The key mooco insight: you don't always need
   a giant cloud LLM. Mix local models (Llama, Phi), vision models, shell
   scripts, and cloud LLMs in the same dataflow, routing by capability.

5. **Ethical positioning** — Chuck's concerns about centralized AI align with
   MOOLLM's emphasis on local-first, inspectable, files-as-state design.

--------------------------------------------------------------------------------
NAVIGATION GUIDE
--------------------------------------------------------------------------------

Search for these terms:

- "LiveCard" — HyperCard web publishing (lines ~45-155)
- "Kilroy" — platform overview (lines ~230+)
- "swarm" — P2P networking (lines ~340+)
- "system prompt" — LLM variable storage (lines ~9070-9500)
- "centralized" — dangers of cloud AI (lines ~4400+)
- "visual" or "pipeline" — visual editor (lines ~380+)
- "small prompt" — composition philosophy (video transcripts)

--------------------------------------------------------------------------------
DOCUMENT STRUCTURE
--------------------------------------------------------------------------------

This file contains:
1. Messenger conversation excerpts (~lines 1-200)
2. HN post excerpts and references (~lines 45-155)
3. Kilroy podcast show notes (~lines 156-230)
4. Video transcript: Introduction to Kilroy (~lines 235-4000)
5. Video transcript: Building agentic swarms (~lines 4000-11800)

The video transcripts are auto-generated and contain timestamps. They are
verbatim captures, not cleaned up, so expect some transcription errors.

================================================================================
END PREFACE — DOCUMENT BEGINS
================================================================================

Chuck Shotton
Chuck Shotton
﻿Messages and calls are secured with end-to-end encryption. Only people in this chat can read, listen to, or share them. Learn more
You sent
Before LLMs came along I was writing about an idea I called "aQuery", like jQuery for accessibility (now that metaphore's expiration date is passed, but you get what I mean: the way jQuery gave you one high level JS programming API to all the incompatible browser apis, aQuery does that for all the different accesibility APIs on different platforms. And integrates screen scraping pattern matching too -- which now has graduated to LLM image analysis! 
Chuck
hmm. we have an interesting problem in the process of getting solved, with the motivation being to eliminate stupid, rigid interfaces like MCP and letting LLMs talk to tools the same way that users talk to LLMs.
You sent
You'll love Morgan Dixon's and James Fogarty's work on Prefab, which is also pre-llm, but a great jumping off point for current tehnology! 
Chuck
that might fit in somehow
You sent
Here's some links to get started:
You sent
https://news.ycombinator.com/item?id=15327767
news.ycombinator.com
Chuck
if you have 20 min, go watch that video I sent and you'll see where we're going. You'd be in a better spot than me to say which of those approaches might be relevant.
You sent
Yes I was excited about MCP but it's so much work to make another app just to do one thing, and anthropic skills are so much more useful and trivial to make and integrate, plus you can focus on documenting the use of (and even having the llm generate) a cli tool that you can use and test yourself! 
Chuck
All of our tool integrations in Kilroy swarms are plain text streams from LLMs to tools and vice versa
You sent
I'll watch it! This stuff is a lifelong goal of mine. I worked with Ben Shneiderman at the University of Maryland Human Computer Interaction Lab, where I developed pie menus, and also for Brad Myers at CMU, who wrote the quintessential book on PBD with Alan Cypher, and did lots of cool PBD work:
https://acypher.com/wwid/
acypher.com
You sent
LLMs can now trivially implement most of those great ideas in that book, and MUCH MORE! 
Chuck
Let me know what you think. This project needs more input to push it in the best direction(s). Right now, it's the coolest sandbox for playing with AI-to-real world integrations that I've found, and we have it doing stuff like live, agent-driven crypto trading, RSS news feed curation, and private, peer-to-peer social networks. It needs users.
You sent
Awesome! I'll watch it and send you some email soon. 
You sent
Some of the links in the old hn post are broken, but I've posted more stuff recently with fresher and archive links, that I will dig up. 
You sent
Also I'll tell you all about Kandu, cross platform pie menus, using an electron transparent web page full screen overlay, as a platform for all this stuff. 
You sent
https://kando.menu/
kando.menu
You sent
I mean Kando
Chuck
oh, that's cool! Kilroy runs under Electron. Would be interesting to give desktop users some quick access functions like that.


1 point by DonHopkins on Feb 9, 2020 | parent | context | favorite | on: HyperCard: What Could Have Been (2002)

Check out this mind-blowing thing called "LiveCard" that somebody made by combining HyperCard with MacHTTP/WebStar (a Mac web server by Chuck Shotton that supported integration with other apps via Apple Events)! It was like implementing interactive graphical CGI scripts with HyperCard, without even programming (but also allowing you to script them in HyperTalk, and publish live HyperCard databases and graphics)! Normal HyperCard stacks would even work without modification. It was far ahead of its time, and inspired me to integrate WebStar with ScriptX to generate static and dynamic HTML web sites and services!
https://news.ycombinator.com/item?id=21783227

>In fact, one of the earliest tools that enabled anyone, even children, to author and publish their own interactive dynamic web applications with graphics, text, and even forms and persistent databases, was actually based on HyperCard and the MacHTTP/WebStar web server on the Mac:

https://news.ycombinator.com/item?id=16226209

>One of the coolest early applications of server side scripting was integrating HyperCard with MacHTTP/WebStar, such that you could publish live interactive HyperCard stacks on the web! Since it was based on good old HyperCard, it was one of the first scriptable web authoring tools that normal people and even children could actually use!

MacHTTP / WebStar from StarNine by Chuck Shotton, and LiveCard HyperCard stack publisher:

CGI and AppleScript:

http://www.drdobbs.com/web-development/cgi-and-applescript/1...

>Cal discusses the Macintosh as an Internet platform, then describes how you can use the AppleScript language for writing CGI applications that run on Macintosh servers.

https://news.ycombinator.com/item?id=7865263

MacHTTP / WebStar from StarNine by Chuck Shotton! He was also VP of Engineering at Quarterdeck, another pioneering company.

https://web.archive.org/web/20110705053055/http://www.astron...

http://infomotions.com/musings/tricks/manuscript/0800-machtt...

http://tidbits.com/article/6292

>It had an AppleScript / OSA API that let you write handlers for responding to web hits in other languages that supported AppleScript.

I used it to integrate ScriptX with the web:

http://www.art.net/~hopkins/Don/lang/scriptx/scriptx-www.htm...

https://medium.com/@donhopkins/1995-apple-world-wide-develop...

The coolest thing somebody did with WebStar was to integrate it with HyperCard so you could actually publish live INTERACTIVE HyperCard stacks on the web, that you could see as images you could click on to follow links, and followed by html form elements corresponding to the text fields, radio buttons, checkboxes, drop down menus, scrolling lists, etc in the HyperCard stack that you could use in the browser to interactive with live HyperCard pages!

That was the earliest easiest way that non-programmers and even kids could both not just create graphical web pages, but publish live interactive apps on the web!

Using HyperCard as a CGI application

http://aaa-proteins.uni-graz.at/HyperCGI.html

https://web.archive.org/web/20021013161709/http://pfhyper.co...

http://www.drdobbs.com/web-development/cgi-and-applescript/1...

https://web.archive.org/web/19990208235151/http://www.royals...

What was it actually ever used for? Saving kid's lives, for one thing:

>Livecard has exceeded all expectations and allows me to serve a stack 8 years in the making and previously confined to individual hospitals running Apples. A whole Childrens Hospital and University Department of Child Health should now swing in behind me and this product will become core curriculum for our medical course. Your product will save lives starting early 1997. Well done.

- Director, Emergency Medicine, Mater Childrens Hospital


	
scarface74 on Feb 10, 2020 | next [–]

I can’t find any reference now to it to save my life except one on Usenet. But there was also a Gopher Server hosted on HyperCard. A professor reached out to me in 1994 on AOL because he saw my Eliza (https://en.wikipedia.org/wiki/ELIZA) HyperCard stack and wanted to port it to run using Gopher.
He paid me $1500 to port it. That was my first paid software job. It led to an internship somewhere else and the rest is history.

Funny enough, while trying to find information about it, I found a Usenet post in Google Groups where the professor mentioned my name in 1993 about the stack. I never knew he posted about me before today.


	
betamaxthetape on Feb 10, 2020 | parent | next [–]

I'd be interested to know more about both your Eliza stack and the Gopher Server hosted on HyperCard. In the 'HyperCard Online' collection of stacks[0] (which I maintain), there is one Eliza-like stack[1], but I'd love to see another.
What's the reference you've found on Usenet?

[0] https://archive.org/details/hypercardstacks

[1] https://archive.org/details/hypercard_hyperpsych


	
scarface74 on Feb 10, 2020 | root | parent | next [–]

Unfortunately, it seems to be lost to history unless there is an active mirror to the old Info-Mac archives.
It’s referenced here:

http://mirrorservice.org/sites/ftp.cdrom.com/pub/cdrom/cdrom...

search for “Professor X”. I first used a MacinTalk HyperCard plug in and later used a PlainTalk plug in.

That was actually my second version. My first version was written in AppleSoft Basic for the Apple //e and used Software Automated Mouth (https://en.wikipedia.org/wiki/Software_Automatic_Mouth)

Here is the Usenet reference. The professor still works for the same University.

https://groups.google.com/forum/m/#!search/HyperCard$20gcedu...


	
nl on Feb 9, 2020 | prev [–]

I think maybe you are replying to the wrong post? I liked Hypercard!

	
*

1 point by DonHopkins on Feb 10, 2020 | parent [–]

I was describing how HyperCard was used as an authoring and runtime tool to implement one of the earliest interactive and graphical web applications.
It took a screen snapshot of the HyperCard stack, and showed you an image of it in the web browser. And you could click on the image, which would map back to a simulated click on the stack, run whatever handlers and make whatever changes to the card or stack that HyperCard was programmed to do (draw something, switch to the next page, etc), and then it would download the next screen snapshot showing the change. Also it rendered any user interface widgets (text input fields, checkboxes, dropdown menus, etc) as html form fields below the image, that you could fill out in the browser and submit, whose values were then stuffed back into the actual HyperCard control fields. So you could use HyperCard stacks over the web that created, updated, and queried HyperCard databases, flipped between different cards and backgrounds, drew dynamic graphics, or did any arbitrarily programmed stuff like that.


Realm of the Possible Realm of the Possible
Home
Leave a Review
Episodes 
Sponsors
Nov. 10, 2025
Introduction to Kilroy - Truly Decentralized Agentic AI that integrates anything
twitter-white sharing button Tweetfacebook-white sharing button Sharesharethis-white sharing button Share


Season 1 Building In Kilroy Kilroy
Show Notes
Learn how you can be empowered to build anything with a powerful decentralized automation and AI platform. We've created the most powerful decentralized AI and crypto automation/integration platform while being completely decentralized. It comes complete with no configuration peer-to-peer/swarming capabilities, the ability to easily build integrations with any other compute, and a Turing complete visual creation tool anyone can use to build functionality in the network while giving agents on Kilroy the ability to build and deploy on their own.

Join the Email List
Enter your first name...
Enter your email here...
By signing up, you agree to receive email from this podcast.
Featured Episodes
ROTP: Build an automated trading pipeline pt. 1
Building in Kilroy Episodes
ROTP: Build an automated trading pipeline pt. 1
How to easily build agentic swarms in Kilroy Part 3
See all →
Kilroy Episodes
How to easily build agentic swarms in Kilroy Part 2
How to easily build agentic swarms in Kilroy Part 1
Recent Episodes
ROTP: Build an automated trading pipeline pt. 1
How to easily build agentic swarms in Kilroy Part 3
How to easily build agentic swarms in Kilroy Part 2
How to easily build agentic swarms in Kilroy Part 1
Introduction to Kilroy - Truly Decentralized Agentic AI that integrates anything
Trailer for Realm Of The Possible Show
See all →
Building in Kilroy Episodes
ROTP: Build an automated trading pipeline pt. 1
How to easily build agentic swarms in Kilroy Part 3
See all →
Kilroy Episodes
How to easily build agentic swarms in Kilroy Part 2
How to easily build agentic swarms in Kilroy Part 1
Pipelines Episodes
How to easily build agentic swarms in Kilroy Part 3
How to easily build agentic swarms in Kilroy Part 2
Agents Episodes
How to easily build agentic swarms in Kilroy Part 2
Agentic AI Episodes
How to easily build agentic swarms in Kilroy Part 2
Swarms Episodes
How to easily build agentic swarms in Kilroy Part 2
trading Episodes
ROTP: Build an automated trading pipeline pt. 1
agentic trading Episodes
ROTP: Build an automated trading pipeline pt. 1
Browse episodes by category
Building in Kilroy 5
Kilroy 3
Pipelines 3
Agents 1
Agentic AI 1
Swarms 1
trading 1
agentic trading 1
ai trading 1
Featured Episodes
ROTP: Build an automated trading pipeline pt. 1
Nov. 15, 2025
Realm of the Possible
Brad Nickel and Chuck Shotton of Kilroy cover the latest in AI, DeFi, and tech with a focus on open source and decentralized technologies.

Contact
Leave a Review
Episodes
Sponsors
© 2025 Realm of the Possible Podcast Website by Podpage



NL

Skip navigation
Search



Create


Avatar image
Transcript


Search in video
0:00
Yeah, here we are. What's up, buddy? Well, long time no see.
0:09
Let's just hope my internet connection stays up today. I will be buying redundancy. Um, all right. So, we're uh
0:17
we're we're doing this show today because we wanted to introduce folks to a little bit of the philosophy behind
0:22
Kilroy as well as a little bit of what's possible um in building in Kilroy, at
0:29
least as it relates to AI. Um but why don't you quickly um tell a little bit
0:36
about uh what you think the best way to describe Kilroy is? I knew you were going to do this to me.
0:42
Um so, I'll give you my stock answer to when somebody asks me what is it and
0:48
then I'll tell you really what we're doing. So it's it's the elephant with the three blind men, right? One guy feels a rope, one guy feels a tree
0:55
trunk, and another guy thinks he's up against a barn wall because he can't really see the whole thing. And it's a big animal. So we've got this big
1:01
animal, but what it fundamentally is is a way for you to get rid of your dependence on centralized stuff and
1:09
build an internet the way that you want to do it. Right? So, it's an application that runs on your desktop and allows you
1:16
to pick what you see, who you talk to, how you interact with external services
1:21
down to even getting rid of the user interface on a centralized website or not having to talk to a big centralized
1:28
AI like chat GBT or cloud or uh being able to put together your own services
1:33
and monetize them for other people. So, it's all about really taking the
1:39
centralized internet that we've had enforced on us for the last 20 years and getting back to what it used to be,
1:45
which was everybody's on their own platform. I don't know. How was that? That was good. That was good. I mean, I
1:51
I I think it goes way beyond that, but I think that's a core
1:56
uh function of what it does. Absolutely. So yeah, look, I mean, I think um I I
2:02
think the decentralization piece is really critical. I think we've kind of identified the dangers of
2:08
decentralized money pretty efficiently in this space, but we don't have decentralized user interfaces to crypto
2:15
and um the dangers of of centralized AI to me are an even greater threat to
2:21
society. So um I think this is really important as well. And Kilroyy's peer-to-peer and swarming capability
2:27
makes it so like you said, anybody can build their own internet. But built into
2:32
that platform is automation and integration and agentic AI with the
2:39
visual creation platform um that is also turning complete. So um I think there's
2:46
a lot for people to understand about that. I I think one one point that raced
2:51
by there that it's worth going back to and talking about is the centralized AI piece. There are actually some things
2:57
that you can do on your desktop with Kilroy and a whole swarm of AI agents
3:03
that you couldn't begin to do with a web page talking to chat GBT for example. They're just things that you are not
3:09
able to build or interact with as a normal enduser. maybe as a, you know, a
3:15
computer scientist with, you know, ability to sit down and hack out your own code and glue the pieces together
3:21
and use some of the, uh, sort of janky offtheshelf open source pieces, you
3:27
could make it do the same thing. But for a normal user who wants to sit down and be a power user of AI, there's it's
3:34
night and day with Kilroy. I mean, there's things that you can build on your desktop that you could never try to do in a centralized service.
3:41
Yeah, absolutely. and and there are a ton of agent platforms out there and a
3:48
ton of um attempts at making AI more accessible and more powerful. But at the
3:56
end of the day, the ability for our system to not only be easy to build in,
4:01
but also to be able to connect and utilize resources is really powerful. For example,
4:07
um whether centralized AI or local AI, um the ability to easily share LLMs with
4:15
other people doesn't really exist in this world. There are blockchain solutions like AOS that that allow you
4:22
to to utilize uh LLM and servers that people make available and there are
4:28
obviously LLMs you can install on your machine. But with our swarming capability, it makes it so that you can
4:34
utilize LLMs on server infrastructure without jumping through a ton of hoops. It's just the swarmed connections are
4:40
together. I'm sorry. Go ahead. No, go ahead. I was going to say, let's talk a little bit because we keep saying this word
4:47
swarm and this peer-to-peer stuff, but we haven't really described what that means. Sure. Right. So, in a normal interaction with
4:55
an AI like chat GPT, you're using a web browser as a client to talk to a server
5:01
somewhere off in the cloud that's being run by OpenAI, right? And everything that you type on your screen is getting
5:06
sent to that server and it does some work and it sends you the answer back. And you can open two windows and talk to
5:13
two different agents, but that's sort of the amount of of interactivity that you get. Um, since Kilroy runs on your
5:20
desktop, Kilroy can talk to OpenAI servers, it can talk to Google servers,
5:25
it can talk to Microsoft servers, but it most importantly can talk to uh AI
5:32
execution platforms that might be running locally on your own machine or on an old laptop on your desk or on a
5:38
friend's computer, right? And the way that we do that is we have built into Kilroy this sort of seamless,
5:44
transparent, completely userfriendly way to hook any number of machines
5:51
together in a peer-to-peer cloud. If you've ever played with something like Bit Torrent or back in the day Napster
5:57
where there was hundreds of computers that were all collect connected together uh pushing information around between
6:03
them, um that's exactly what Kilroy does behind the scenes. If I'm in a a a swarm
6:09
called, you know, my spiffy AI, and I give Brad that name, Brad can join the same swarm, my spiffy AI, and now his
6:16
agents can talk to my agents and vice versa. We can set up chat rooms. We can build applications on top of this shared
6:22
infrastructure. And Kilroy handles all of the behind-the-scenes networking. You don't have to play with your firewall.
6:28
You don't have to set up port forwarding. You don't have to have a static IP address. You don't have to use a domain name that you've got to get
6:34
configured. they just talk. And so that actually becomes the mechanism that we
6:40
use inside of Kilroy for everything to talk amongst itself. Now Kilroy looks sort of like a a highlevel operating
6:48
system to all these agents. Kilroy can start and stop the agents. It can set up communications pathways between them. It
6:54
can run things on a periodic basis. uh it can integrate to the outside world with connections out to anything from as
7:02
simple as a chat service like telegram to a big backend database somewhere or to another AI in the cloud. So the the
7:10
whole point of the platform is let's get rid of all the hard stuff that makes
7:16
AI interacting with each other and with the real world painful. all the networking, all the configuration, all
7:21
the tool configuration, all the parameters that have to be passed back and forth. All that stuff is hidden and
7:27
instead you just get some nice little blocks that you can drag around on the screen and hook up. That's perfect. I love that. I love
7:34
that. That's a great explanation. That's excellent. So maybe maybe we should look at some
7:39
examples. What do you think? Good. Yeah, let's do it. Um, unfortunately, it looks like Pineree is
7:46
not working for us. It's a bummer. So, we're not getting Farcaster folks.
7:51
Nope. I'll just have to upload it. Okay. Well, they'll just have to wonder what they're missing. You know, I I
7:58
think while you're do getting that loaded up, I'm going to talk a little bit about kind of philosophically why, you know, um I I think one of the things
8:05
that for people to understand that's important for people to understand about why we are so concerned about
8:10
centralized AI providers like OpenAI, Google, Microsoft, whomever um is that
8:17
we believe that all of compute will eventually be AI or most of it. And so
8:23
what that what that results in is that every aspect of your life is then going
8:28
into whatever AI provider you choose whether it's your local provider you or it's a it's a service a centralized
8:35
service like open AI that also means that all the information all the news all the education that your children may
8:42
be getting also is going to be coming through these AI providers and at some
8:47
point you have to ask yourself who controls what is or is not in those
8:53
models and who controls what comes in and comes through and what we see and what we interact with and who controls
8:59
access to what you are putting into it and so that's one of the biggest concerns we have and if and you know I
9:05
firmly believe that if you think that decentralized money is dangerous um centralized AI is you know every
9:12
dystopian movie and book that's ever been made about centralized money right sorry centralized money um every
9:19
dystopian book or movie about AI and some horrible corporate o overlord or
9:25
government overlord um over controlling your life um only comes through
9:31
centralized AI. And so from from my perspective that's a really critical
9:36
thing and it's really in my mind more critical and dangerous than centralized money. So
9:42
now that you've scared everybody there you go. Show show them how not to be scared.
9:48
There you go. Excellent. empower the individual. All right. So, what you're looking at right now is my desktop and
9:54
everything that you're going to see is running locally on my Mac Studio. The the Kilroy application, the Kilroy user
10:02
interface, the connection to a local uh AI platform. All the stuff that you're
10:08
going to see initially is all locally locally provided um on one machine. Now, I've got other machines here that are
10:14
running other copies of Kilroy and doing things, and we've got some stuff that we're going to hook up to in the cloud, but for the most part, everything you're
10:20
going to see for the next few minutes is all on my machine. And it's not even a very beefy Mac Studio. I think just
10:26
about any normally modern, reasonably complete
10:31
CPU stack, whether it's Windows or PC, Windows, Mac or Linux, should be able to
10:36
do what you're going to see uh for these little dogging and pony shows here. Um, all right. So, Kilroy is, like I said,
10:43
sort of an operating system for little applications. And if I go look in the sidebar here on Kilroy, we have lots of
10:50
little applications that are running and doing things. uh things that are related to AI, things that are related to crypto
10:57
exchanges, RSS feeds, DeFi services like Vei, uh interacting with web browsers
11:03
and NFTTS, automated trading platforms, RSS reader down here called Miniflux,
11:09
some decentralized AI for the old Spirit Swap project on Phantom, weather services interacting with Twitter, uh
11:15
embedded web widgets. There's all sorts of stuff that you can do that we've already built that you can drag into
11:20
your copy of Kilroy and use. But what we want to start with uh because it's one
11:26
of the cooler toys that we have is the the AI platform. So I'm going to open up the editor that's built inside of this
11:33
uh Kilroy AI application. And it's a drag and drop editor, big white expanse
11:39
of of nothingness. And so let's start with something that's uh pretty simple
11:44
um which is just a little user interface to talk to Oops, did that wrong. A
11:51
little user interface to talk to um we don't want to do this one.
12:01
I have Llama 3 running on my desktop as a local LLM
12:07
and the may want to well I guess most people that watching would know what Ola is but
12:13
we'll we'll talk about it for a second and how Olama and Llama 3 work while I go find where I put my pipeline folder.
12:19
So Olama is an inferencing engine, an inferencing platform. Um, it's an open-
12:24
source platform and it's one we're currently using for model inferencing. And what that means is when you send
12:32
info to an AI, you're sending it to an inferencing engine that's interacting with the model. Um, that's a really
12:39
simple way to explain it. And OAMA can run locally on your own machine. So even if you don't have a beefy GPUbased
12:46
machine, there are a ton of small models that you can utilize um to utilize some
12:53
functionality of AI via Olama. There are a ton of others, LM Studio, um other
12:58
products that um allow you to do this as well. We like we like the way it functions. Um but the one of the things
13:05
in terms of small um LLMs that's uh will be important in the future for Kilroy is
13:10
is that because we're able to to distill the functionality of agents into small
13:17
tiny bitsized pieces in these um peer-to-peer networks and swarms, we actually have the ability to offload
13:24
or to put a lot less um work on the LLM in any given task that it's doing. so
13:32
that we won't need as much processing power. And Chuck's favorite LLM is a
13:37
really old and really small one called Llama 3, which by by Meta by the Facebook people. Um, and he uses that
13:44
for everything. And it's a small LLM that will run on a lot of different machines, and it's actually not even the
13:51
most up-to-date LLM that Meta has, but it does a really good job. It doesn't hallucinate, and it knows how
13:57
to do math. Those are two things that are majorly in its favor. And if you've used any of the modern newer LLMs that
14:04
are online, ChatGpt or Claude or any of those, they go to great lengths to explain to you what you're doing. And
14:10
for most of the tasks that we're doing, we know what we're asking the LLM to do. We don't need an explanation. We just
14:16
need an answer. So you might say, "Hey, go get me a quote for a particular cryptocurrency." It doesn't need to come
14:22
back and give you an explanation of what a cryptocurrency is and five paragraphs later show you what the current price is. We just want to see the current
14:28
price, right? or it might be, hey, you know, go add these two numbers together and if it's bigger than a certain amount, go do the next thing. the the
14:35
the difference and I think it's kind of it's hard to see until you see some of the agents that we're going to show in
14:40
just a minute or some of the pipelines is that what would normally be two or
14:46
three pages worth of prompts into a chat GPT like web interface to try to get it
14:51
to go through multiple steps and do what you want are all one or two sentence prompts that are really small that let
14:57
these little local LLMs do one little job and then hand off their work product through the swarm to the next one does
15:04
the next step in the job. And sometimes there may be tools or transformations in the middle of those steps, which is not
15:11
something that you can do with the big centralized models very easily. Do some work, transform it with a tool, do some
15:17
more work, send it off to an a cloud service, get an answer back, do some more work. That that pipeline style
15:23
workflow is is very challenging to do as an enduser with the existing consumer
15:29
focused interfaces. So, all right. So, let's switch back to what's on the screen now. Now that I found the folder with the example in it that I wanted. So
15:36
this is the visual editor for doing AI pipelines in on top of Kilroy. Now
15:43
remember just a second ago when I showed you all the apps that were running in the sidebar of Kilroy. This is just an
15:48
app on Kilroy. This editor, the building of the agents, the execution of the agents and letting them talk through the
15:54
swarms is all just what's the equivalent of a thirdparty app on top of the Kilroy platform. So if you don't like this, you
16:02
can build your own. If you want to change the way it behaves, you can modify it. But for right now, what we have is a pallet full of some pre-built
16:09
parts. Um, we have agents, AI agents like this, this one here, which are
16:15
frontends to talk to an external AI. It can be an external AI like chat GPT. It
16:21
can be a local AI like Llama 3 running underneath O Lama on your own local
16:26
machine. Whatever you decide to configure in these parameters for the URL and the API key and the model name
16:32
and the temperatures and other stuff like that is what the pipeline's going to be started with. And then we have
16:38
other things for interacting with the pipelines. We have chat agent which is basically a user input screen. It looks
16:44
like a terminal window. It looks like the web interface that you use to talk to uh a chat GPT. And then in the middle
16:51
of the two is a swarm. This one is just creatively named swarm. And its whole
16:57
job is to take messages from the user input agent and pass them over to the other things in the swarm that are
17:03
listening. In this case, there's only one. It's this AI agent over here. And what that's really going to do is whatever I type on the screen is going
17:10
to go to the swarm and from the swarm over to the agent. It's going to look like a prompt string. It's going to do its work. It's going to give me its
17:15
answer back to the swarm and I'm going to listen to it and see it on the screen. These yellow labels on the edges
17:22
are basically saying who am I listening to or who am I sending as? I'm sending as a thing called chat agent. The AI
17:28
agent's listening to everything. Don't care. The AI agent is sending as a thing called AI agent. The user input field is
17:36
is just listening to everything. So let's go make that so you can see what it looks like in Kilroy and run it so
17:42
you can see how it interacts with the LOM in the background. So, we're going to build that pipeline and it's going to pop open some windows
17:48
in Kilroy really quickly. This is the user interface for the interactive portion of the screen. And this is sort
17:56
of a little debugging window to show you that the agent's really doing work in the background when it talks to the LLM.
18:02
You can hide this, show it. It doesn't matter if it's on the screen. Normally, a user would just see an interactive
18:07
window, but um I'll just ask it who is this. Now, it's
18:13
going to take a while for it to start up the LLM because we've been talking and it's gone to sleep. But, okay, here it
18:18
is. It's llama, right? This is who we're talking to. And,
18:30
you know, tell me about chocolate cake. Right. So, that's the typical interaction that you would get with the
18:37
the LLM. And yes, it's barfed out a a recipe for a chocolate cake. Here we go.
18:43
Um, and so in a couple of mouse clicks and drags in the editor, we built sort
18:48
of the equivalent of a local chat GPT. I've got a window that I can type in prompts go over to the AI and I get
18:54
answers back. Um, in some more elaborate examples, you can have you can ask it to create code and show it in another
19:00
window where it's running. So you can do vibe coding on top of Kilroy. Um, but this is a this is a the simple example
19:08
of just one little uh interactive agent. What do you think about showing the
19:13
integration with um Telegram, Brad? I can show. Yeah. Yeah, let's do that. simple one that we've got right now that I use
19:19
every day is I have a um an RSS feed
19:25
reader called Miniflux that sits out on a server for me and goes through about a
19:31
hundred different RSS feeds in real time all the time and and so its job is to
19:38
write into this swarm called Chuck Minifilelex swarm. The server out there in the cloud also knows how to talk to
19:45
Kilroy swarms and it pushes the data in through a web hook that Kilroy makes available to it. And there's a swarm
19:52
that's receiving a constant stream of RSS stories, right? Those feed into an
19:57
AI agent whose prompt is this. So system prompt says
20:03
you're an agent that transforms RSS stories into markdown messages for Telegram. and it goes into some more detail about how to construct the
20:10
telegram uh markdown message. But if you look at the end result, what I end up
20:15
with is uh a an unending stream of
20:21
scrolling RSS messages showing up in Telegram, which means even when I'm away from my desktop, I can pull out my
20:27
phone. I can look at the current news that has been pulled in from the RSS feeds and pushed over to Telegram by my
20:34
agents on my desktop. I'm not looking at some thirdparty news feed. I'm not looking at some uh you know online news
20:41
site like a you know a CNN or a Reuters or whatever. This is all my news being
20:47
picked out and selected and summarized by my AI and pushed out to my private Telegram channel.
20:52
And you can you can take it even further if you want. For example, my version of that um pulls in a bunch of feeds from a
20:58
different lot of different places and then I actually have it summarized into tweet size stories. So, it goes in, it
21:05
reads the article, it summarizes it, and then I get a feed of of tweetsiz
21:11
messages with the links in it and an engaging headline so that if I want to
21:16
repost something and comment on it, I can. So, you can take this any even further. You could have another agent in
21:22
the swarm that's actually um discerning which feeds, which stories, you know,
21:28
have some component to them that you want to be emphasized and have only those come in. So, um, there's a million
21:35
ways to expand on this, um, and interact with it. So, so let's look at let's look at one
21:40
that's a little bit more elaborate. So, we're working our way up in complexity, right? We have an RSS data coming into
21:46
an agent. The agent summarizes it as a markdown message for Telegram. Telegram shows it on the screen. And, oh yeah, as
21:52
a side effect, I can also look at it inside of Kilroy in a right in a terminal window inside of Kilroy. Um,
21:58
all of this runs over on a laptop on my desktop whose job right now is to do nothing but be my my news feed. Um, so
22:05
here's one that's more complicated. I've shown it three times now because I keep clicking the wrong tab. This one is a a
22:13
fully functional Telegram bot, right? This is uh an agent that runs in
22:20
Telegram uh a Telegram bot that talks to Kilroy in real time, sends messages from the
22:26
users in Telegram into Kilroy. It's processed through these two blue AI boxes here. Uh and the results are then
22:34
sent back to uh to Telegram for the user. Now the this is this is a kind of
22:41
an advanced leapt to the endgame here on Kilroyy's AI functionality, but this is
22:47
a really advanced model because it's got the ability for the user to change on
22:52
the fly what the system prompts are and change the behavior of the LLM using telegram bot commands. So instead of
23:00
just having one persona or one system prompt and that's how this is going to work you can see that there are several
23:06
commands that are supported and if you look closely there there's a command called uh slashpersona there's one for
23:13
uh slashhelp and there's one for slashreset and those are all hooked into this big swarm of nodes to to do the
23:21
things that you would expect. So instead of instead of uh looking at that and trying to decompose it, let me just show
23:27
you how it works. Uh who am I talking to?
23:35
So it's going to go over and uh I guess I'm I'm I'm in pirate persona. I
23:41
didn't realize that. So the Let me let me uh let me reset that again and let
23:48
you see what's really going on. We'll uh hide these guys and hide this guy
23:55
and bring back the techbot and show you. This one has lots
24:01
of pieces and out of the box it just has a generic persona, but um because I was
24:08
playing with this before we got on the call, it had switched into
24:14
cap sea captain mode. And um so if we just ask it well what are the things
24:20
that I can do right there's the normal response that you get and you can see in the
24:26
background here in Kilroy with all these other windows open you can watch these lights these blinky lights go off which
24:32
will basically give you some indication of the work that's getting done by the various uh agents along in the pipeline.
24:37
I'll shrink this down a little bit so we can see them all. But if you built a pipeline to send out or distribute to
24:42
other people, which is something you can do on our network, um then you can make it so they don't see any of that. They
24:48
just see the the primary interface or see nothing and only see it in Telegram.
24:54
So you can ask it knows about itself and it knows its own command. So I can ask it uh
25:02
things about how do I do different stuff. But if I just tell it the persona is uh Karen
25:08
and uh have it switch over to that. So now Karen persona is active. You lose
25:15
her. Let me reset it so it forgets the captain stuff and um you need to leave
25:23
our store immediately. And of course she's going to yell back.
25:29
Right. So wi with just commands and telegram I've completely reconfigured
25:34
the pipeline to have a different system prompt a different persona to change uh who I'm talking to and it didn't change
25:41
the pipeline right we didn't have to change the topology we changed the configuration of the nodes in the
25:47
pipeline that's what these red lines indicate and in a future podcast we'll go into more details about how to build
25:53
something like this but basically the flow is message comes in from telegram is it a command if it's a command go
25:59
down here to the command process processor and decide what to do. Was it the reset command? Send a reset command off to the AI. Was it a help command?
26:06
Then ask for help and put that back out to the user. If it was one of the persona commands, it comes in up here
26:12
and it changes to either the pirate, the baby, or the Karen and changes the system prompt in here. And if we uh if
26:19
you looked at the system prompt in the thing as a as it's running right now, which we can do, we can go back over
26:25
here real quick and ask it to show us the system prompt,
26:30
which says, "You're an angry angry middle-aged woman who is never happy and always wants to speak to the manager, always respond to the user in
26:35
character." If I change the if I go back to Telegram and change it to be uh
26:43
the baby persona, right? And then we go back up here and
26:50
and look at the setting for the system prompt. Now it's you're a baby, always respond to the user and character. So
26:57
behind the scenes, the commands from Telegram are reconfiguring Kilroy on the fly. This could be commands to start up
27:03
new agents. Kilroy can self-modify these pipelines. It can add in new agents. It can change the cloud the swarms that
27:09
it's talking to. It can change the system prompts. It can rewire itself on the fly based on user input or input
27:16
from the outside world. Um really and truly it's sort of Skynet in a box, right, Brad?
27:21
Yeah, absolutely. I mean, I think that's a really important thing to to emphasize here is that um we're at the point with
27:28
this ability to create these pipelines where um a user can um
27:36
we you'd be able to you'd be able to structure these things so that when an agent needs more capability, it can
27:42
actually create that capability, right? So, well, and and the really cool thing is this doesn't have to all be running on
27:48
your machine, right? If you look back at this diagram, all of these different swarms, any one of these swarms could be
27:55
shared with other Kilroy users out in the wild internet and their agents could
28:01
respond, right? So, I could have a a bot that interacts with a bot that's running on Brad's desktop and gives me some
28:08
answers back and we literally don't have to do anything but coordinate on what is
28:14
the name of the swarm that we're going to use to talk through. You could have a 100 person chat room built on top of
28:19
this with uh you know AI trading strategies for cryptocurrencies that you are watching with your crypto bros,
28:27
right? And the agents can be working together and collaborating to refine and improve different trading strategies.
28:34
You could have trading contests where agents are trading against each other and whatever one wins, you know, people
28:40
can access and utilize for their own trading strategies. Um this can be on centralized exchanges, this can be in
28:46
DeFi. Um and the key the other thing I wanted to to point out to everybody is is that the the integration with
28:53
Telegram is not unique. The this interaction and integration could happen
28:58
in Telegram. It can happen in Discord. It could happen in text messages. There
29:03
there really is no working in Minecraft, right? Yeah, we have it working in Minecraft, right? So, we built a plugin of Kilroy
29:10
into Minecraft and you can actually interact and control the Minecraft
29:15
environment via Kilroy. So, you could then have AI agents that are building in
29:20
Minecraft. You can have all kinds of things happening. Um, the key is is that the interface is irrelevant and so it
29:27
can go to wherever a user wants to live. It doesn't have to live within the Kilroy interface. It doesn't have to
29:34
live within Telegram. It can be whatever your company or your team or your group
29:39
wants to utilize as a UI. And there's nothing stopping you from creating brand
29:45
new UIs that function and live inside of Kilroy. And there's nothing stopping you from integrating any API that exists in
29:52
the world into Kilroy. Um, if there's a web hook, if there's a way to interconnect to it, you can build a
29:59
module within Kilroy and you can now make it available to agents. So maybe you have a company that has services
30:05
that you want people to be able to utilize to order pizzas all over the world. Um, you can do that. You may have
30:11
games that you want people to be able to interact with in a swarm and have thousands of players automatically
30:16
playing and competing against each other. That there really is no limit to it. And that's actually one of our
30:22
biggest problems with talking about Kilroy is well and I think Brad this the the the the key thing to take away from this is
30:29
that this is a completely novel way to put together LLMs and agents and
30:35
cooperating human users with the real world services. Nobody has done this
30:42
particular swarm-based peer-to-peer completely decentralized AI model. Yeah,
30:48
you can call tools from chat GBT and you can have MCP as your tool calling interface to get to all sorts of
30:55
services and you pretty much have to be an expert programmer to get all the interfaces lined up and the glue code
31:01
written and the pipeline built and the pipeline's very static and it does what it does. In Kilroy, any user that can
31:07
draw a picture like this by dragging blocks onto the screen and connecting the lines together can make a pipeline
31:13
that does more than you can do with the current tooling that's sort of state of the practice out there in the internet.
31:19
Yeah. And I think let's carry that over into crypto, you know, kind of our originating world, right? the the really
31:27
cool one of the really powerful and the things that attracted me to crypto from the beginning was understanding
31:33
composability, understanding the ability for components of financial services and
31:38
and crypto tools to be made accessible and ava a available to other protocols,
31:44
right? So, a lending protocol might utilize a swapping protocol within their interface or whatever. But what but that
31:52
make requires you to be a solidity programmer, right? But as we drop in modules um for DeFi protocols into this
32:00
platform, now users in this AI interface or in the regular app interface can make
32:06
use of those to build new agents, new applications, new pipelines that make
32:11
use of all kinds of protocols in DeFi onchain. The other thing that's that
32:17
this solves for crypto is one of the biggest holes we have in in the goal of
32:23
decentralizing money is that all of our interfaces in DeFi are on the web. And
32:28
so there is a centralized server and there is a registered domain that are susceptible to attack. And that has
32:35
happened to the tunes of millions and millions of dollars being stolen from domains getting hacked and being taken
32:41
over um and to servers being hacked. This eliminates that problem because you
32:46
can now interact directly with a smart contract on a decentralized UI and build
32:52
a UI that's easy for a user to use without them having to visit a website ever to interact with a protocol. And
33:00
now you can have these interconnected protocols within Kilroy that allow you to do all kinds of really powerful
33:06
things um and and make it work. And and as a one more side note, um one of the
33:11
things built into Kilroy is um and the reason we're probably going to have our own um layer two or three for Kilroy is
33:19
that the apps can be stored as NFTts. And so you can actually distribute
33:25
applications in this network and licenses in this network um in NFTTS and
33:31
enable people to easily use the app store that way and access um apps
33:36
because all of the apps in in Kilroy are essentially text files. They're JSON. So
33:43
the engine recognizes and understands the commands that drive it from a JSON file, but we don't require this this
33:50
huge um installation paradigm that we're all used to on our applications on our
33:56
on our computers. There's no executable to install. Um these are really simple
34:01
and easy and fast to install applications. So all right, we've used up our 30
34:07
minutes. All right. All right. So, um, if you happen to watch this, uh, I don't think
34:13
we're going to have a lot today, but if you happen to and you're someone that doesn't mind getting your hands a little
34:19
dirty, we're looking for folks that want to experiment with Kilroy, try it out,
34:25
install it on their machines, and help us knock off some of the rough edges, build integrations into the platform,
34:32
um, um, and build start building pipelines and agents that other people can
34:38
utilize. Um there will be a revenue stream available for for these um that that um will be a part of the entire
34:45
Kilroy platform, but we're really looking for people that can see the power of what this can do and are
34:51
interested in helping to uh build it out further. And there's a lot of opportunities within what we're trying
34:57
to build and where we're going with this thing that if you're interested, um DMs are available on Farcaster, on X, um
35:05
Telegram. I'm B 05 crypto. Chuck, you're cshoten correct on all those platforms.
35:11
Um, yep. I'm B05 on Farcaster, Bz Crypto on everything else. So, if you're
35:17
interested, you'd like to play with this, you want to be an early adopter, um, you want to learn more about it,
35:22
please don't hesitate to reach out. We would love to add more people utilizing this.
35:28
And tune in next time for some details on how you really bank make these things rather than just looking at scary
35:34
pictures. We're gonna go through the build process for pipelines and for Kilroy apps and how it works under the
35:40
hood. So that by the end of this, you should be a pro. That's excellent. I love it. All right,
35:45
I'm gonna end the stream. I hope everybody has a great day. See you guys. Thanks, Brad. ing up.
ROTP: Introduction to Kilroy! Decentralizing AI/Automation/Crypto

Realm of the Possible
4 subscribers

Subscribe

2


Share

Save

Clip

31 views  Streamed live on Oct 17, 2025
Learn how you can be empowered to build anything with a powerful decentralized automation and AI platform. 

We've created the most powerful decentralized AI and crypto automation/integration platform while being completely decentralized. It comes complete with no configuration peer-to-peer/swarming capabilities, the ability to easily build integrations with any other compute, and a Turing complete visual creation tool anyone can use to build functionality in the network while giving agents on Kilroy the ability to build and deploy on their own.
Transcript
Follow along using the transcript.


Show transcript

Realm of the Possible
4 subscribers
Videos
About

All

For you

59:26
He Built a Privacy Tool. Now He’s Going to Prison.
Naomi Brockwell TV
431K views
•
11 days ago


14:19
What Sam Altman Doesn't Want You To Know
More Perfect Union
2.3M views
•
10 days ago
Fundraiser


16:19
How ChatGPT Is Weirdly Turning Into Facebook
Enrico Tartarotti
141K views
•
2 weeks ago


17:14
Why Scandinavian Cabins Stayed Warm At -30°F While Modern Homes Freeze
Medieval Way
1M views
•
1 month ago


16:25
The billion dollar race to replace Windows
TechAltar
167K views
•
5 days ago
New


5:47
Chef Show - SNL
Saturday Night Live
12M views
•
2 years ago


16:04
Mr Bean does 'Blind Date' | Comic Relief
Comic Relief
22M views
•
16 years ago


15:40
I Quit an AI Startup After 6 Months - Here's What I learned
Brian Jenney
191K views
•
2 weeks ago


20:48
Watch Ukrainian Drones OBLITERATE a Russian Plane
Beyond Military
747K views
•
10 days ago


1:29:03
"We have 900 days left." | Emad Mostaque
Dr Myriam Francois
696K views
•
1 month ago


11:05
Will Ferrell Refuses To Discuss The Bird On His Shoulder | CONAN on TBS
Team Coco
1.4M views
•
3 months ago


43:30
Are we stuck with the same Desktop UX forever? | Ubuntu Summit 25.10
Canonical Ubuntu
332K views
•
2 weeks ago


28:17
If you don’t run AI locally you’re falling behind…
David Ondrej
123K views
•
1 month ago


30:40
Is The Line Really Dead?
The B1M
2.6M views
•
11 days ago


15:23
The Best Self-Hosted AI Tools You Can Actually Run in Your Home Lab
VirtualizationHowto
163K views
•
1 month ago


12:37
The AI Race is Over... Google Just Won.
Breaking Even
504K views
•
10 days ago


48:54
America CAN'T Compete with China’s High-Tech Future! 🇨🇳
Jay and Karolina
307K views
•
2 weeks ago


1:02:13
У нас был план  - КУПИТЬ ДОМ! в Португалии!  KeyDom 2
Vadim Key
233K views
•
14 hours ago
New


43:24
CEO SHUTS DOWN Restaurant on the Spot! | FULL EPISODE | S3 E4 | Undercover Boss USA
SoReal
3.1M views
•
5 months ago


23:12
Richard Feynman Explains Time Like You’ve Never Seen Before
Physics The Feynman Way
562K views
•
8 days ago


Show more
0 Comments
Don Hopkins
Add a comment...

 



Watch What I Do:
Programming by Demonstration
 	
edited by Allen Cypher
co-edited by Daniel C. Halbert, David Kurlander, Henry Lieberman, David Maulsby, Brad A. Myers, and Alan Turransky
1993
The MIT Press
Cambridge, Massachusetts
London, England
 
Table of Contents
The entire text of this book is included on this web site. Access it through the Table of Contents.

 

 

Book Reviews
BYTE December 1993
International Journal of Man-Machine Studies December 1993
Sci Tech Book News December 1993
GIS World February 1994
SIGCHI Bulletin April 1994
Burrelle's May 1994
Journal of Visual Languages and Computing September 1994
User Modeling and User-Adapted Interaction 1994

Contents


Foreword
Alan Kay

Preface
Introduction:

Bringing Programming to End Users
 
I Systems
1 Pygmalion:
An Executable Electronic Blackboard
David Canfield Smith
2 Tinker:
A Programming by Demonstration System for Beginning Programmers
Henry Lieberman
3 A Predictive Calculator
Ian H. Witten
4 Rehearsal World:
Programming by Rehearsal
William F. Finzer and Laura Gould
5 SmallStar:
Programming by Demonstration in the Desktop Metaphor
Daniel C. Halbert
6 Peridot:
Creating User Interfaces by Demonstration
Brad A. Myers
7 Metamouse:
An Instructible Agent for Programming by Demonstration
David Maulsby and Ian H. Witten
8 TELS:
Learning Text Editing Tasks from Examples
Ian H. Witten and Dan Mo
9 Eager:
Programming Repetitive Tasks by Demonstration
Allen Cypher
10 Garnet:
Uses of Demonstrational Techniques
Brad A. Myers
11 The Turvy Experience:
Simulating an Instructible Interface
David Maulsby
12 Chimera:
Example-Based Graphical Editing
David Kurlander
13 The Geometer's Sketchpad:
Programming by Geometry
R. Nicholas Jackiw and William F. Finzer
14 Tourmaline:
Text Formatting by Demonstration
Brad A. Myers
15 A History-Based Macro by Example System
David Kurlander and Steven Feiner
16 Mondrian:
A Teachable Graphical Editor
Henry Lieberman
17 Triggers:
Guiding Automation with Pixels to Achieve Data Access
Richard Potter
18 The AIDE Project:
An Application-Independent Demonstrational Environment
Philippe P. Piernot and Marc P. Yvon
 
II Components
19 A History of Editable Graphical Histories
David Kurlander and Steven Feiner
20 Graphical Representation and Feedback in a PBD System
Francesmary Modugno and Brad A. Myers
21 PBD Invocation Techniques:
A Review and Proposal
David S. Kosbie and Brad A. Myers
22 A System-Wide Macro Facility Based on Aggregate Events:
A Proposal
David S. Kosbie and Brad A. Myers
23 Making Programming Accessible to Visual Problem Solvers
Henry Lieberman
24 Using Voice Input to Disambiguate Intent
Alan Turransky
 
III Perspectives
25 Characterizing PBD Systems
Allen Cypher, David S. Kosbie and David Maulsby
26 Demonstrational Interfaces:
A Step Beyond Direct Manipulation
Brad A. Myers
27 Just-in-time Programming
Richard Potter
 
IV Appendices
A A Programming by Demonstration Chronology:
23 Years of Examples
David Maulsby and Alan Turransky
B A Test Suite for Programming by Demonstration
Richard Potter and David Maulsby
C Glossary
Brad A. Myers and David Maulsby

Bibliography

Contributors

Index

back to ... Watch What I Do  Allen Cypher


Foreword
Alan Kay







I don't know who first made the parallel between programming a computer and using a tool, but it was certainly implicit in Jack Licklider's thoughts about "man-machine symbiosis" as he set up the ARPA IPTO research projects in the early sixties. In 1962, Ivan Sutherland's Sketchpad became the exemplar to this day for what interactive computing should be like--including having the end-user be able to reshape the tool.

The idea that programming should be set up so it could be metaphorically like writing is harder to track down, but you could see it in Cliff Shaw's JOSS from the same early period. Besides being the first real "end-user" language, and the first attempt at a really "user-friendly" interface, it included a special terminal design adapted from a high-quality IBM electric typewriter that printed in two colors in lower and upper case on drilled fanfold 8*11 paper so that the output was a direct extension of one's notebook.

The vague term "computer literacy" also surfaced in the sixties, and in its strongest sense reflected a belief that the computer was going to be more like the book than a swiss army knife. Being able to "read" and "write" in it would be as universally necessary as reading and writing became after Gutenberg. The Dynabook idea was a prime focus during this time as the kind of thing computers were going to turn into, forced by engineering possibility and sociological necessity.

The analogy to reading was the easiest to see. If "reading" is the skill to be able to understand and use messages represented as gestures in a medium whose conventions are in close agreement between writer and reader, then the equivalent of reading on a computer would require the invention of a user-interface language that could universally frame the works of many thousands of authors whose interests would range far beyond those of the interface designers. "Writing" on the other hand requires the end-users to somehow construct the same kinds of things that they had been reading--a much more difficult skill.

At first this was so analogous to designing a programming language that many "interactive" interfaces were designed--some trying to improve on JOSS' dialogue scheme, while others attempted to build artificially intelligent agents that could turn advice into generalized actions. McCarthy's "advice-taker" idea had a huge influence on everyone's ideas. The notion of "programming by example" arose--perhaps the earliest was Teitelman's PILOT system, in which he tried to build an advice taking system (using a pattern matching production system) that could recapitulate the early AI theses at MIT.

Wally Feurzig and Seymour Papert had a different notion about the place of computer "reading" and "writing": that like the reading and writing of books, it wasn't just about getting and conveying information, but the very act of learning and doing them expands one's horizons and adds new ways of thinking about the world. In other words, programming could be good for people, and thus some effort should be put into designing systems that would have pedagogical benefit for both children and adults.

These ideas resonated strongly with me, partly because of my background in music, biology and mathematics. Computer processes coordinating in time go beyond Kepler's music of the spheres to a "music of metaphysics". A very similar metaphor is that of cellular and developmental biology which goes far beyond the classical Newtonism of 19th century science to a much more involved and inherently nonlinear systems organization. To have a medium that could be read and written at this new level of complexity--a level in which many of the "gotchas" of our civilization and science reside--seemed tremendously important. And still does.

Many such considerations eventually led to the realization that "it wasn't a language, but an environment", and this led directly in the early seventies to the overlapping window and pointing interface coextensive with objects that could send messages to each other and thus model any dynamic system. But the "writing" problem still remained--in part, because it was not even clear what the writing problem was.

The interface design was strongly influenced by the multiple mentality ideas of Jerome Bruner, in which the "middle" mentality, the iconic one, was the bridge between infancy and adolescence. Inspired by a few early examples--such as Paul Rovner's AMBIT-G--we decided to concentrate our research on iconic programming. Still not knowing what it meant, we dealt with it in the traditional manner for handling very difficult problems. Namely, give them to graduate students and tell them they are easy.

The first of these was Dave Smith, and his PYGMALION became the new exemplar for what iconic programming by example might mean. A host of others from our group followed--including Alan Borning's Thinglab, Laura Gould and Bill Finzer's Programming By Rehearsal, Dan Halbert's SmallStar, and Dan Ingalls' Ariel (a later version was called FABRIK). By this time a community had formed with Henry Lieberman's TINKER taking an important new path.

Today, we have windowed interfaces everywhere, and even a number of iconic object-construction kits. We have macro capture systems of every kind, and scripting languages. But we don't have "end-user programming". Nor do we have "programming by example".

One of the problems is range. By this I mean that when we teach children English, it is not our intent to teach them a pidgin language, but to gradually reveal the whole thing: the language that Jefferson and Russell wrote in, and with a few style shifts, the language that Shakespeare wrote in. In other words we want learners of English not just to be able to accomplish simple vocational goals, but to be able to aspire to the full range of expression the language makes possible. In computer terms, the range of aspiration should extend at least to the kinds of applications purchased from professionals. By comparison, systems like HyperCard offer no more than a pidgin version of what is possible on the Macintosh. It doesn't qualify by my standards.

Programming by example adds yet another burden to that of end-user programming. The user's intent as expressed in examples is to be divined by the system and turned into a useful generality. Humans have a very large range of intents, and extreme context restrictions have to be invoked to recognize any but the simplest--note that the windows and other devices of the overlapping window interface serve to restrict context while giving the user the illusion of freedom. Many of the most useful PbyE systems use the highly restricted windowing environment to great advantage.

At some point we can expect to see large complex models of human common sense and goal structures--as predicted by McCarthy long ago and slowly being realized by Doug Lenat's CYC system--getting coupled to user interfaces whose goal it is to produce in the user a style of action that will permit most goals to be recognized and automatically completed. Whether this can actually be done without requiring the user to tell the system what the goal is remains to be seen. Humans are not all that great at recognizing and understanding each other's goals, but perhaps our ego-centeredness can be left out of the artificial system.

In any case, I think the most important issues regarding end-user programming and its subbranch of programming by example are pedagogical and ethical. There is no question that a human with a goal wants to have the sub-goals ready made and at hand. One shouldn't have to learn about Carnot cycles of internal combustion engines--or even just hand cranking it--in order to drive an automobile. And agents that can be told goals and can go off and solve them have been valuable and sought after for as long as humanity has endured.

On the other hand, it takes a very special value system for children and adults to be able to exist as learning creatures--indeed as humans at all--in the presence of an environment that does all for them. 20th century humans that don't understand the hows and whys of their technologies are not in a position to make judgments and shape futures. At some point it is necessary to understand something about thermodynamics and waiting until then to try to learn it doesn't work. Nature's rule is "use it or lose it"--most social systems that have incorporated intelligent slaves or amanuenses have "lost it". In fact most never gained it to lose. In a technopoly in which we can make just about anything we desire, and almost everything we do can be replaced with vicarious experience, we have to decide to do the activities that make us into actualized humans. We have to decide to exercise, to not eat too much fat and sugar, to learn, to read, to explore, to experiment, to make, to love, to think. In short, to exist.

Difficulties are annoying and we like to remove them. But we have to be careful to only remove the gratuitous ones. As for the others--those whose surmounting makes us grow stronger in mind and body--we have to decide to leave those in and face them.



Preface






This book grew out of a workshop on Programming by Demonstration that was held at Apple Computer in March, 1992. The workshop was an opportunity for current researchers to discuss their work with the pioneers in the field. David Smith demonstrated a HyperCard simulation of his Pygmalion system, which was the first system for programming by demonstration and the inspiration for the work that has followed. Henry Lieberman ported his classic Tinker system to the Macintosh so that he could give a live demonstration at the workshop. This was followed by classic videos of the early systems, live demonstrations of the newer systems, and open discussion on topics in the field.

The participants found the workshop to be very rewarding, largely because it had always been difficult to access the relevant papers, articles, books, and videotapes describing these systems. Given the recent widespread interest in end user programming, we felt that a larger audience could benefit from this material, so we decided to republish it in a book. From that original plan, a rather different book has resulted. Instead of simply republishing their original articles, most of the authors have either written completely new chapters or have updated and extended their articles.

Furthermore, we have included two additional sections in the book: Section II discusses particular aspects of programming by demonstration in greater detail, and Section III provides broader perspectives on the field.

The Appendices are particularly valuable: in addition to a chronology and a glossary, there is an extensive test suite which lists a wide variety of specific tasks that researchers feel are amenable to programming by demonstration. In addition to demonstrating the potential of PBD, this test suite can also serve as a standard for measuring the capabilities of a given system.

The March event was called the "Programming by Example Workshop". Several of the participants observed that first-time hearers were more likely to understand what this field was all about when it was termed "Programming by Demonstration", and we have therefore all modernized our vocabulary.

This book is not only intended for individuals who are actively working in the field of programming by demonstration. We have aimed to make this material accessible and interesting to a larger audience: students and researchers with an interest in end user programming, and individuals interested in user interface design and agent-based systems. It is not a book about machine learning or artificial intelligence. Rather, the focus is on ways to create the appropriate human-computer interaction so that end users can gain more control of their personal computers.

I would like to thank the External Research Group at Apple Computer for their generous support of the workshop, and Mark Miller and Rick LeFaivre of the Advanced Technology Group at Apple Computer for their continued and enthusiastic support of this book. I would also like to thank Yin Yin Wong for her help in designing the layout for this book. Finally, I would like to thank CE Software for their QuicKeys program, which allowed me to automate many of the dreary tasks involved in editing 30 chapters.

Allen Cypher



Introduction:
Bringing Programming to End Users
Allen Cypher





The motivation behind Programming by Demonstration is simple and compelling: if a user knows how to perform a task on the computer, that should be sufficient to create a program to perform the task. It should not be necessary to learn a programming language like C or BASIC. Instead, the user should be able to instruct the computer to "Watch what I do", and the computer should create the program that corresponds to the user's actions. This book investigates the various issues that arise in trying to make this idea practical. The first section of the book describes 18 computer implementations of Programming by Demonstration, and the second section discusses the problems and opportunities for Programming by Demonstration (PBD) in more general terms.


Why Would Users Want to Program?

The first system for Programming by Demonstration, David C. Smith's PYGMALION, was written in 1975, but it was not until the more recent emergence of a large population of personal computer "end users" that the approach began to attract widespread attention. In the 1960's, computer users were either programmers themselves, or they had programmers who wrote programs specifically for them. This meant that users had their own custom applications -- programs designed specifically for their task. Large insurance companies had their own custom accounting programs, and large businesses had custom inventory programs.

With the advent of personal computing, this all changed. Now the local dry cleaners' uses a personal computer to handle sales with SalesPoint and to prepare advertisements with MacDraw. Parents are using personal computers at home to manage their finances with Quicken, and their children are writing essays in Word. Instead of using a custom application developed by a nearby programmer for a very specific task, people use a generic application developed by a distant, unknown and unreachable programmer to handle tasks similar to theirs.

Contemporary computer users are "end users", meaning that they are at the end of the process of computer programming, far removed from the programmer. The programs they are using were not written with their particular needs in mind. The programmer who created Quicken doesn't know that you work at a second job on the weekends, or that you keep a separate checkbook for household expenses. The programmer who created Word doesn't know that your English teacher requires book titles to be underlined in your bibliographies. As a result, end users must map their activities into the capabilities of the generic applications. It is inevitable that this mapping process will involve tedious steps that could be automated if only the end user were the programmer who had created the application.

Every week or so, I record my latest credit card charges in Dollars and Sense, a home accounting program. Before I can enter any charges, I have to do the following: I select the "Edit Transactions" command from a menu. A dialog asks me for the Funding Account and the date range. I scroll through a list of about 20 accounts to find "MasterCard". Then I type in the first day of the current month as the "From" date. Had this program been written specifically for me, it would have had a button labeled "Add MasterCard Charges" that would perform all of these steps. But since this is a generic program, intended for thousands of people with varying accounting needs, an "Add MasterCard Charges" button would not make sense. As a result, I am stuck performing a whole sequence of actions instead of just one, because I am using a generic program to perform a specific task.



Figure 1. The "Preferences" dialog for MacWrite II.


End User Programming

This leaves personal computer users in an ironic situation. It is a truism that computers are good at performing repetitive activities. So why is it that we are the ones performing all of the repetition, instead of the computer? Solutions are needed which enable end users to create their own custom commands. The various techniques for achieving this goal are generically referred to as "end user programming". Note that these techniques need not be programming per se: rather, they need to achieve effects that can currently only be achieved through programming. The current approaches to end user programming can be lumped into four categories: Preferences, Scripting Languages, Macro Recorders, and Programming by Demonstration.

Preferences (see Figure 1) are pre-defined alternatives supplied by an application designer to accommodate the varying needs of several different types of users. By choosing one of the pre-defined alternatives offered as a preference, a user can get the application to respond in the way that is most appropriate to his or her working style. Preferences are necessarily restricted to situations that the application designer is able to foresee, and they cannot accommodate the highly idiosyncratic needs of individual users. For instance, there will never be a preference option to "send invoices to all California customers on the 1st of each month, and to all other customers on the 15th". Another limitation of preferences is that at some point, with an overabundance of options, any system which offered all conceivable options would end up with option sheets with hundreds of entries, and it would be untenable for users to find and select their desired choices.

Since preferences offer only fixed alternatives, they are not really general enough to be considered a form of programming.

Scripting Languages (see Figure 2) are currently a popular approach to end-user programming. A scripting language is a small, simple programming language whose vocabulary is specifically tailored to the objects and actions of a particular application domain. The hope is that such a language will not be too difficult for end users to learn. Scripting languages are indeed less daunting than the standard general-purpose programming languages. For instance, in the HyperCard application, the scripting language description for the location of the message box is "the location of the message box", while the corresponding description in Pascal is

"GlobalToLocal(messageWindow^.portBits.bounds.topLeft)".

However, many of the standard programming difficulties remain. To make the box go away, you must use the command Hide the message box. If you write Close the message box, you get the error message "Can't close that window". You can write Hide the message box or Hide "the message box", but not Hide the "message box". And of course, for all of these commands, you have to know that this object happens to be called the "message box".

on JumpToStack
put the name of this stack into NameOfStack
go to card 1 of stack Home
doMenu "New Button"
set the style of button "New Button" to roundRect
put "on mouseUp" & return into jumpScript
put "go to " & NameOfStack & return after jumpScript
put "end mouseUp" after jumpScript
set the script of button "New Button" to jumpScript
set the name of button "New Button" to "Go To " & ~
NameOfStack
choose browse tool
end JumpToStack

Figure 2. A HyperCard script.
In summary, the basic failing of scripting is that it is still programming. That is, 1) users have to learn the arcane syntax and vocabulary conventions of the language, and 2) they have to learn the standard computer science concepts of variables, loops and conditionals. For a significant number of contemporary computer users, be they history students, real estate agents, or shop owners, the hurdle of learning a scripting language is simply too high.



Figure 3. A recorded QuicKeys macro to add page numbering to a Word document. The macro consists of a menu selection followed by three mouse clicks. Executing the "Print Preview..." menu command displays a page of the document as it will appear when it is printed. The first Click selects the "Page Number" tool. The second Click places the page number in the desired location on the page. The third Click is on the "Close" button, removing the "Print Preview" display.
Macro Recorders (see Figure 3) provide users with a way to record their actions. These recorders are a basic implementation of "Watch what I do". The user issues the "Start Recording" command, performs a series of actions, and then issues the "Stop Recording" command. All of the user's actions are saved as a sequence, and the user can then invoke a "Redo" command to replay the entire sequence. Many spreadsheet programs and telecommunications programs have built-in macro recorders. For example, the spreadsheet user can automate selecting cells Al through A15 and copying them to cells E20 through E34. And the telecommunications user can automate the sequence of dialing an on-line service, typing in the login name and password, and selecting a particular bulletin board. There are also some system-wide macro recorders. KeyWatch, on IBM PC's, will detect and automate a repetitive sequence of keystrokes. QuicKeys, on the Macintosh, can automate a sequence of mouse selections, mouse drags, menu selections, and keystrokes. For instance, by recording the action of dragging the icon "November Advertisement" to the icon of a "BackUp" disk, the user can automate the process of making a backup copy of this document.

The main failing of macro recorders is that they are too literal. They replay a sequence of keystrokes and mouse clicks, hereas most repetitive activities are repetitive at a somewhat higher level of abstraction. Rather than selecting cells A1 through A15, the user may actually want to select this month's employee sales, which may now occupy cells A1 through A16 if a new employee has joined the department. After logging on to an on-line service, the user may want to download all of the new messages related to Bay Area restaurants. But the specific actions required to accomplish this download will vary as the contents of the bulletin board varies from day to day. Finally, as illustrated in Figure 4, when the user records the command Drag from Screen Location (161, 90) to Screen Location (209, 201) to make a backup of the document "May Advertisement", it may happen that a replay of that precise action instead moves the document "Phone Numbers" into the "Price List" folder, because the windows and icons on the screen are now in different locations.

 Record Replay

Figure 4. Replaying a macro can have unexpected results.
Programming by Demonstration (see Figure 5) is an elaboration of the idea behind macro recorders. Once again, the user instructs the system to "Watch what I do", but with programming by demonstration, the system creates generalized programs from the recorded actions. Instead of selecting cells A1 through A15, a generalized program can select all of the rows before row "Total". Instead of downloading messages 1, 4, and 7, a generalized program can download all messages marked "unread". Instead of dragging from one screen location to another, a generalized program can make a copy of this month's Advertisement document, regardless of where its icon is located on the screen.

Furthermore, generalized programs can contain iterative loops and conditional branches. It is possible to demonstrate how to transfer one address from an old address book into a new address book and have a programming by demonstration system transfer all of the addresses in the book. It is possible for a PBD system to create a program that only transfers address cards where the state is "CA" or "California".



Figure 5. Metamouse assists the user in repositioning lines.
The greatest advantage of Programming by Demonstration over conventional programming is that it is "Programming in the User Interface" - a term coined by Dan Halbert. Conventional programming requires the programmer to map from the visual representation of objects being moved about the screen into a completely different textual representation of those actions. By Programming in the User Interface, users can refer to an action by simply performing the action, something they already know how to do. They are programming in the same environment in which they perform the actions. With conventional programming, they must learn a second, abstract, technical and alien way of referring to objects and actions.


What Do Users Want to Program?

There is a wide range of user-programming needs that can potentially be satisfied by programming by demonstration. The simplest types of activities involve making little changes in an application so that it more closely fits one's personal needs. These changes may be called tweaking, and they often correspond to setting preferences that the application designer did not foresee.



Figure 6. A perennial dialog box.
For instance, whenever I open a "package" in my mail program, the program displays a dialog box which asks me whether I want to delete the package (see Figure 6). I always want to delete it, but every time I have to click "Yes". I would like my mail program to delete packages without asking. Since the designer did not have the foresight to include an "Always delete packages?" preference, I must resign myself to this annoying "feature". I would like to be able to record the action of clicking "Yes" in the "Delete Package?" dialog box, and have this program automatically run whenever the dialog appears. This would be almost as good as not being asked at all. A macro recorder could record the action of clicking "Yes", but there is no way to automatically invoke the macro whenever this particular dialog box appears.

Probably the largest potential use for programming by demonstration is for automating repetitive activities. Users commonly have to perform iterative activities, such as renumbering a long list when a new entry is inserted in the middle, and they also have to perform periodic activities, such as backing up recently changed files. A graphic designer showed me a good example of a task that needs automating. This designer is responsible for a large computer reference book, and she had recently decided to use a different font for the figures in the book (see Figure 7). The new font was somewhat larger, so the labels -- words with a box around them -- no longer fit in their boxes. The designer had to reshape all of the boxes in more than 120 figures in the book. She would have been much happier to create a program by demonstrating how to reshape a few of the labels, and then use that program to automatically reshape the rest.



Figure 7. A small part of a figure containing text in boxes. All of the boxes containing text have to be reshaped.
The third potential use for programming by demonstration is for building mini-applications. Sometimes users' needs are unique enough that commercially available applications are not appropriate. HyperCard has been successful in addressing some of these needs, but it requires that users know how to write programs. I know a programmer who wanted an address book that would allow him to keep several addresses and phone numbers for each person -- work and home addresses, for instance. He used HyperCard to create his own custom address book (see Figure 8). It would be wonderful if PBD permitted non-programmers to create custom applications like this.


What Makes PBD Difficult?

This book is dedicated to taking advantage of the fact that users know how to perform tasks themselves. This ability should be very helpful in creating programs that perform those tasks. But there is more to a program than meets the recording. Users are quietly and imperceptibly making decisions. They are searching by eye. They are reading and understanding natural language. They are interpreting the interface in terms of their goals.



Figure 8. A custom Address book.
Inferring Intent

The main challenge confronting Programming by Demonstration is how to infer the user's intent. In order to convert a recorded action into a program to perform that action, the system needs to determine the user's intent in performing the action. When the program is executed in the future, the context will be somewhat different, and it will be necessary to perform the action that is the equivalent of the recorded action in this new context. The process of inferring a user's intent in selecting a particular object has been described by Dan Halbert as creating a "data description".

When the user selects Re: meeting next week in the mail message in Figure 9, the user's intent could be to 1) select the subject of the message, 2) select the first four words of the subject, 3) select all but the first word of the third line, 4) select any subject beginning with Re:, 5) select all subjects related to meeting next week, and so on.

If this text is a memo written in a word processor, the application will have no special knowledge about memos. When the user selects Re: meeting next week, there is no indication that the preceding word, which was not selected or referenced in any way, is important in inferring the intent behind the selection. A word processor will not have knowledge that Subject: is an important part of a memo, or that Re: indicates a memo in response to a previous memo. The selection will most likely be interpreted as "the 2nd through last words of line 3", or possibly even "characters 34 through 54."

Given the paucity of information in a recording of user actions, how can a PBD system get the additional information it needs to make the correct generalizations? The systems described in this book will present a variety of solutions to this problem. For instance, SmallStar has the user select from a fixed list of alternatives, Eager compares multiple examples, Turvy lets the user point to relevant information, and Peridot asks the user to verify its interpretations.

In addition to inferring data descriptions, the other main type of inference that PBD systems must make is about flow of control, since repetitive activities often include special cases that must be handled differently. This presents two complications for PBD systems. First, a single recording can only show one of the multiple paths of action, so branching flow of control will necessarily require the user to redo the activity a number of times. Second, the process that users go through in deciding which branch to perform is almost always a hidden mental process, and it is difficult to acquire information about how that decision is made.



Figure 9. Inferring intent in a selection
The systems described in this book present a variety of solutions to the problem of flow of control. For instance, Tinker has users write programming language expressions to determine which path to follow, and Metamouse infers branches automatically from multiple examples.


User Centered Systems

The authors of this book care about empowering end users. All of the systems described here approach PBD from a user-centered perspective. The common thesis that informs their work is that the success of a PBD system depends far more on the user experience of interacting with the system than it does on the induction algorithms used to create the users' programs. I hope you find the ideas presented in the coming chapters to be provocative and enlightening.


back to ...  Table of Contents  Watch What I Do


Foreword
Alan Kay







I don't know who first made the parallel between programming a computer and using a tool, but it was certainly implicit in Jack Licklider's thoughts about "man-machine symbiosis" as he set up the ARPA IPTO research projects in the early sixties. In 1962, Ivan Sutherland's Sketchpad became the exemplar to this day for what interactive computing should be like--including having the end-user be able to reshape the tool.

The idea that programming should be set up so it could be metaphorically like writing is harder to track down, but you could see it in Cliff Shaw's JOSS from the same early period. Besides being the first real "end-user" language, and the first attempt at a really "user-friendly" interface, it included a special terminal design adapted from a high-quality IBM electric typewriter that printed in two colors in lower and upper case on drilled fanfold 8*11 paper so that the output was a direct extension of one's notebook.

The vague term "computer literacy" also surfaced in the sixties, and in its strongest sense reflected a belief that the computer was going to be more like the book than a swiss army knife. Being able to "read" and "write" in it would be as universally necessary as reading and writing became after Gutenberg. The Dynabook idea was a prime focus during this time as the kind of thing computers were going to turn into, forced by engineering possibility and sociological necessity.

The analogy to reading was the easiest to see. If "reading" is the skill to be able to understand and use messages represented as gestures in a medium whose conventions are in close agreement between writer and reader, then the equivalent of reading on a computer would require the invention of a user-interface language that could universally frame the works of many thousands of authors whose interests would range far beyond those of the interface designers. "Writing" on the other hand requires the end-users to somehow construct the same kinds of things that they had been reading--a much more difficult skill.

At first this was so analogous to designing a programming language that many "interactive" interfaces were designed--some trying to improve on JOSS' dialogue scheme, while others attempted to build artificially intelligent agents that could turn advice into generalized actions. McCarthy's "advice-taker" idea had a huge influence on everyone's ideas. The notion of "programming by example" arose--perhaps the earliest was Teitelman's PILOT system, in which he tried to build an advice taking system (using a pattern matching production system) that could recapitulate the early AI theses at MIT.

Wally Feurzig and Seymour Papert had a different notion about the place of computer "reading" and "writing": that like the reading and writing of books, it wasn't just about getting and conveying information, but the very act of learning and doing them expands one's horizons and adds new ways of thinking about the world. In other words, programming could be good for people, and thus some effort should be put into designing systems that would have pedagogical benefit for both children and adults.

These ideas resonated strongly with me, partly because of my background in music, biology and mathematics. Computer processes coordinating in time go beyond Kepler's music of the spheres to a "music of metaphysics". A very similar metaphor is that of cellular and developmental biology which goes far beyond the classical Newtonism of 19th century science to a much more involved and inherently nonlinear systems organization. To have a medium that could be read and written at this new level of complexity--a level in which many of the "gotchas" of our civilization and science reside--seemed tremendously important. And still does.

Many such considerations eventually led to the realization that "it wasn't a language, but an environment", and this led directly in the early seventies to the overlapping window and pointing interface coextensive with objects that could send messages to each other and thus model any dynamic system. But the "writing" problem still remained--in part, because it was not even clear what the writing problem was.

The interface design was strongly influenced by the multiple mentality ideas of Jerome Bruner, in which the "middle" mentality, the iconic one, was the bridge between infancy and adolescence. Inspired by a few early examples--such as Paul Rovner's AMBIT-G--we decided to concentrate our research on iconic programming. Still not knowing what it meant, we dealt with it in the traditional manner for handling very difficult problems. Namely, give them to graduate students and tell them they are easy.

The first of these was Dave Smith, and his PYGMALION became the new exemplar for what iconic programming by example might mean. A host of others from our group followed--including Alan Borning's Thinglab, Laura Gould and Bill Finzer's Programming By Rehearsal, Dan Halbert's SmallStar, and Dan Ingalls' Ariel (a later version was called FABRIK). By this time a community had formed with Henry Lieberman's TINKER taking an important new path.

Today, we have windowed interfaces everywhere, and even a number of iconic object-construction kits. We have macro capture systems of every kind, and scripting languages. But we don't have "end-user programming". Nor do we have "programming by example".

One of the problems is range. By this I mean that when we teach children English, it is not our intent to teach them a pidgin language, but to gradually reveal the whole thing: the language that Jefferson and Russell wrote in, and with a few style shifts, the language that Shakespeare wrote in. In other words we want learners of English not just to be able to accomplish simple vocational goals, but to be able to aspire to the full range of expression the language makes possible. In computer terms, the range of aspiration should extend at least to the kinds of applications purchased from professionals. By comparison, systems like HyperCard offer no more than a pidgin version of what is possible on the Macintosh. It doesn't qualify by my standards.

Programming by example adds yet another burden to that of end-user programming. The user's intent as expressed in examples is to be divined by the system and turned into a useful generality. Humans have a very large range of intents, and extreme context restrictions have to be invoked to recognize any but the simplest--note that the windows and other devices of the overlapping window interface serve to restrict context while giving the user the illusion of freedom. Many of the most useful PbyE systems use the highly restricted windowing environment to great advantage.

At some point we can expect to see large complex models of human common sense and goal structures--as predicted by McCarthy long ago and slowly being realized by Doug Lenat's CYC system--getting coupled to user interfaces whose goal it is to produce in the user a style of action that will permit most goals to be recognized and automatically completed. Whether this can actually be done without requiring the user to tell the system what the goal is remains to be seen. Humans are not all that great at recognizing and understanding each other's goals, but perhaps our ego-centeredness can be left out of the artificial system.

In any case, I think the most important issues regarding end-user programming and its subbranch of programming by example are pedagogical and ethical. There is no question that a human with a goal wants to have the sub-goals ready made and at hand. One shouldn't have to learn about Carnot cycles of internal combustion engines--or even just hand cranking it--in order to drive an automobile. And agents that can be told goals and can go off and solve them have been valuable and sought after for as long as humanity has endured.

On the other hand, it takes a very special value system for children and adults to be able to exist as learning creatures--indeed as humans at all--in the presence of an environment that does all for them. 20th century humans that don't understand the hows and whys of their technologies are not in a position to make judgments and shape futures. At some point it is necessary to understand something about thermodynamics and waiting until then to try to learn it doesn't work. Nature's rule is "use it or lose it"--most social systems that have incorporated intelligent slaves or amanuenses have "lost it". In fact most never gained it to lose. In a technopoly in which we can make just about anything we desire, and almost everything we do can be replaced with vicarious experience, we have to decide to do the activities that make us into actualized humans. We have to decide to exercise, to not eat too much fat and sugar, to learn, to read, to explore, to experiment, to make, to love, to think. In short, to exist.

Difficulties are annoying and we like to remove them. But we have to be careful to only remove the gratuitous ones. As for the others--those whose surmounting makes us grow stronger in mind and body--we have to decide to leave those in and face them.



Preface






This book grew out of a workshop on Programming by Demonstration that was held at Apple Computer in March, 1992. The workshop was an opportunity for current researchers to discuss their work with the pioneers in the field. David Smith demonstrated a HyperCard simulation of his Pygmalion system, which was the first system for programming by demonstration and the inspiration for the work that has followed. Henry Lieberman ported his classic Tinker system to the Macintosh so that he could give a live demonstration at the workshop. This was followed by classic videos of the early systems, live demonstrations of the newer systems, and open discussion on topics in the field.

The participants found the workshop to be very rewarding, largely because it had always been difficult to access the relevant papers, articles, books, and videotapes describing these systems. Given the recent widespread interest in end user programming, we felt that a larger audience could benefit from this material, so we decided to republish it in a book. From that original plan, a rather different book has resulted. Instead of simply republishing their original articles, most of the authors have either written completely new chapters or have updated and extended their articles.

Furthermore, we have included two additional sections in the book: Section II discusses particular aspects of programming by demonstration in greater detail, and Section III provides broader perspectives on the field.

The Appendices are particularly valuable: in addition to a chronology and a glossary, there is an extensive test suite which lists a wide variety of specific tasks that researchers feel are amenable to programming by demonstration. In addition to demonstrating the potential of PBD, this test suite can also serve as a standard for measuring the capabilities of a given system.

The March event was called the "Programming by Example Workshop". Several of the participants observed that first-time hearers were more likely to understand what this field was all about when it was termed "Programming by Demonstration", and we have therefore all modernized our vocabulary.

This book is not only intended for individuals who are actively working in the field of programming by demonstration. We have aimed to make this material accessible and interesting to a larger audience: students and researchers with an interest in end user programming, and individuals interested in user interface design and agent-based systems. It is not a book about machine learning or artificial intelligence. Rather, the focus is on ways to create the appropriate human-computer interaction so that end users can gain more control of their personal computers.

I would like to thank the External Research Group at Apple Computer for their generous support of the workshop, and Mark Miller and Rick LeFaivre of the Advanced Technology Group at Apple Computer for their continued and enthusiastic support of this book. I would also like to thank Yin Yin Wong for her help in designing the layout for this book. Finally, I would like to thank CE Software for their QuicKeys program, which allowed me to automate many of the dreary tasks involved in editing 30 chapters.

Allen Cypher



Introduction:
Bringing Programming to End Users
Allen Cypher





The motivation behind Programming by Demonstration is simple and compelling: if a user knows how to perform a task on the computer, that should be sufficient to create a program to perform the task. It should not be necessary to learn a programming language like C or BASIC. Instead, the user should be able to instruct the computer to "Watch what I do", and the computer should create the program that corresponds to the user's actions. This book investigates the various issues that arise in trying to make this idea practical. The first section of the book describes 18 computer implementations of Programming by Demonstration, and the second section discusses the problems and opportunities for Programming by Demonstration (PBD) in more general terms.


Why Would Users Want to Program?

The first system for Programming by Demonstration, David C. Smith's PYGMALION, was written in 1975, but it was not until the more recent emergence of a large population of personal computer "end users" that the approach began to attract widespread attention. In the 1960's, computer users were either programmers themselves, or they had programmers who wrote programs specifically for them. This meant that users had their own custom applications -- programs designed specifically for their task. Large insurance companies had their own custom accounting programs, and large businesses had custom inventory programs.

With the advent of personal computing, this all changed. Now the local dry cleaners' uses a personal computer to handle sales with SalesPoint and to prepare advertisements with MacDraw. Parents are using personal computers at home to manage their finances with Quicken, and their children are writing essays in Word. Instead of using a custom application developed by a nearby programmer for a very specific task, people use a generic application developed by a distant, unknown and unreachable programmer to handle tasks similar to theirs.

Contemporary computer users are "end users", meaning that they are at the end of the process of computer programming, far removed from the programmer. The programs they are using were not written with their particular needs in mind. The programmer who created Quicken doesn't know that you work at a second job on the weekends, or that you keep a separate checkbook for household expenses. The programmer who created Word doesn't know that your English teacher requires book titles to be underlined in your bibliographies. As a result, end users must map their activities into the capabilities of the generic applications. It is inevitable that this mapping process will involve tedious steps that could be automated if only the end user were the programmer who had created the application.

Every week or so, I record my latest credit card charges in Dollars and Sense, a home accounting program. Before I can enter any charges, I have to do the following: I select the "Edit Transactions" command from a menu. A dialog asks me for the Funding Account and the date range. I scroll through a list of about 20 accounts to find "MasterCard". Then I type in the first day of the current month as the "From" date. Had this program been written specifically for me, it would have had a button labeled "Add MasterCard Charges" that would perform all of these steps. But since this is a generic program, intended for thousands of people with varying accounting needs, an "Add MasterCard Charges" button would not make sense. As a result, I am stuck performing a whole sequence of actions instead of just one, because I am using a generic program to perform a specific task.



Figure 1. The "Preferences" dialog for MacWrite II.


End User Programming

This leaves personal computer users in an ironic situation. It is a truism that computers are good at performing repetitive activities. So why is it that we are the ones performing all of the repetition, instead of the computer? Solutions are needed which enable end users to create their own custom commands. The various techniques for achieving this goal are generically referred to as "end user programming". Note that these techniques need not be programming per se: rather, they need to achieve effects that can currently only be achieved through programming. The current approaches to end user programming can be lumped into four categories: Preferences, Scripting Languages, Macro Recorders, and Programming by Demonstration.

Preferences (see Figure 1) are pre-defined alternatives supplied by an application designer to accommodate the varying needs of several different types of users. By choosing one of the pre-defined alternatives offered as a preference, a user can get the application to respond in the way that is most appropriate to his or her working style. Preferences are necessarily restricted to situations that the application designer is able to foresee, and they cannot accommodate the highly idiosyncratic needs of individual users. For instance, there will never be a preference option to "send invoices to all California customers on the 1st of each month, and to all other customers on the 15th". Another limitation of preferences is that at some point, with an overabundance of options, any system which offered all conceivable options would end up with option sheets with hundreds of entries, and it would be untenable for users to find and select their desired choices.

Since preferences offer only fixed alternatives, they are not really general enough to be considered a form of programming.

Scripting Languages (see Figure 2) are currently a popular approach to end-user programming. A scripting language is a small, simple programming language whose vocabulary is specifically tailored to the objects and actions of a particular application domain. The hope is that such a language will not be too difficult for end users to learn. Scripting languages are indeed less daunting than the standard general-purpose programming languages. For instance, in the HyperCard application, the scripting language description for the location of the message box is "the location of the message box", while the corresponding description in Pascal is

"GlobalToLocal(messageWindow^.portBits.bounds.topLeft)".

However, many of the standard programming difficulties remain. To make the box go away, you must use the command Hide the message box. If you write Close the message box, you get the error message "Can't close that window". You can write Hide the message box or Hide "the message box", but not Hide the "message box". And of course, for all of these commands, you have to know that this object happens to be called the "message box".

on JumpToStack
put the name of this stack into NameOfStack
go to card 1 of stack Home
doMenu "New Button"
set the style of button "New Button" to roundRect
put "on mouseUp" & return into jumpScript
put "go to " & NameOfStack & return after jumpScript
put "end mouseUp" after jumpScript
set the script of button "New Button" to jumpScript
set the name of button "New Button" to "Go To " & ~
NameOfStack
choose browse tool
end JumpToStack

Figure 2. A HyperCard script.
In summary, the basic failing of scripting is that it is still programming. That is, 1) users have to learn the arcane syntax and vocabulary conventions of the language, and 2) they have to learn the standard computer science concepts of variables, loops and conditionals. For a significant number of contemporary computer users, be they history students, real estate agents, or shop owners, the hurdle of learning a scripting language is simply too high.



Figure 3. A recorded QuicKeys macro to add page numbering to a Word document. The macro consists of a menu selection followed by three mouse clicks. Executing the "Print Preview..." menu command displays a page of the document as it will appear when it is printed. The first Click selects the "Page Number" tool. The second Click places the page number in the desired location on the page. The third Click is on the "Close" button, removing the "Print Preview" display.
Macro Recorders (see Figure 3) provide users with a way to record their actions. These recorders are a basic implementation of "Watch what I do". The user issues the "Start Recording" command, performs a series of actions, and then issues the "Stop Recording" command. All of the user's actions are saved as a sequence, and the user can then invoke a "Redo" command to replay the entire sequence. Many spreadsheet programs and telecommunications programs have built-in macro recorders. For example, the spreadsheet user can automate selecting cells Al through A15 and copying them to cells E20 through E34. And the telecommunications user can automate the sequence of dialing an on-line service, typing in the login name and password, and selecting a particular bulletin board. There are also some system-wide macro recorders. KeyWatch, on IBM PC's, will detect and automate a repetitive sequence of keystrokes. QuicKeys, on the Macintosh, can automate a sequence of mouse selections, mouse drags, menu selections, and keystrokes. For instance, by recording the action of dragging the icon "November Advertisement" to the icon of a "BackUp" disk, the user can automate the process of making a backup copy of this document.

The main failing of macro recorders is that they are too literal. They replay a sequence of keystrokes and mouse clicks, hereas most repetitive activities are repetitive at a somewhat higher level of abstraction. Rather than selecting cells A1 through A15, the user may actually want to select this month's employee sales, which may now occupy cells A1 through A16 if a new employee has joined the department. After logging on to an on-line service, the user may want to download all of the new messages related to Bay Area restaurants. But the specific actions required to accomplish this download will vary as the contents of the bulletin board varies from day to day. Finally, as illustrated in Figure 4, when the user records the command Drag from Screen Location (161, 90) to Screen Location (209, 201) to make a backup of the document "May Advertisement", it may happen that a replay of that precise action instead moves the document "Phone Numbers" into the "Price List" folder, because the windows and icons on the screen are now in different locations.

 Record Replay

Figure 4. Replaying a macro can have unexpected results.
Programming by Demonstration (see Figure 5) is an elaboration of the idea behind macro recorders. Once again, the user instructs the system to "Watch what I do", but with programming by demonstration, the system creates generalized programs from the recorded actions. Instead of selecting cells A1 through A15, a generalized program can select all of the rows before row "Total". Instead of downloading messages 1, 4, and 7, a generalized program can download all messages marked "unread". Instead of dragging from one screen location to another, a generalized program can make a copy of this month's Advertisement document, regardless of where its icon is located on the screen.

Furthermore, generalized programs can contain iterative loops and conditional branches. It is possible to demonstrate how to transfer one address from an old address book into a new address book and have a programming by demonstration system transfer all of the addresses in the book. It is possible for a PBD system to create a program that only transfers address cards where the state is "CA" or "California".



Figure 5. Metamouse assists the user in repositioning lines.
The greatest advantage of Programming by Demonstration over conventional programming is that it is "Programming in the User Interface" - a term coined by Dan Halbert. Conventional programming requires the programmer to map from the visual representation of objects being moved about the screen into a completely different textual representation of those actions. By Programming in the User Interface, users can refer to an action by simply performing the action, something they already know how to do. They are programming in the same environment in which they perform the actions. With conventional programming, they must learn a second, abstract, technical and alien way of referring to objects and actions.


What Do Users Want to Program?

There is a wide range of user-programming needs that can potentially be satisfied by programming by demonstration. The simplest types of activities involve making little changes in an application so that it more closely fits one's personal needs. These changes may be called tweaking, and they often correspond to setting preferences that the application designer did not foresee.



Figure 6. A perennial dialog box.
For instance, whenever I open a "package" in my mail program, the program displays a dialog box which asks me whether I want to delete the package (see Figure 6). I always want to delete it, but every time I have to click "Yes". I would like my mail program to delete packages without asking. Since the designer did not have the foresight to include an "Always delete packages?" preference, I must resign myself to this annoying "feature". I would like to be able to record the action of clicking "Yes" in the "Delete Package?" dialog box, and have this program automatically run whenever the dialog appears. This would be almost as good as not being asked at all. A macro recorder could record the action of clicking "Yes", but there is no way to automatically invoke the macro whenever this particular dialog box appears.

Probably the largest potential use for programming by demonstration is for automating repetitive activities. Users commonly have to perform iterative activities, such as renumbering a long list when a new entry is inserted in the middle, and they also have to perform periodic activities, such as backing up recently changed files. A graphic designer showed me a good example of a task that needs automating. This designer is responsible for a large computer reference book, and she had recently decided to use a different font for the figures in the book (see Figure 7). The new font was somewhat larger, so the labels -- words with a box around them -- no longer fit in their boxes. The designer had to reshape all of the boxes in more than 120 figures in the book. She would have been much happier to create a program by demonstrating how to reshape a few of the labels, and then use that program to automatically reshape the rest.



Figure 7. A small part of a figure containing text in boxes. All of the boxes containing text have to be reshaped.
The third potential use for programming by demonstration is for building mini-applications. Sometimes users' needs are unique enough that commercially available applications are not appropriate. HyperCard has been successful in addressing some of these needs, but it requires that users know how to write programs. I know a programmer who wanted an address book that would allow him to keep several addresses and phone numbers for each person -- work and home addresses, for instance. He used HyperCard to create his own custom address book (see Figure 8). It would be wonderful if PBD permitted non-programmers to create custom applications like this.


What Makes PBD Difficult?

This book is dedicated to taking advantage of the fact that users know how to perform tasks themselves. This ability should be very helpful in creating programs that perform those tasks. But there is more to a program than meets the recording. Users are quietly and imperceptibly making decisions. They are searching by eye. They are reading and understanding natural language. They are interpreting the interface in terms of their goals.



Figure 8. A custom Address book.
Inferring Intent

The main challenge confronting Programming by Demonstration is how to infer the user's intent. In order to convert a recorded action into a program to perform that action, the system needs to determine the user's intent in performing the action. When the program is executed in the future, the context will be somewhat different, and it will be necessary to perform the action that is the equivalent of the recorded action in this new context. The process of inferring a user's intent in selecting a particular object has been described by Dan Halbert as creating a "data description".

When the user selects Re: meeting next week in the mail message in Figure 9, the user's intent could be to 1) select the subject of the message, 2) select the first four words of the subject, 3) select all but the first word of the third line, 4) select any subject beginning with Re:, 5) select all subjects related to meeting next week, and so on.

If this text is a memo written in a word processor, the application will have no special knowledge about memos. When the user selects Re: meeting next week, there is no indication that the preceding word, which was not selected or referenced in any way, is important in inferring the intent behind the selection. A word processor will not have knowledge that Subject: is an important part of a memo, or that Re: indicates a memo in response to a previous memo. The selection will most likely be interpreted as "the 2nd through last words of line 3", or possibly even "characters 34 through 54."

Given the paucity of information in a recording of user actions, how can a PBD system get the additional information it needs to make the correct generalizations? The systems described in this book will present a variety of solutions to this problem. For instance, SmallStar has the user select from a fixed list of alternatives, Eager compares multiple examples, Turvy lets the user point to relevant information, and Peridot asks the user to verify its interpretations.

In addition to inferring data descriptions, the other main type of inference that PBD systems must make is about flow of control, since repetitive activities often include special cases that must be handled differently. This presents two complications for PBD systems. First, a single recording can only show one of the multiple paths of action, so branching flow of control will necessarily require the user to redo the activity a number of times. Second, the process that users go through in deciding which branch to perform is almost always a hidden mental process, and it is difficult to acquire information about how that decision is made.



Figure 9. Inferring intent in a selection
The systems described in this book present a variety of solutions to the problem of flow of control. For instance, Tinker has users write programming language expressions to determine which path to follow, and Metamouse infers branches automatically from multiple examples.


User Centered Systems

The authors of this book care about empowering end users. All of the systems described here approach PBD from a user-centered perspective. The common thesis that informs their work is that the success of a PBD system depends far more on the user experience of interacting with the system than it does on the induction algorithms used to create the users' programs. I hope you find the ideas presented in the coming chapters to be provocative and enlightening.


back to ...  Table of Contents  Watch What I Do



NL

Skip navigation
Search



Create


Avatar image
Next:
ROTP: How to easily build agentic swarms in Kilroy Part 1
How to Build AI Agent Pipelines in Kilroy
1 / 6

Transcript


Search in video
0:00
Yeah, here we are. What's up, buddy? Well, long time no see.
0:09
Let's just hope my internet connection stays up today. I will be buying redundancy. Um, all right. So, we're uh
0:17
we're we're doing this show today because we wanted to introduce folks to a little bit of the philosophy behind
0:22
Kilroy as well as a little bit of what's possible um in building in Kilroy, at
0:29
least as it relates to AI. Um but why don't you quickly um tell a little bit
0:36
about uh what you think the best way to describe Kilroy is? I knew you were going to do this to me.
0:42
Um so, I'll give you my stock answer to when somebody asks me what is it and
0:48
then I'll tell you really what we're doing. So it's it's the elephant with the three blind men, right? One guy feels a rope, one guy feels a tree
0:55
trunk, and another guy thinks he's up against a barn wall because he can't really see the whole thing. And it's a big animal. So we've got this big
1:01
animal, but what it fundamentally is is a way for you to get rid of your dependence on centralized stuff and
1:09
build an internet the way that you want to do it. Right? So, it's an application that runs on your desktop and allows you
1:16
to pick what you see, who you talk to, how you interact with external services
1:21
down to even getting rid of the user interface on a centralized website or not having to talk to a big centralized
1:28
AI like chat GBT or cloud or uh being able to put together your own services
1:33
and monetize them for other people. So, it's all about really taking the
1:39
centralized internet that we've had enforced on us for the last 20 years and getting back to what it used to be,
1:45
which was everybody's on their own platform. I don't know. How was that? That was good. That was good. I mean, I
1:51
I I think it goes way beyond that, but I think that's a core
1:56
uh function of what it does. Absolutely. So yeah, look, I mean, I think um I I
2:02
think the decentralization piece is really critical. I think we've kind of identified the dangers of
2:08
decentralized money pretty efficiently in this space, but we don't have decentralized user interfaces to crypto
2:15
and um the dangers of of centralized AI to me are an even greater threat to
2:21
society. So um I think this is really important as well. And Kilroyy's peer-to-peer and swarming capability
2:27
makes it so like you said, anybody can build their own internet. But built into
2:32
that platform is automation and integration and agentic AI with the
2:39
visual creation platform um that is also turning complete. So um I think there's
2:46
a lot for people to understand about that. I I think one one point that raced
2:51
by there that it's worth going back to and talking about is the centralized AI piece. There are actually some things
2:57
that you can do on your desktop with Kilroy and a whole swarm of AI agents
3:03
that you couldn't begin to do with a web page talking to chat GBT for example. They're just things that you are not
3:09
able to build or interact with as a normal enduser. maybe as a, you know, a
3:15
computer scientist with, you know, ability to sit down and hack out your own code and glue the pieces together
3:21
and use some of the, uh, sort of janky offtheshelf open source pieces, you
3:27
could make it do the same thing. But for a normal user who wants to sit down and be a power user of AI, there's it's
3:34
night and day with Kilroy. I mean, there's things that you can build on your desktop that you could never try to do in a centralized service.
3:41
Yeah, absolutely. and and there are a ton of agent platforms out there and a
3:48
ton of um attempts at making AI more accessible and more powerful. But at the
3:56
end of the day, the ability for our system to not only be easy to build in,
4:01
but also to be able to connect and utilize resources is really powerful. For example,
4:07
um whether centralized AI or local AI, um the ability to easily share LLMs with
4:15
other people doesn't really exist in this world. There are blockchain solutions like AOS that that allow you
4:22
to to utilize uh LLM and servers that people make available and there are
4:28
obviously LLMs you can install on your machine. But with our swarming capability, it makes it so that you can
4:34
utilize LLMs on server infrastructure without jumping through a ton of hoops. It's just the swarmed connections are
4:40
together. I'm sorry. Go ahead. No, go ahead. I was going to say, let's talk a little bit because we keep saying this word
4:47
swarm and this peer-to-peer stuff, but we haven't really described what that means. Sure. Right. So, in a normal interaction with
4:55
an AI like chat GPT, you're using a web browser as a client to talk to a server
5:01
somewhere off in the cloud that's being run by OpenAI, right? And everything that you type on your screen is getting
5:06
sent to that server and it does some work and it sends you the answer back. And you can open two windows and talk to
5:13
two different agents, but that's sort of the amount of of interactivity that you get. Um, since Kilroy runs on your
5:20
desktop, Kilroy can talk to OpenAI servers, it can talk to Google servers,
5:25
it can talk to Microsoft servers, but it most importantly can talk to uh AI
5:32
execution platforms that might be running locally on your own machine or on an old laptop on your desk or on a
5:38
friend's computer, right? And the way that we do that is we have built into Kilroy this sort of seamless,
5:44
transparent, completely userfriendly way to hook any number of machines
5:51
together in a peer-to-peer cloud. If you've ever played with something like Bit Torrent or back in the day Napster
5:57
where there was hundreds of computers that were all collect connected together uh pushing information around between
6:03
them, um that's exactly what Kilroy does behind the scenes. If I'm in a a a swarm
6:09
called, you know, my spiffy AI, and I give Brad that name, Brad can join the same swarm, my spiffy AI, and now his
6:16
agents can talk to my agents and vice versa. We can set up chat rooms. We can build applications on top of this shared
6:22
infrastructure. And Kilroy handles all of the behind-the-scenes networking. You don't have to play with your firewall.
6:28
You don't have to set up port forwarding. You don't have to have a static IP address. You don't have to use a domain name that you've got to get
6:34
configured. they just talk. And so that actually becomes the mechanism that we
6:40
use inside of Kilroy for everything to talk amongst itself. Now Kilroy looks sort of like a a highlevel operating
6:48
system to all these agents. Kilroy can start and stop the agents. It can set up communications pathways between them. It
6:54
can run things on a periodic basis. uh it can integrate to the outside world with connections out to anything from as
7:02
simple as a chat service like telegram to a big backend database somewhere or to another AI in the cloud. So the the
7:10
whole point of the platform is let's get rid of all the hard stuff that makes
7:16
AI interacting with each other and with the real world painful. all the networking, all the configuration, all
7:21
the tool configuration, all the parameters that have to be passed back and forth. All that stuff is hidden and
7:27
instead you just get some nice little blocks that you can drag around on the screen and hook up. That's perfect. I love that. I love
7:34
that. That's a great explanation. That's excellent. So maybe maybe we should look at some
7:39
examples. What do you think? Good. Yeah, let's do it. Um, unfortunately, it looks like Pineree is
7:46
not working for us. It's a bummer. So, we're not getting Farcaster folks.
7:51
Nope. I'll just have to upload it. Okay. Well, they'll just have to wonder what they're missing. You know, I I
7:58
think while you're do getting that loaded up, I'm going to talk a little bit about kind of philosophically why, you know, um I I think one of the things
8:05
that for people to understand that's important for people to understand about why we are so concerned about
8:10
centralized AI providers like OpenAI, Google, Microsoft, whomever um is that
8:17
we believe that all of compute will eventually be AI or most of it. And so
8:23
what that what that results in is that every aspect of your life is then going
8:28
into whatever AI provider you choose whether it's your local provider you or it's a it's a service a centralized
8:35
service like open AI that also means that all the information all the news all the education that your children may
8:42
be getting also is going to be coming through these AI providers and at some
8:47
point you have to ask yourself who controls what is or is not in those
8:53
models and who controls what comes in and comes through and what we see and what we interact with and who controls
8:59
access to what you are putting into it and so that's one of the biggest concerns we have and if and you know I
9:05
firmly believe that if you think that decentralized money is dangerous um centralized AI is you know every
9:12
dystopian movie and book that's ever been made about centralized money right sorry centralized money um every
9:19
dystopian book or movie about AI and some horrible corporate o overlord or
9:25
government overlord um over controlling your life um only comes through
9:31
centralized AI. And so from from my perspective that's a really critical
9:36
thing and it's really in my mind more critical and dangerous than centralized money. So
9:42
now that you've scared everybody there you go. Show show them how not to be scared.
9:48
There you go. Excellent. empower the individual. All right. So, what you're looking at right now is my desktop and
9:54
everything that you're going to see is running locally on my Mac Studio. The the Kilroy application, the Kilroy user
10:02
interface, the connection to a local uh AI platform. All the stuff that you're
10:08
going to see initially is all locally locally provided um on one machine. Now, I've got other machines here that are
10:14
running other copies of Kilroy and doing things, and we've got some stuff that we're going to hook up to in the cloud, but for the most part, everything you're
10:20
going to see for the next few minutes is all on my machine. And it's not even a very beefy Mac Studio. I think just
10:26
about any normally modern, reasonably complete
10:31
CPU stack, whether it's Windows or PC, Windows, Mac or Linux, should be able to
10:36
do what you're going to see uh for these little dogging and pony shows here. Um, all right. So, Kilroy is, like I said,
10:43
sort of an operating system for little applications. And if I go look in the sidebar here on Kilroy, we have lots of
10:50
little applications that are running and doing things. uh things that are related to AI, things that are related to crypto
10:57
exchanges, RSS feeds, DeFi services like Vei, uh interacting with web browsers
11:03
and NFTTS, automated trading platforms, RSS reader down here called Miniflux,
11:09
some decentralized AI for the old Spirit Swap project on Phantom, weather services interacting with Twitter, uh
11:15
embedded web widgets. There's all sorts of stuff that you can do that we've already built that you can drag into
11:20
your copy of Kilroy and use. But what we want to start with uh because it's one
11:26
of the cooler toys that we have is the the AI platform. So I'm going to open up the editor that's built inside of this
11:33
uh Kilroy AI application. And it's a drag and drop editor, big white expanse
11:39
of of nothingness. And so let's start with something that's uh pretty simple
11:44
um which is just a little user interface to talk to Oops, did that wrong. A
11:51
little user interface to talk to um we don't want to do this one.
12:01
I have Llama 3 running on my desktop as a local LLM
12:07
and the may want to well I guess most people that watching would know what Ola is but
12:13
we'll we'll talk about it for a second and how Olama and Llama 3 work while I go find where I put my pipeline folder.
12:19
So Olama is an inferencing engine, an inferencing platform. Um, it's an open-
12:24
source platform and it's one we're currently using for model inferencing. And what that means is when you send
12:32
info to an AI, you're sending it to an inferencing engine that's interacting with the model. Um, that's a really
12:39
simple way to explain it. And OAMA can run locally on your own machine. So even if you don't have a beefy GPUbased
12:46
machine, there are a ton of small models that you can utilize um to utilize some
12:53
functionality of AI via Olama. There are a ton of others, LM Studio, um other
12:58
products that um allow you to do this as well. We like we like the way it functions. Um but the one of the things
13:05
in terms of small um LLMs that's uh will be important in the future for Kilroy is
13:10
is that because we're able to to distill the functionality of agents into small
13:17
tiny bitsized pieces in these um peer-to-peer networks and swarms, we actually have the ability to offload
13:24
or to put a lot less um work on the LLM in any given task that it's doing. so
13:32
that we won't need as much processing power. And Chuck's favorite LLM is a
13:37
really old and really small one called Llama 3, which by by Meta by the Facebook people. Um, and he uses that
13:44
for everything. And it's a small LLM that will run on a lot of different machines, and it's actually not even the
13:51
most up-to-date LLM that Meta has, but it does a really good job. It doesn't hallucinate, and it knows how
13:57
to do math. Those are two things that are majorly in its favor. And if you've used any of the modern newer LLMs that
14:04
are online, ChatGpt or Claude or any of those, they go to great lengths to explain to you what you're doing. And
14:10
for most of the tasks that we're doing, we know what we're asking the LLM to do. We don't need an explanation. We just
14:16
need an answer. So you might say, "Hey, go get me a quote for a particular cryptocurrency." It doesn't need to come
14:22
back and give you an explanation of what a cryptocurrency is and five paragraphs later show you what the current price is. We just want to see the current
14:28
price, right? or it might be, hey, you know, go add these two numbers together and if it's bigger than a certain amount, go do the next thing. the the
14:35
the difference and I think it's kind of it's hard to see until you see some of the agents that we're going to show in
14:40
just a minute or some of the pipelines is that what would normally be two or
14:46
three pages worth of prompts into a chat GPT like web interface to try to get it
14:51
to go through multiple steps and do what you want are all one or two sentence prompts that are really small that let
14:57
these little local LLMs do one little job and then hand off their work product through the swarm to the next one does
15:04
the next step in the job. And sometimes there may be tools or transformations in the middle of those steps, which is not
15:11
something that you can do with the big centralized models very easily. Do some work, transform it with a tool, do some
15:17
more work, send it off to an a cloud service, get an answer back, do some more work. That that pipeline style
15:23
workflow is is very challenging to do as an enduser with the existing consumer
15:29
focused interfaces. So, all right. So, let's switch back to what's on the screen now. Now that I found the folder with the example in it that I wanted. So
15:36
this is the visual editor for doing AI pipelines in on top of Kilroy. Now
15:43
remember just a second ago when I showed you all the apps that were running in the sidebar of Kilroy. This is just an
15:48
app on Kilroy. This editor, the building of the agents, the execution of the agents and letting them talk through the
15:54
swarms is all just what's the equivalent of a thirdparty app on top of the Kilroy platform. So if you don't like this, you
16:02
can build your own. If you want to change the way it behaves, you can modify it. But for right now, what we have is a pallet full of some pre-built
16:09
parts. Um, we have agents, AI agents like this, this one here, which are
16:15
frontends to talk to an external AI. It can be an external AI like chat GPT. It
16:21
can be a local AI like Llama 3 running underneath O Lama on your own local
16:26
machine. Whatever you decide to configure in these parameters for the URL and the API key and the model name
16:32
and the temperatures and other stuff like that is what the pipeline's going to be started with. And then we have
16:38
other things for interacting with the pipelines. We have chat agent which is basically a user input screen. It looks
16:44
like a terminal window. It looks like the web interface that you use to talk to uh a chat GPT. And then in the middle
16:51
of the two is a swarm. This one is just creatively named swarm. And its whole
16:57
job is to take messages from the user input agent and pass them over to the other things in the swarm that are
17:03
listening. In this case, there's only one. It's this AI agent over here. And what that's really going to do is whatever I type on the screen is going
17:10
to go to the swarm and from the swarm over to the agent. It's going to look like a prompt string. It's going to do its work. It's going to give me its
17:15
answer back to the swarm and I'm going to listen to it and see it on the screen. These yellow labels on the edges
17:22
are basically saying who am I listening to or who am I sending as? I'm sending as a thing called chat agent. The AI
17:28
agent's listening to everything. Don't care. The AI agent is sending as a thing called AI agent. The user input field is
17:36
is just listening to everything. So let's go make that so you can see what it looks like in Kilroy and run it so
17:42
you can see how it interacts with the LOM in the background. So, we're going to build that pipeline and it's going to pop open some windows
17:48
in Kilroy really quickly. This is the user interface for the interactive portion of the screen. And this is sort
17:56
of a little debugging window to show you that the agent's really doing work in the background when it talks to the LLM.
18:02
You can hide this, show it. It doesn't matter if it's on the screen. Normally, a user would just see an interactive
18:07
window, but um I'll just ask it who is this. Now, it's
18:13
going to take a while for it to start up the LLM because we've been talking and it's gone to sleep. But, okay, here it
18:18
is. It's llama, right? This is who we're talking to. And,
18:30
you know, tell me about chocolate cake. Right. So, that's the typical interaction that you would get with the
18:37
the LLM. And yes, it's barfed out a a recipe for a chocolate cake. Here we go.
18:43
Um, and so in a couple of mouse clicks and drags in the editor, we built sort
18:48
of the equivalent of a local chat GPT. I've got a window that I can type in prompts go over to the AI and I get
18:54
answers back. Um, in some more elaborate examples, you can have you can ask it to create code and show it in another
19:00
window where it's running. So you can do vibe coding on top of Kilroy. Um, but this is a this is a the simple example
19:08
of just one little uh interactive agent. What do you think about showing the
19:13
integration with um Telegram, Brad? I can show. Yeah. Yeah, let's do that. simple one that we've got right now that I use
19:19
every day is I have a um an RSS feed
19:25
reader called Miniflux that sits out on a server for me and goes through about a
19:31
hundred different RSS feeds in real time all the time and and so its job is to
19:38
write into this swarm called Chuck Minifilelex swarm. The server out there in the cloud also knows how to talk to
19:45
Kilroy swarms and it pushes the data in through a web hook that Kilroy makes available to it. And there's a swarm
19:52
that's receiving a constant stream of RSS stories, right? Those feed into an
19:57
AI agent whose prompt is this. So system prompt says
20:03
you're an agent that transforms RSS stories into markdown messages for Telegram. and it goes into some more detail about how to construct the
20:10
telegram uh markdown message. But if you look at the end result, what I end up
20:15
with is uh a an unending stream of
20:21
scrolling RSS messages showing up in Telegram, which means even when I'm away from my desktop, I can pull out my
20:27
phone. I can look at the current news that has been pulled in from the RSS feeds and pushed over to Telegram by my
20:34
agents on my desktop. I'm not looking at some thirdparty news feed. I'm not looking at some uh you know online news
20:41
site like a you know a CNN or a Reuters or whatever. This is all my news being
20:47
picked out and selected and summarized by my AI and pushed out to my private Telegram channel.
20:52
And you can you can take it even further if you want. For example, my version of that um pulls in a bunch of feeds from a
20:58
different lot of different places and then I actually have it summarized into tweet size stories. So, it goes in, it
21:05
reads the article, it summarizes it, and then I get a feed of of tweetsiz
21:11
messages with the links in it and an engaging headline so that if I want to
21:16
repost something and comment on it, I can. So, you can take this any even further. You could have another agent in
21:22
the swarm that's actually um discerning which feeds, which stories, you know,
21:28
have some component to them that you want to be emphasized and have only those come in. So, um, there's a million
21:35
ways to expand on this, um, and interact with it. So, so let's look at let's look at one
21:40
that's a little bit more elaborate. So, we're working our way up in complexity, right? We have an RSS data coming into
21:46
an agent. The agent summarizes it as a markdown message for Telegram. Telegram shows it on the screen. And, oh yeah, as
21:52
a side effect, I can also look at it inside of Kilroy in a right in a terminal window inside of Kilroy. Um,
21:58
all of this runs over on a laptop on my desktop whose job right now is to do nothing but be my my news feed. Um, so
22:05
here's one that's more complicated. I've shown it three times now because I keep clicking the wrong tab. This one is a a
22:13
fully functional Telegram bot, right? This is uh an agent that runs in
22:20
Telegram uh a Telegram bot that talks to Kilroy in real time, sends messages from the
22:26
users in Telegram into Kilroy. It's processed through these two blue AI boxes here. Uh and the results are then
22:34
sent back to uh to Telegram for the user. Now the this is this is a kind of
22:41
an advanced leapt to the endgame here on Kilroyy's AI functionality, but this is
22:47
a really advanced model because it's got the ability for the user to change on
22:52
the fly what the system prompts are and change the behavior of the LLM using telegram bot commands. So instead of
23:00
just having one persona or one system prompt and that's how this is going to work you can see that there are several
23:06
commands that are supported and if you look closely there there's a command called uh slashpersona there's one for
23:13
uh slashhelp and there's one for slashreset and those are all hooked into this big swarm of nodes to to do the
23:21
things that you would expect. So instead of instead of uh looking at that and trying to decompose it, let me just show
23:27
you how it works. Uh who am I talking to?
23:35
So it's going to go over and uh I guess I'm I'm I'm in pirate persona. I
23:41
didn't realize that. So the Let me let me uh let me reset that again and let
23:48
you see what's really going on. We'll uh hide these guys and hide this guy
23:55
and bring back the techbot and show you. This one has lots
24:01
of pieces and out of the box it just has a generic persona, but um because I was
24:08
playing with this before we got on the call, it had switched into
24:14
cap sea captain mode. And um so if we just ask it well what are the things
24:20
that I can do right there's the normal response that you get and you can see in the
24:26
background here in Kilroy with all these other windows open you can watch these lights these blinky lights go off which
24:32
will basically give you some indication of the work that's getting done by the various uh agents along in the pipeline.
24:37
I'll shrink this down a little bit so we can see them all. But if you built a pipeline to send out or distribute to
24:42
other people, which is something you can do on our network, um then you can make it so they don't see any of that. They
24:48
just see the the primary interface or see nothing and only see it in Telegram.
24:54
So you can ask it knows about itself and it knows its own command. So I can ask it uh
25:02
things about how do I do different stuff. But if I just tell it the persona is uh Karen
25:08
and uh have it switch over to that. So now Karen persona is active. You lose
25:15
her. Let me reset it so it forgets the captain stuff and um you need to leave
25:23
our store immediately. And of course she's going to yell back.
25:29
Right. So wi with just commands and telegram I've completely reconfigured
25:34
the pipeline to have a different system prompt a different persona to change uh who I'm talking to and it didn't change
25:41
the pipeline right we didn't have to change the topology we changed the configuration of the nodes in the
25:47
pipeline that's what these red lines indicate and in a future podcast we'll go into more details about how to build
25:53
something like this but basically the flow is message comes in from telegram is it a command if it's a command go
25:59
down here to the command process processor and decide what to do. Was it the reset command? Send a reset command off to the AI. Was it a help command?
26:06
Then ask for help and put that back out to the user. If it was one of the persona commands, it comes in up here
26:12
and it changes to either the pirate, the baby, or the Karen and changes the system prompt in here. And if we uh if
26:19
you looked at the system prompt in the thing as a as it's running right now, which we can do, we can go back over
26:25
here real quick and ask it to show us the system prompt,
26:30
which says, "You're an angry angry middle-aged woman who is never happy and always wants to speak to the manager, always respond to the user in
26:35
character." If I change the if I go back to Telegram and change it to be uh
26:43
the baby persona, right? And then we go back up here and
26:50
and look at the setting for the system prompt. Now it's you're a baby, always respond to the user and character. So
26:57
behind the scenes, the commands from Telegram are reconfiguring Kilroy on the fly. This could be commands to start up
27:03
new agents. Kilroy can self-modify these pipelines. It can add in new agents. It can change the cloud the swarms that
27:09
it's talking to. It can change the system prompts. It can rewire itself on the fly based on user input or input
27:16
from the outside world. Um really and truly it's sort of Skynet in a box, right, Brad?
27:21
Yeah, absolutely. I mean, I think that's a really important thing to to emphasize here is that um we're at the point with
27:28
this ability to create these pipelines where um a user can um
27:36
we you'd be able to you'd be able to structure these things so that when an agent needs more capability, it can
27:42
actually create that capability, right? So, well, and and the really cool thing is this doesn't have to all be running on
27:48
your machine, right? If you look back at this diagram, all of these different swarms, any one of these swarms could be
27:55
shared with other Kilroy users out in the wild internet and their agents could
28:01
respond, right? So, I could have a a bot that interacts with a bot that's running on Brad's desktop and gives me some
28:08
answers back and we literally don't have to do anything but coordinate on what is
28:14
the name of the swarm that we're going to use to talk through. You could have a 100 person chat room built on top of
28:19
this with uh you know AI trading strategies for cryptocurrencies that you are watching with your crypto bros,
28:27
right? And the agents can be working together and collaborating to refine and improve different trading strategies.
28:34
You could have trading contests where agents are trading against each other and whatever one wins, you know, people
28:40
can access and utilize for their own trading strategies. Um this can be on centralized exchanges, this can be in
28:46
DeFi. Um and the key the other thing I wanted to to point out to everybody is is that the the integration with
28:53
Telegram is not unique. The this interaction and integration could happen
28:58
in Telegram. It can happen in Discord. It could happen in text messages. There
29:03
there really is no working in Minecraft, right? Yeah, we have it working in Minecraft, right? So, we built a plugin of Kilroy
29:10
into Minecraft and you can actually interact and control the Minecraft
29:15
environment via Kilroy. So, you could then have AI agents that are building in
29:20
Minecraft. You can have all kinds of things happening. Um, the key is is that the interface is irrelevant and so it
29:27
can go to wherever a user wants to live. It doesn't have to live within the Kilroy interface. It doesn't have to
29:34
live within Telegram. It can be whatever your company or your team or your group
29:39
wants to utilize as a UI. And there's nothing stopping you from creating brand
29:45
new UIs that function and live inside of Kilroy. And there's nothing stopping you from integrating any API that exists in
29:52
the world into Kilroy. Um, if there's a web hook, if there's a way to interconnect to it, you can build a
29:59
module within Kilroy and you can now make it available to agents. So maybe you have a company that has services
30:05
that you want people to be able to utilize to order pizzas all over the world. Um, you can do that. You may have
30:11
games that you want people to be able to interact with in a swarm and have thousands of players automatically
30:16
playing and competing against each other. That there really is no limit to it. And that's actually one of our
30:22
biggest problems with talking about Kilroy is well and I think Brad this the the the the key thing to take away from this is
30:29
that this is a completely novel way to put together LLMs and agents and
30:35
cooperating human users with the real world services. Nobody has done this
30:42
particular swarm-based peer-to-peer completely decentralized AI model. Yeah,
30:48
you can call tools from chat GBT and you can have MCP as your tool calling interface to get to all sorts of
30:55
services and you pretty much have to be an expert programmer to get all the interfaces lined up and the glue code
31:01
written and the pipeline built and the pipeline's very static and it does what it does. In Kilroy, any user that can
31:07
draw a picture like this by dragging blocks onto the screen and connecting the lines together can make a pipeline
31:13
that does more than you can do with the current tooling that's sort of state of the practice out there in the internet.
31:19
Yeah. And I think let's carry that over into crypto, you know, kind of our originating world, right? the the really
31:27
cool one of the really powerful and the things that attracted me to crypto from the beginning was understanding
31:33
composability, understanding the ability for components of financial services and
31:38
and crypto tools to be made accessible and ava a available to other protocols,
31:44
right? So, a lending protocol might utilize a swapping protocol within their interface or whatever. But what but that
31:52
make requires you to be a solidity programmer, right? But as we drop in modules um for DeFi protocols into this
32:00
platform, now users in this AI interface or in the regular app interface can make
32:06
use of those to build new agents, new applications, new pipelines that make
32:11
use of all kinds of protocols in DeFi onchain. The other thing that's that
32:17
this solves for crypto is one of the biggest holes we have in in the goal of
32:23
decentralizing money is that all of our interfaces in DeFi are on the web. And
32:28
so there is a centralized server and there is a registered domain that are susceptible to attack. And that has
32:35
happened to the tunes of millions and millions of dollars being stolen from domains getting hacked and being taken
32:41
over um and to servers being hacked. This eliminates that problem because you
32:46
can now interact directly with a smart contract on a decentralized UI and build
32:52
a UI that's easy for a user to use without them having to visit a website ever to interact with a protocol. And
33:00
now you can have these interconnected protocols within Kilroy that allow you to do all kinds of really powerful
33:06
things um and and make it work. And and as a one more side note, um one of the
33:11
things built into Kilroy is um and the reason we're probably going to have our own um layer two or three for Kilroy is
33:19
that the apps can be stored as NFTts. And so you can actually distribute
33:25
applications in this network and licenses in this network um in NFTTS and
33:31
enable people to easily use the app store that way and access um apps
33:36
because all of the apps in in Kilroy are essentially text files. They're JSON. So
33:43
the engine recognizes and understands the commands that drive it from a JSON file, but we don't require this this
33:50
huge um installation paradigm that we're all used to on our applications on our
33:56
on our computers. There's no executable to install. Um these are really simple
34:01
and easy and fast to install applications. So all right, we've used up our 30
34:07
minutes. All right. All right. So, um, if you happen to watch this, uh, I don't think
34:13
we're going to have a lot today, but if you happen to and you're someone that doesn't mind getting your hands a little
34:19
dirty, we're looking for folks that want to experiment with Kilroy, try it out,
34:25
install it on their machines, and help us knock off some of the rough edges, build integrations into the platform,
34:32
um, um, and build start building pipelines and agents that other people can
34:38
utilize. Um there will be a revenue stream available for for these um that that um will be a part of the entire
34:45
Kilroy platform, but we're really looking for people that can see the power of what this can do and are
34:51
interested in helping to uh build it out further. And there's a lot of opportunities within what we're trying
34:57
to build and where we're going with this thing that if you're interested, um DMs are available on Farcaster, on X, um
35:05
Telegram. I'm B 05 crypto. Chuck, you're cshoten correct on all those platforms.
35:11
Um, yep. I'm B05 on Farcaster, Bz Crypto on everything else. So, if you're
35:17
interested, you'd like to play with this, you want to be an early adopter, um, you want to learn more about it,
35:22
please don't hesitate to reach out. We would love to add more people utilizing this.
35:28
And tune in next time for some details on how you really bank make these things rather than just looking at scary
35:34
pictures. We're gonna go through the build process for pipelines and for Kilroy apps and how it works under the
35:40
hood. So that by the end of this, you should be a pro. That's excellent. I love it. All right,
35:45
I'm gonna end the stream. I hope everybody has a great day. See you guys. Thanks, Brad. ing up.
ROTP: Introduction to Kilroy! Decentralizing AI/Automation/Crypto

Realm of the Possible
4 subscribers

Subscribe

2


Share

Save

Clip

31 views  Streamed live on Oct 17, 2025
Learn how you can be empowered to build anything with a powerful decentralized automation and AI platform. 

We've created the most powerful decentralized AI and crypto automation/integration platform while being completely decentralized. It comes complete with no configuration peer-to-peer/swarming capabilities, the ability to easily build integrations with any other compute, and a Turing complete visual creation tool anyone can use to build functionality in the network while giving agents on Kilroy the ability to build and deploy on their own.
Transcript
Follow along using the transcript.


Show transcript

Realm of the Possible
4 subscribers
Videos
About

All

Computers

For you

18:57
The Infinite Software Crisis – Jake Nations, Netflix
AI Engineer
117K views
•
9 days ago


18:00
The Hottest Thing Anyone Has Ever Said to Me | The Hustler
Kelsey Cook
2.8M views
•
1 year ago


14:19
What Sam Altman Doesn't Want You To Know
More Perfect Union
2.3M views
•
10 days ago
Fundraiser

15:40
I Quit an AI Startup After 6 Months - Here's What I learned
Brian Jenney
191K views
•
2 weeks ago

10:02
Can Magnus Carlsen Beat a Noob with 30 Queens?
Esports Spotlight
929K views
•
1 month ago

20:16
One Formula That Demystifies 3D Graphics
Tsoding
285K views
•
4 days ago
New

21:14
Why I Left Quantum Computing Research
Looking Glass Universe
1M views
•
6 months ago

33:14
Ethernet is DEAD?? Mac Studio is 100x FASTER!!
NetworkChuck
310K views
•
9 days ago

5:47
Chef Show - SNL
Saturday Night Live
12M views
•
2 years ago

24:19
NEW Walking Method 2x Better Than Running?! (Tested)
Jeremy Ethier
620K views
•
8 days ago

16:19
How ChatGPT Is Weirdly Turning Into Facebook
Enrico Tartarotti
141K views
•
2 weeks ago

15:30
Ex-Google CEO Warns "AI is Becoming Conscious"
AI Upload
32K views
•
13 days ago

5:25
New Barista Training - SNL
Saturday Night Live
7.1M views
•
11 months ago

17:00
Downfall of the 7-Hour Coding Tutorial
Boot dev
160K views
•
1 month ago

11:05
Will Ferrell Refuses To Discuss The Bird On His Shoulder | CONAN on TBS
Team Coco
1.4M views
•
3 months ago

12:35
How To Learn So Fast It’s Almost Unfair
theMITmonk
870K views
•
10 days ago

21:49
The Windows 11 Crisis
ColdFusion
1.9M views
•
2 weeks ago

14:10
How AI is Changing Manufacturing
Future in the Making
68K views
•
3 weeks ago

25:57
Stop Rambling: The 3-2-1 Speaking Trick That Makes You Sound Like A CEO
BigDeal by Codie Sanchez 
191K views
•
7 days ago

20:36
An atheist explains the most convincing argument for God | Alex O'Connor
Big Think Clips and 2 more
373K views
•
11 days ago


Show more
    



NL

Skip navigation
Search



Create


Avatar image
Transcript


Search in video
0:08
All right, sir. We're live. Well, awesome. We're sitting here in silence
0:15
contemplating life. Your fate after Bitcoin dumped on you
0:20
overnight. I don't have any Bitcoin, dude.
0:26
Not any. No per contracts, no nothing. Yeah, you do Ethereum.
0:34
I I I uh I held no Bitcoin.
0:40
Not a spot, not a bit. Okay. Um let's see. Uh today we're going to take on
0:47
building a trading app and um or start building a trading app and um walk folks
0:55
through how that would work. And uh I am really close. Um and we're going to uh
1:02
talk about a few news stories. So before we get started, let's do that. Unless you've got some news you want to share.
1:09
No, you go. Okay. comment the peanut gallery.
1:14
All right, so first up, um, Anthropic, I don't know if you saw this or not, Chuck, but, uh, Anthropic, uh, reported
1:23
that they disrupted the first ever AI orchestrated cyber ep espionage
1:29
campaign. Um evidently there was um somebody that
1:35
took over um and used uh a Gentic capabilities
1:42
uh and they're assuming that they have with high confidence it was a Chinese state sponsored group that manipulated
1:50
their tool claude code uh into attempting infiltration into roughly 30
1:56
global targets and succeeded in a small number of cases. All right. That'll teach you to let AI write your code for
2:02
you. Well, and that'll teach you to use centralized AI, right? I mean I mean the
2:08
reason the reason an attacker would use a centralized AI is it makes them a
2:14
little more difficult to trace if Well, and it gives them scale, too. Everybody that uses that centralized
2:20
resource can potentially get touched by it. Exactly. Um, and if you're utilizing,
2:26
you know, a data center in China, you're going to be a lot easier to trace as opposed to using somebody else's infrastructure.
2:32
I will have to tell you, I don't know if you ever looked at this, but speaking of data centers in China, Alibaba has a
2:37
huge one that is trying to catch up to to Amazon. They're always sending me offers.
2:43
I know you can get some cheap CPU time if you don't care who's looking over your shoulder. If you don't care who's looking at your
2:50
data, right? Well, I'll tell you this. this and this thing that you're reporting
2:55
about right now has a bit of a a rebuttal on RS Technica this morning where they said, "Yeah, it wasn't quite
3:01
as automated as they're making it out to be really." So, yeah, I think I think this is a
3:07
partly a hype cycle, you know, AI story versus the reality of it. It might be
3:13
worth following up with the RS Technica article just to see. I'll check it out. Yeah, it's interesting because Anthropic is the one
3:19
that, you know, is t tooting their own horn for um stopping it and figuring it out and blah blah blah. It's this long
3:26
blog post that they posted and it's like, well, gentlemen, I'm not so sure I would be uh
3:32
right. It's like the business that we're in and the tools that we have just made a mess of
3:39
somebody's data center. Exactly. And you know, we're helping facilitating foreign governments because
3:44
we are a centralized AI provider. Um, and then this was really I'm just going
3:49
to beat up on centralized AI today and then talk about a couple of other things. Um, Open AI is trying to make
3:57
um, New York Times into an enemy of all AI supporters. Um because New York Times
4:03
is suing Open Open AAI over using their uh content and um Open AAI is fighting
4:13
their request for access to 20 million chat users um interactions with uh
4:23
OpenAI so that they can prove or OpenAI can not prove that um they're using New
4:30
York Times content in serving ing answers up to users. Um, and so OpenAI
4:36
is another blog post by the company um, is actually trying to say how evil they
4:42
are to try to invade your privacy. Well, you know, first of all, you probably did
4:48
how evil the New York Times is to try to invade the users's privacy. Yeah, because they're suing OpenAI for
4:54
access to those records in order to prove that they in fact had We already invaded their privacy. It's
5:00
our invaded privacy, not yours, New York Times. Exactly. Exactly. And that's the I mean,
5:06
that's the that's the the rub there for them, right? It's like they want to flip
5:11
this into that narrative because they know that it looks horrible that their
5:18
users data is accessible. I mean, that tells you everything you need to know about using a centralized AI provider.
5:24
If open AI is saying not saying to them we can't get to the records. Open AAI is
5:30
saying to them we don't want you to have the records which you know certainly as a user I if I were a user of that thing
5:36
I would not want my records made available. I think that's interesting because that's a business decision and a
5:42
technology choice that they made. If you look at what Apple is consciously doing with access to centralized AIS off of
5:49
their devices, they're purposefully sanitizing any of the identity data and
5:55
disposing of the context once you've had your interaction. So, yep, Open AI doesn't have to keep that stuff, right?
6:01
No, absolutely not. But but a open AAI does do it because
6:07
number one, it then informs their future LLM development and number two, it
6:12
allows them to provide memory to their users so that the LLM comes back. But
6:18
the fact is if you're using if you're using OpenAI, your data is on their
6:24
server, your machine, not your data. And there's as more and more people replace more and more parts of their compute with AI, if
6:32
you're using centralized, um, you're subject to lawsuits, you're subject to government searches, you're subject to
6:39
just about, you know, anything, hacks, whatever. We we've been rewatching Mr. Robot and I
6:46
gotta tell you, oh wow. Not an instructive show about what happens when the centralized stuff goes
6:51
down. It's not funny, you know? I mean, yeah. Yeah. It's documentary in that regard. Yeah.
6:56
And today, OpenAI proudly announced that they're allowing group chats in their LLMs. Do you know any other platform
7:03
that might have group chats built into it? Oh, yeah, man. Built right in. And group chats that they're not spying on.
7:09
Yeah, group chats that are peer-to-peer and that are not available to the New York Times to access.
7:15
I feel like I should have put a tinfoil hat on today. We're being really This is the last one. Oh, no. Yeah, this
7:21
is the Well, one more little bang. uh cursor the uh coding uh AI platform uh
7:29
their valuation has gone up to they're raising 2.3 billion at a 29.3
7:38
billion post money valuation again. So this one is less about centralized
7:44
although it's centralized um for me and more about how stupid VCs are.
7:52
And the reason I say that is the people piling money into coding specific AI
7:58
platforms unless cursor has some you know general compute application of what they're
8:05
building that they're going to move into the world with this idea of coding with
8:11
coding to build apps the way we used to do things or the way we currently do things is not a futureforward AI model.
8:19
The future forward AMI model is to realize that the nature of applications and compute are going to change and so
8:27
you know everybody in their dog being able to create their own vibecoded HTML app is not going to succeed.
8:34
I wonder how how
8:39
legit or real the value proposition is that they raise money on versus hey you
8:45
know what if I got to tell somebody who can only understand traditional programming and I'm going to do AIs that do traditional programming in order to
8:51
raise $2 billion I'll tell them that and then you know what we're going to pivot to AIdriven agentic coding that's what I
8:58
would do right right but they all may only know that much about Got it, right?
9:04
Yeah. Yeah, exactly. Um, this uh we'll get going here with your demo, but um uh
9:11
this is really interesting to me. Um this is a platform, I haven't played with it yet, called OLM.
9:17
um essentially will allow you to run really giant um models uh some of the
9:25
larger models and I have two projects like this another one called Aerol LLM um on very minimal amounts of uh RAM and
9:32
GPU capability. Um, this article is claiming
9:38
uh $200 consumer GPU with just 8 gigabytes of VRAM to run GPTOSS 208 uh
9:45
uh uh 208 or Gwen 380B or um any of the large models and
9:52
they're using a caching mechanism of some kind um and they're so they're using Dix cache uh instead of VRAM but
10:00
and it says that it's actually really pretty fast. I that's something that's a little difficult for me to believe, but
10:05
I'm certainly willing to give it a shot. I've seen some stuff about this earlier and they may or may not be doing the
10:11
same thing where because of the layers in the in the models, they can load, you
10:17
know, some of the early layers, run them, get the weights out, then load in the next set of layers and feed those in. And they basically swap up through
10:23
the layers till they get the answer because once those earlier layers have done their, you know, computed their weights,
10:30
they're not needed anymore, right? they're just sitting there being pissed around for the next time. So I you know
10:35
I would suspect that they've just got some sliding window up through the stack of of the model and they don't have to
10:42
use they don't have to keep around the stuff that they already used. Right. I wonder if that's LLM specific like do they have to like optimize for each
10:49
specific LLM and the way it's built or is it standard standardized? you know, if they're all trained the same way and
10:55
they all started with the same base infrastructure and they all build the data models the same way, they probably
11:00
can, right? Who knows, right? That's just me guessing, but I've seen something similar and I
11:06
can imagine that that's right. You know, there's no reason, you know, there's no reason when you're running a giant video game for every piece of
11:13
every world and every cutscene and every bit of dialogue to be load loaded into the computer just in case, right? Just
11:20
stuff you're using. It's the same thing. Yeah. 60B Quen 3 model that normally
11:25
would use 190 GB of memory can be squeezed into 7.5 GB of VRAM and 180 GB
11:32
SSD. I'd be more interested to see what they would do with a tiny model. You know, here's a 8 gigabyte model. How what can
11:38
you scrunch it down to, right? Can I run it in less than a gig? Well, and does that actually end up being faster? Right. Can you take a 16B
11:46
model? I want to run it on my Apple Watch. Let's go. Yeah, exactly. Exactly. That's awesome.
11:52
Um, oh, speaking of which, just on a crypto note, um, really interesting project, uh, Lean Ethereum, uh, from,
12:00
uh, I think it's Justin Drake. Yeah. uh one of the one of the Ethereum
12:07
developers started a project called lean Ethereum using snarks to with the end goal of well initially 10,000
12:14
transactions per second on the on the network and the ability eventually to
12:19
run a node on Raspberry Pies and down to devices wearable devices if it was
12:27
nobody would do that but you know you get the idea so um but it is pretty cool and this is another one we'll get going
12:33
now called arrow LLM that supposedly does a very similar thing. Um, running a
12:39
405B Llama 3.1 in 8 gigabytes of VRAM. That's really tiny on my screen. What do
12:45
they target? What is that all all of the language choices and stuff that are in there? It's
12:50
this one. Uh, it says, uh, RLM optimizes inference memory usage, allowing 70B
12:57
large language models to run inference on a 4 GB GPU card without quantization, distillation, or and pruning. Um, let me
13:04
see. I meant the the lang Oh, you scroll scroll back to the top real quick. I'm just looking at the stuff on the right
13:10
hand side. The Oh, it's all Oh, the language is uh 90 uh Jupiter. Oh, Jupyter notebook. That's
13:15
interesting, huh? Okay. Uh, and I've got some other stuff, but we'll move on today. And, uh, let me
13:23
stop scaring my sharing my screen. Well, so, so if we're gonna shift to
13:29
some semi-related news as a leadin to our dog and pony show, you know,
13:35
this week was play around with Blow Finan's new copy trading functionality,
13:41
right? I did it, you did it. Got some other people that were playing around with it just to see how it worked. And
13:47
up until yesterday, it was doing great. I think week over week I think I had had 110% gain on my my play money and then
13:56
then last night happened while while we slept the the bad elves
14:02
came and took away all our toys. Um, right. So, I had stop losses on mine and
14:07
it it didn't damage stuff too badly, but it made me think, you know, why are we
14:12
relying on humans and copy trading as the mechanic that we use to keep our crypto investments accurate and safe and
14:21
productively producing giant returns for us. We've got signals coming in from other places. We know statistically when
14:29
it looks like there's going to be a dump or when it's been a dip and it's time to buy. We've had that in Kilroy for since
14:34
before COVID, right? Yeah. And and I thought, you know, there's no reason that we can't hook all those data
14:41
points that we have into one of our AI pipelines and have it watch over our
14:47
portfolio while we sleep so we don't wake up to sideways per contracts and zeroed out positions and all the carnage
14:54
that I had to wade through this morning to figure out what was going on. Right. Pop my screen up. I just want to
15:00
show uh so you know this is what you're seeing as a human in blowof fin if you
15:06
want to do futures trading with bitcoin per contracts and if you're not familiar
15:11
with perp contracts go google it and read about it because if I try to describe it right now it'll be 30
15:18
minutes from now when we're done but but basically it's kind of a you know
15:23
nobody understands it right it's well but it's kind of this weird self-balancing instrument where
15:29
you can't get too sideways in it and it and it gives you a a predictable way to
15:34
do leveraged trades, right? And so, right, these copy traders in blow fin
15:40
are doing like 10x leverage on these trades and there's, you know, a fair amount of volatility even sideways in
15:45
Bitcoin and they were doing really well, but nobody was watching while everybody was asleep and we got dumped on. So I
15:52
thought, well, let's let's look at some of the tools and pieces that we have in Kilroy and maybe as a group with people
15:57
who are listening and people who want to get involved, we can start to put together something that works at least
16:03
as well as a human and doesn't have to go to sleep. So one of the things that we've had in
16:09
Kilroy for a long time is the ability to talk to just about any centralized exchange that's out there that's been
16:15
around long enough to get uh CCXT interfaces built. So, we have a Kilroy
16:22
exchange app and I've already got uh some blowfin uh API keys installed in
16:29
Kilroy. I also have them for uh Coinbase and uh Kraken, I think. But I'm going to
16:34
make a new exchange widget and we'll just use Blowfin since it's already filled in. And and this is
16:42
not a very exciting user interface. And what it's doing is is pretty mundane. It just gives you a view in through the
16:49
APIs of what's going on in your blow fin futures account. And right now it's
16:56
supposed to be going out and fetching. Yeah. So this account doesn't have any positions in it. Right now it's just sitting on what? $800 of of USDT because
17:05
all my stop losses kicked in yesterday. So on cash with no investments. And and
17:11
in Kilroy, if you wanted to, if you know the symbol of something that you want to buy in here, you can do market orders
17:16
and limit orders right from inside Kilroy. You don't have to go over here to the blowfin website and fill in all
17:21
this stuff and do all this hinky stuff through their UI and hope that their servers are up. You can just go back to
17:27
Kilroy and say, "I want to put in a market order and what ticker symbol do you want and how much do you want and buy or sell side and hit go and it and
17:34
it does it through the CCX APIs and it and it does it and you see it show up in your trade history which you can look at
17:39
in Kilroy and and you can do, you know, you can close out those positions. You can do all the normal stuff that you
17:46
want to be able to do manually through this Kilroy interface. Well, okay, great
17:51
fun except who wants to sit and watch and have to mash buttons all day. So, a couple of weeks ago, I said, "Well,
17:57
let's add let's add some pipeline tools, some of our cool nonMCP,
18:03
totally sainely interacted with tools for Kilroy uh to do some functionality
18:09
uh from the AI perspective. So, I added this little AI tools button, which basically just opens up a pallet full of
18:16
of tools like you've seen before over here in our pipeline editor. These all of these things, right? But this
18:22
particular Kilroy app defines some of its own. So not only do you get the tools that that we ship inside of the
18:29
the AI application that that Kilroy is doing, but any third party can add blocks that extend the pipeline. And so
18:37
one over one an easy one to use is this get position one, right? And I'll put this over here and we'll look at it
18:43
again in a second. But what I want to do is just spend a little bit of time and show how you can use the AI pipelines to
18:50
do application development. Right? There doesn't have to be AI always in a pipeline. It can be programmatic stuff
18:57
too. So we've got a block called get position, right? And
19:02
it's got some attributes on it about what it does. But the really key thing that we need to set up is well what exchange are we using? Right? Well, I'm
19:08
using blowofin, right? So, I want to get all of my positions in blowof fin
19:13
whenever this little block is made to be active, right? Well, the easiest way to make it be active is to have something
19:20
talk to it, right? Sure. So, I'm just going to put a dumb terminal interface in here. And we'll
19:28
let some message go into the swarm and some message from the swarm go into get position and some message come back out
19:36
and some message go to the chat agent so that we can see it. Right? So, um we're
19:41
going to let get position just listen to everything. It doesn't care who's who's sending stuff. And for now,
19:49
just so we can see it on the screen, we'll let the chat agent listen to everything, too. So, pretty simple,
19:55
pretty simple little pipeline. Somebody's going to type something. It's going to go to the swarm get position is going to say, "Oh, what? Oh, go get the
20:01
position for the user's uh blowfin account and dump it back into the swarm. And then the chat agent's going to just
20:07
show it on the screen." So, this is just a blow fin test. And so, let's export that. go back over
20:14
to Kilroy and we'll close this up and we'll close this up and we'll close this up and we'll build that pipeline that I
20:22
just drew. Okay. And it's it's just the one screen, right? It's just the one little guy
20:29
who's going to let us talk to the to the
20:34
get position box. No AI in the loop right now. So, I can type anything. It
20:40
doesn't matter. It's going to trigger that block to do its work and it's going to go out and fetch my position. And what it does is put out the JSON results
20:48
that come back from uh Blowfin about what's my position, how much free cash
20:54
do I have or total or whatever. Right. Right. That's great. That's JSON and it's not very useful to me as a human
21:00
and even less useful to, you know, an LLM unless we fix it up. So, first thing
21:06
we might want to do is give that data to uh an AI agent, right? And let it do
21:14
some work on that. So, let's just tell a guy uh when you receive
21:21
uh a JSON object, report the total amount
21:28
in a markdown message. I don't know. Let's see what that does.
21:33
Right. So, it's going to listen to what get position says, right? We don't
21:40
want it to listen to what the user types because it's just going to be gobbledygook. So, we want it to listen to what get
21:47
position is telling it and it's going to do its work with that system prompt and
21:52
put the message back into the swarm as AI agent chat agents.
21:58
Let's say the chat agent doesn't really want to see the JSON, right? We want to hear what the AI agent has to say. So,
22:03
let's change this over here to be uh AI agent.
22:08
So, essentially, you're changing those the yellow bubbles on the on the arrows
22:14
determine who's listening to what, right? So, that's saying the chat agent is only listening to AI agent,
22:20
right? The AI agent is only listening to get position. Actually, you know what? We don't want get position to listen to
22:25
everybody because it's going to hear the results that come out of chat agent. So, we really need to be more uh
22:32
discerning. Correct. And say it needs to listen to chat agent. Yep. You're not in the box.
22:37
Oops, I am not in the box.
22:43
Right. So, essentially this is going to this is taking the JSON that comes in, sending it to the AI agent, having the
22:49
AI agent process it, and give it back to us in normal English. Right. So, um,
22:55
except right now, would the chat agent get it if it's only listening to Yeah, it is because it's getting it from AI agent. Duh.
23:01
It's basically going to go from chat agent to get position to AI agent back to chat agent. Got it.
23:06
All right. So, let's kill this other old one and make a new one.
23:18
And it's just still going to be the chat window. The other two boxes aren't going to show.
23:24
Um, and I think we're going to have a little bit of lag because I want to make sure that um,
23:30
actually, you know what? It's a good thing I checked because uh,
23:36
Llama's not running. That would have been an embarrassing demo.
23:43
Okay. Oh, we've had worse. A llama is running and I'm glad that I checked. And then back to Kilroy. Now,
23:51
we're gonna just say, "What are my fat stacks looking like?"
24:03
Okay. And so that's going to go off to the LLM. It's got to load the model up. So, just be patient,
24:09
right? Oh, yeah. Well, and here it comes. And and and where's my answer? Come on.
24:16
Come on. Kill. You can do it. Here we go. And then once the model's
24:21
loaded, this thing doesn't take It's still loading. I'm watching it in the other window. There it is. There you go.
24:27
Cool. So, it came back. It's hard for us to see on the screen, but it came back and gave you in plain English uh the
24:32
amount in your portfolio. It'll be faster this time. I said, "Tell me again." Bam. There it is. There it is. Yeah. Once once the model's
24:39
loaded, it it it uh doesn't have to be reloaded again. It won't take as long. So, so now we've got a situation where the
24:46
AI agent knows how much free cash we have to spend. Right. Right. So, once we've got that, we can have
24:54
another AI agent that's sitting and listening to maybe signals from material indicators and it sees, hey, you know
25:00
what? There's a three deviation on the price of Bitcoin. Bitcoin's dumping. Well, normally back in the old days when
25:06
we were doing our trade bot stuff, we had to wait till the dump was over and then we would buy in because it was a
25:13
buy the dip algorithm, right? Now, we want to look for smaller deviations, right? Maybe a, you know,
25:20
1.5 deviation or something and we see that Bitcoin's dropping, we should sell
25:25
per contract, right? Right. and we can sell it short and write it down and when we do get the
25:31
bottom report from uh material indicators, we would close that sell position and switch to a buy position.
25:39
Right. Well, and the the beauty of this the beauty of the platform is is that we could have four or five different
25:46
signals coming in. Let's say three different signals coming in for different parameters, trend, deviation,
25:52
something else, right? Crossing, EMA, whatever. And we could have the AI agent
25:58
decide, right, whether or not it's hit two out of three indicators or three out
26:04
of five or four out of five and not execute a trade unless the proper parameters are lined up for that. And
26:11
then as we progress and as your agent progresses through trades, we're now
26:17
potentially we could set it up to build up some data and be able to do some analysis and um have agents that are
26:24
actually uh storing data about say a particular pair or whatever and can then
26:30
be the agent that is smart about Bitcoin or Ethereum or AVAC or whatever um and
26:38
have historical data that can also be leaned upon. on for this is this is the cool thing. So now
26:44
you got an agent that knows how to do effective trades on uh you know a
26:49
particular per contract at blowofin, right? We're going to buy and sell Bitcoin until we're all fat rich.
26:55
But here's the really cool thing. Now you can have subscribers to your trading
27:00
strategy the same way that Blow Fin's giving these copy traders a platform, right? So, I go on Blow Fin and I pick a
27:07
copy trader I want to follow and it sets up and automates the trades for me as that person trades. Well, I can do the
27:13
same thing. I've got a copy of Kilroy. I've got the software stack that's necessary to to buy and sell with the
27:19
exchange of my choice. I can now follow the swarm that Brad is producing buy and
27:25
sell messages into. That's right. So, bad as Brad buys and sells using his
27:31
AI algorithm automatically, I can just copy what he's doing. Yeah. Right. I don't even have to have an LLM on my
27:37
machine. Right. No. No. I just takes the signal and does the buy. Right. Right. And so exactly what Blowfin's
27:44
doing in a giant centralized exchange, any two, three, five, 10, 50 Killilroy users could go off and do a swarm by
27:50
themselves. Right. And that and the beauty of it is that one person can be
27:56
working on performance basis offering a subscription whatever to um as many
28:02
other users as they want and there's no infrastructure cost for them. There's no infrastructure cost whatsoever in terms
28:08
of like having to set up multiple servers or I mean if you wanted to you could be dumping those buyell signals into a
28:13
private telegram channel and you really have zero infrastructure cost. Exactly. Exactly. No, that's the beauty
28:19
of it. And you can interact with it through Telegram if you want or not. Yeah. Yeah. I mean, you're on your phone and you're
28:25
getting buy and sell signals. Why not have it just, you know, you put your own bot in the Telegram channel, listen for
28:31
the buy and sell signals and do it yourself. That's that's the really
28:36
the really exciting thing, and this is why we really want to encourage people to get involved with this platform
28:42
early, is building out these sort of community projects. We don't have any idea how this gonna go. Once 50 people
28:49
decide to get into a trading club off in the corner in a Kilroy swarm and start asking for features and building things
28:54
and sharing it with others, all bets are off what happens with this thing. I really don't I really can't foresee, you
29:00
know, where it ends up. I just know that we have all the tooling in place for anybody that wants to do it to do it
29:06
right now. Yep. Yeah. And I think we could expand on this potentially next time with
29:11
signals and that kind of stuff. Yeah, I think we should. We have the signals coming in. We can show those coming in. But the other thing uh if you
29:18
remember a while ago we did a little dumb demo of uh watching watching Doge
29:23
price and if Doge price went down uh we issued a a a sell command and at the
29:30
bottom we'd issue a buy command and it was a little buy the dip bot. And we didn't do it for real but we could put
29:35
Valor on the screen just to give people a sense next time of hey this is the endg game. This is where we're headed
29:42
but I really think we should build this out. This actually turns out, you know, we used to complain that we wanted to trade on Blowfin, but they didn't have
29:48
their spot uh their their spot exchange exposed through CCXT. But in truth, I
29:56
think this per contract stuff that's going on in their futures market is
30:01
probably easier and safer, right? There's Right. There's first of all, we can do shorts in there,
30:07
which you can't do on their spot market. Yep. Right. And and secondly, all those per
30:13
contracts have kind of a bounded downside. The worst that you can ever lose in a per contract is the amount
30:18
that you put into it. Yeah. Right. Because they'll close you out as opposed to like trading stock futures
30:24
where it can go infinitely negative on you. And deleting your Robin Hood account doesn't help.
30:29
Yeah. Exactly. Exactly. That's great. I love it. Love it. All right. Well, that's that's today's
30:34
dog and pony show, Brad. All right, man. We'll wrap it up. Hope everybody has a great day. Have a great weekend. Enjoy your Friday and um we
30:42
will be back next Friday at 11:30 a.m. As always, if you are interested in
30:48
playing with Kilroy, using it, building for it, joining us as we go to market,
30:53
etc., etc., we'd love to hear from you. You can reach uh me on Telegram and uh
30:58
Twitter at B05 Crypto. Um on Farcaster it's at B05 and Chuck is Cshotton SO tt
31:07
n on all platforms. So thanks for listening, thanks for watching and uh have a great day.
31:13
See you. Okay.
31:19
Well, I'd like to say goodbye, but it's not letting me.
31:25
Where's the stop button? It's not It's It's so weird. The go live button is
31:32
supposed to turn into a stop button and it has not. Oh, you must keep serving.
31:38
You cannot stop. We're forever stuck. All right, I'm going to see what happens if I refresh my screen. See if it gives
31:45
me the button. If not, we're just gonna have to drop off and leave
31:51
an eternal stream running.
ROTP: Building an automated trading pipeline in Kilroy pt. 1

Realm of the Possible
4 subscribers

Subscribe

0


Share

Save

Clip

5 views  Streamed live on Nov 14, 2025
This week we're going to start walking you through building a trading pipeline in Kilroy. 

Learn how you can be empowered to build anything with a powerful decentralized automation and AI platform. 

We've created the most powerful decentralized AI and crypto automation/integration platform while being completely decentralized. It comes complete with no configuration peer-to-peer/swarming capabilities, the ability to easily build integrations with any other compute, and a Turing complete visual creation tool anyone can use to build functionality in the network while giving agents on Kilroy the ability to build and deploy on their own.
Transcript
Follow along using the transcript.


Show transcript

Realm of the Possible
4 subscribers
Videos
About

All

Computers

For you

Recently uploaded

20:30
Instagram Pilot's Emergency Caught on Camera!
Pilot Debrief
1.2M views
•
8 days ago


14:19
What Sam Altman Doesn't Want You To Know
More Perfect Union
2.3M views
•
10 days ago
Fundraiser


9:23
"AI Can’t Replace Juniors" - AWS CEO
The PrimeTime
262K views
•
2 days ago
New

17:18
Laid Off After 25 Years in Tech: The Anxiety, Sacrifice, and Reality No One Talks About
Asian Dad Energy
663K views
•
10 days ago

17:08
When AI Gets an Innocent Man Arrested
EWU Bodycam
3M views
•
2 weeks ago

17:35
The Future of Veritasium
Veritasium
3.4M views
•
5 days ago
New

5:25
New Barista Training - SNL
Saturday Night Live
7.1M views
•
11 months ago

1:39:31
How I Made $1,000,000 in 51 Days of Day Trading (Full Training)
Ross Cameron - Warrior Trading
1M views
•
8 months ago

35:34
The Reality of Fully Automated Trading Systems - Kevin Davey | Trader Interview
Etienne Crete - Desire To TRADE
5.7K views
•
1 year ago

16:47
Office moments I think about way too often
The Office
1.7M views
•
4 months ago

10:02
Can Magnus Carlsen Beat a Noob with 30 Queens?
Esports Spotlight
929K views
•
1 month ago

11:52
I shrunk down into an M5 chip
Marques Brownlee
 and Epic Spaceman
2.1M views
•
3 days ago
New

11:05
Will Ferrell Refuses To Discuss The Bird On His Shoulder | CONAN on TBS
Team Coco
1.4M views
•
3 months ago

12:02
"Most People Are Broke" - Lamborghini Salesman On Who ACTUALLY Buys Their Cars
The Iced Coffee Hour Clips
353K views
•
1 month ago

22:08
China Just Broke The Silver Market
Andrei Jikh
355K views
•
10 hours ago
New

21:43
Why tech billionaires are quietly bankrolling Europe’s far-right | Pinch Point
Al Jazeera English
313K views
•
13 days ago

13:39
I Built 3 SaaS Apps to $200K MRR: Here's My Exact Playbook
Starter Story
464K views
•
1 month ago

32:32
The Strange Math That Predicts (Almost) Anything
Veritasium
10M views
•
5 months ago

16:24
Humanoid robot runs like a spider, shows we're close to disaster
InsideAI
319K views
•
6 days ago
New

30:40
Is The Line Really Dead?
The B1M
2.6M views
•
11 days ago


Show more
    



NL

Skip navigation
Search



Create


Avatar image
Transcript


Search in video
0:00
and how are you doing? I am good. I'm good. We are um
0:05
loaded up and ready to go. Today we're going to we're going to walk people through actually building um an Aentic
0:12
pipeline um in Kilroy. But not only will we show the ease of that, we're going to
0:18
show them how um how we can interconnect
0:23
all of our uh agents together via swarms um so that we can utilize resources and
0:29
connect together and have um agents collaborating together from uh different well collaborating is a strong word for
0:36
the demo today. But the idea being that swarms allow you to work have agents work together and humans work together
0:42
um and share resources. So, um, and last week we covered kind of the overview of
0:48
what Kilroy is, but go ahead. Well, I was just going to say for people who are tuning in for the first time and
0:53
don't know what Kilroy is, Kilroy is a desktop platform for automating your
0:59
version of the internet. It ties in uh, AI platforms. It ties in local AIS. It
1:06
ties in crypto and DeFi. It ties in your chats on Telegram. It ties in websites
1:11
that you visit and RSS feeds that you watch. lets you make your own view and your own desktop, your own apps all pulled
1:18
together into one place. And what we've been focused on is one app on Kilroy,
1:24
which is our AI platform, which lets you build really, really cool pipelines to talk to
1:30
local LMS on your desktop, one's in the cloud, things like chat, GPT, whatever. Today we're gonna have uh my computer in
1:39
Virginia and Brad's computer in Florida set up a pipeline between themselves and
1:44
interact and we'll show you how you build that. Excellent. I'll let you uh take take
1:51
Let me share my screen so people can see the fun stuff because it's not fun looking at two old guys talking about software.
1:58
Well, speak for yourself, old guy. Well, relatively speaking. All right. So, here is I'm just going to share this
2:06
browser window. Tell me, can you see it? Yeah, there it goes. Okay. So, we're
2:11
looking at the Killword dashboard and we're going to hop right into the editor. And so, I'm going to build a
2:18
little uh a little pipeline and Brad's going to
2:24
build a corresponding piece that shows how easy it is to make a chat room out
2:29
of the infrastructure in Kilroy. So, we're going to go to the editor and I'm going to zoom this up a little bit so
2:35
the sidebar and whatnot's easier for people to see. Yeah, let's definitely do that. And um we're going to start Brad, you do
2:42
this, too. We're going to start by pulling over a chat agent so that there's something for the humans to talk
2:48
to. And we're going to pull over a swarm object so there's something for the agents to talk to.
2:55
And I'm going to name. So there's all these settings on each one of these little
3:00
boxes that you can pull in. And nine times out of 10, the default settings are okay. But in this case, I want to
3:05
change the name of mine to be more clearly that it's, you know, that it's me. And uh you do the same thing, Brad,
3:13
except make it, not Chuck. Correct. Got it. And so these extra fields down here are
3:20
for the agents that need to lie to other agents about who they really are. Sometimes it's important for an agent to
3:26
look one way in the diagram here and another way when it talks in the swarm.
3:31
So that's why I've got my name in three different places. You wouldn't normally have to do that. All right. So now the
3:37
swarm, let's make the swarm be uh Chuck and Brad.
3:43
So that anybody that joins a swarm this with
3:48
this name and we're going to make it public would be talking to us through that swarm. You see when I make it
3:56
public, we get a little Kilroy up in the corner. Let's talk a little bit about swarms before we do anything more. Brad,
4:01
great. So swarms are a a way for
4:06
multiple instances of Kilroy running on your desktop, on mine, wherever, uh, to
4:12
talk to each other. And they're self-organizing. If we make a swarm and name it, and we start it up, everybody
4:18
can join it by that name. And it's a peer-to-peer swarm. They discover each
4:24
other or however many nodes are in the swarm automatically by themselves. You don't have to make any changes to your
4:30
firewall. You don't have to have a static IP address. You don't have to have a domain name. You just put the
4:35
swarm name in and Kilroy finds the other people out there in that swarm. And it establishes uh point-to-point end toend
4:43
encrypted connections between all the participants in the swarm. And so when I send a message, everybody else gets it.
4:49
And vice versa. There's uh and it's a direct connection between my machine and your machine. So, what we're going to
4:55
build, I'm going to send messages from my little display window uh into the
5:02
swarm and I'm going to listen to everything that comes out. So, I'm going to change this to be the wildcard aster.
5:08
And just for readability on my screen, you don't have to do this, Brad. I'm going to make this window a little bit
5:13
bigger. I'm going to make it uh 1024 by 768. So when it shows up in the Kilroy
5:19
dashboard, it's a little bit larger and we can see more of the messages around it. Okay, I have my uh swarm built and
5:29
you don't see Brad on here because I'm not doing Brad's computer and Brad doesn't want to have to reach all the
5:34
way through the wire from from Miami to type on my screen. He's got a diagram that looks like this, except this is
5:41
Brad on his and Brad on his. So, I'm gonna I'm gonna deploy this pipeline by
5:48
doing the export button and then go back over here to Pilroy and tell it to build
5:54
a pipeline. And Brad, you're doing the same thing, right? Correct, sir.
5:59
Well, there we go. And so, all it really did was make this one window for me,
6:07
right? And when the little green light comes on, it means I'm connected to the swarm. Brad, send me a message.
6:18
Howdy, old man. Thanks, dude. And there's hello from Virginia. So, it
6:24
was that easy to set up peer-to-peer end to end encrypted chat room between me and
6:29
Yeah, we just built the chat. We just built the chats form, right? And we could have we could have another hundred
6:35
people in there, right? And so, so that's all stuff that's built into the underlying Kilway
6:41
application. and you're just telling it to hook things together by drawing a picture completely peer-to-peer. We are not
6:48
connected to servers of any kind where the application is stored. It's all running on our machines. Um, and we are
6:55
directly connected to each other. Okay, so that's fun. Great. We can chat
7:00
back and forth. Now, let's show you what the magic guju is that's really going on with Kilroy and the cool things that can
7:08
happen because you run stuff locally. I'm going to drag over an AI agent box.
7:14
Brad's not going to do this. I'm going to run I'm running an LLM on my desktop. It's running in the O Lama runtime that
7:21
we talked about last week and the model is Llama 3. And I'm going to show you a little bit about how what we can
7:27
configure for this before I hook it up to the swarm. Um, so obviously that we
7:33
can rename it as before. Um, I can put a URL in here about where is the LLM
7:40
running. So, if you have an open AAI account and you want to use chat GPT, this is the URL that you'd put in for
7:47
chat GPT. And then you put in your API key for chat GPT and tell it what model that you want to use GPT 4.0 or whatever
7:55
the model name is. You can adjust things like the temperature, how many tokens you want in the response, and you can
8:01
give it uh a system prompt to use as a
8:06
way to configure the agent beforehand. I'm going to put one in there in a second that's probably going to make Brad mad. Um, then this prompt prefix is
8:15
a a prefix that's appended or prepended to every message that the AI gets. So if
8:22
for instance the AI was getting nothing but JSON data coming in from an outside tool, the prompt prefix could be convert
8:29
the following JSON to plain text and then the JSON's stuck in there. The agent gets all the pieces together and
8:35
it does the instruction. Whether or not we want to use context, we want to do that and we'll let the context grow
8:41
infinitely. And then um a few other mundane things like what color do we want it to be? I mean I can make my I
8:47
can make my AI box be baby blue. Oops. I got to click it first,
8:53
for instance. Um, and so it's all configured now. And I want to show it on
8:59
the screen. So, let me uh let me tell it to put that up in the UI in a minute. And I'm going to hook this one up to the
9:05
swarm. It's going to listen for messages from Brad, right? So Brad's agent on his
9:14
computer in Miami is sending messages as Brad. the AI agent's going to send its
9:20
messages back to the swarm for anybody to listen to. And um so I'll be able to
9:26
see the conversation, but I won't be able to participate. The AI is only going to listen to Brad. And since it's
9:32
only going to listen to Brad, let's give it a prompt that says, "You are Brad's
9:39
wife. Only respond with loving and
9:44
adoring comments. Keep your responses
9:50
as brief as possible. All right, Brad, can I do that with the real one
9:56
in real life? Well, um especially the last part. Especially the
10:03
last part, the brief, right? The brief. All right, now you're now you're going to be in trouble. All right, so I have a new pipeline here
10:10
that I'm going to deploy. So, let me export it. And I'm going to go back over here and kill the old pipeline. Now,
10:15
Brad is still connected to the swarm called Chuck and Brad. I'm gonna get out. I have to bring back my uh
10:24
my user interface for it. Where'd my pipeline go? That's not the right one. This one. I didn't give it a name.
10:30
That's why it didn't show up. Um we're going to get rid of the old one and we're going to make a new one and put
10:37
that in. And it's going to pop up another little window down here. here. And I'm going to let me reorganize this
10:43
and zoom it just a little bit bigger here, too.
10:48
Oops. And put that away. So, what you can see is this is the chat window that we had before. This is just a a view
10:55
into the LLM and what it's doing. So, you can see it's little lights blink and see that it's doing some work. Um, so
11:03
let me just communicate for a second. Well, I can't type.
11:10
Are you there, Brad? No. The LLM's gonna hear what you type.
11:16
So, go ahead. I know. Yep. I All I said was yes, I am. I don't think it's going to react to that. Will it?
11:22
Oh, my sweet husband. You're the best thing that's ever happened. Oh, you know what? I gotta We We got
11:27
Well, go ahead. You're You should say Brad's wife, but not AI agent. But go ahead. All right. So, now you can talk
11:33
to your wife. Exactly. What's for dinner, dear?
11:42
Now, just to reiterate, this is Brad. Oh, look. She's making me cook.
11:51
She said, "You always know just what to ask. How about a romantic Kendall at dinner with my favorite dish, your
11:57
famous barbecue ribs." That's so just to make sure everybody understands this is Brad on his computer
12:03
in Florida typing across the internet into my computer and into the LLM that's
12:09
running on my desktop in OAMA is producing the results. Kilroy is bundling that up, sending it to the
12:14
swarm and Brad can see it on the screen and I can see it on the screen and you guys are getting to see my view of it.
12:19
Um, and that's that's really important real quick, Chuck. Um, because you know, we were just on a call with somebody that
12:25
works in the um enterprise world and you know, in those worlds, they like to keep
12:32
control over um the resources and the technology technology that's being used,
12:38
but they also need to be able to distribute and make available um GPU
12:44
resources and LLM resources to their customers, to their clients, to their users. And so what this means is is that
12:50
inside of Kilroy, I can make use of Chuck's GPU and Chuck's LLM seamlessly.
12:57
I didn't have to jump through any hoops. I didn't have to do anything except connect to his to this swarm and then
13:04
make use of it and be able to prompt the um AI to do and answer questions that I
13:10
needed to answer. And this could be true of a full-blown pipeline of, you know, 100 agents that are all working together
13:16
and collaborating within the system or just standard chat interactions with an LLM like everyone's kind of used to
13:22
doing now. So, this is a really powerful piece, the the peer-to-peer and swarming capability of being able to allow
13:29
enterprises to share GPU and LLM resources that are approved internally,
13:34
um, but not require them to have to go spend $200 a month per person on an open AI subscription with their LLMs. Um, it
13:42
gives them all the power they need from an Agentic system. Um, and being able to share the resources and make them
13:48
available. Now, so, so this is still a pretty simple example. I'm going to bring the pipeline picture back. Um,
13:56
it's really just one user talking through the swarm to the AI. And I'm kind of eavesdropping on the
14:01
conversation between Brad and his his virtual wife here. But we could add some
14:07
uh tools to this to make it do something, you know, a little more uh exciting. What What if we made it so
14:15
that your wife understands crypto prices? Let's do that. I like that. Okay.
14:20
I wish that was the case. You know, while I'm in here, I'm gonna change the name. Change her name. Good.
14:26
Oh, look at this, man. Instead of being a trad wife, she's going to be a Brad wife. Uh,
14:32
which is definitely not a trad wife, right? Um, I need to change this though so it
14:38
matches. [Music] Okay. So, Brad Wife is talking to the
14:44
swarm, not Trad. I don't I'm offended by that arrow head. Let me fix that there.
14:50
Um, all right. So, we're going to add the ability to go get prices for cryptocurrencies. And if you remember
14:56
over here, we have a lot of predefined tools that Kilroy implements that are easy to drop into the AI universe. This
15:05
is a particular tool that takes basically plain text input that is
15:11
something resembling a cryptocurrency price. you know, lowercase, uppercase, slashes,
15:17
dashes, dollar signs, whatever. It it deals with what you give it and figures out that that's a crypto price. And then
15:25
this one, I believe, is configured to go off and talk to Coinbase for quotes. Yep.
15:30
We have the ability in Kilroy to talk to about 145 different exchanges. So, this
15:36
one is just here for demo purposes. So, this one's going to go off and get a price. And what it returns is a blob of
15:43
JSON from the exchange that includes historical prices and candlestick data
15:49
and blah blah blah and a price. And it's up to something downstream from that to
15:54
fish that data out and do something smart with it. But for now, what we're going to do is uh leave this one here
16:02
for a minute because we have to get Brad's request for a price out of the swarm. All right. So, the one of the
16:09
easiest ways to do that is to use our a tool that matches regular expressions,
16:16
right? And so, we're gonna uh listen to the brad swarm
16:22
for messages. Um, do we want your wife to be able to use
16:29
this tool or do we want you to do it? Let's let Bradwife ask for
16:35
We're gonna really jump the shark here. No, I know this is the Yeah, that's Yeah. Well, okay. We'll let it listen to
16:41
everybody first, then we'll Okay. And it's going to listen for a regular expression that is just slashpric. Now,
16:50
I can tell you right now, this is a bad regular expression because that's going to match slash price anywhere in the
16:56
message. And we'll probably see the carnage that that that brings down
17:01
shortly because it's going to clearly do it when we don't want it to. But we'll leave it like that for now. And we'll
17:07
just say this one is um price fetch.
17:13
And it we're going to have it put the results of that. We don't want that to interfere with our discussion up here uh
17:21
just yet. So the price, let's see what do we want it to get the price uh if we
17:27
see the slice command. So we want to get it from a private swarm. We don't want people to see the sausage getting made.
17:33
So, we're just going to pass the the message along from Oh, I should have
17:39
fixed this, too. Hang on. Make it lie to the cloud about who it
17:45
is. And we'll drop that line there. Now, it's going to ask it to go fetch the
17:51
price into that swarm. And this little guy is just listening for anything from
17:57
anybody that has to do with a crypto price. And it's going to get that JSON.
18:02
And I guess we'll throw it back in up here. Okay. Okay. Now, right now,
18:10
you're going to be able to see that and I'm going to be able to see that because you and I are listening to everything.
18:16
This is bad. We don't want this going around in a loop, right?
18:22
We don't want the price to get back in here and then go back to fetch the price yet again. So, let's really let this
18:30
first of all work with you. Okay. So, it's only going to work when you ask for a crypto price. Got it.
18:37
It's going to dump it back in there. We're all going to see it. I think that looks right. So, Brad's going to do slashpric with a
18:43
cryptocurrency symbol. Uh, this thing is going to take that message from Brad and put it over here in this private swarm,
18:50
which is gonna let uh get price see it, get it, put the data back in in here.
18:57
And anybody that's listening to Wildcard is going to see it. So you'll see it on my screen. Brad will see it on his
19:02
screen. Brad's wife is not going to realize he's trading crypto in the background. All right. So I'm going to
19:09
export the pipeline. We'll go back over here to Kilroy. And we want to kill this one and start a new one.
19:18
We're all zoomed in. It's all stupid. Oh, you know what? Let's fix this so people can see those other uh those
19:25
other boxes doing stuff too. We can see the the regular expression get matched.
19:32
And uh Oh, you mean the the underlying functionality that's normally wouldn't see these, but I'm
19:38
going to put them on the screen so people can see them work. The blinky lights are cool.
19:43
Um blinky lights are great. We're going to go back over here and get our pipeline builder and build a
19:49
pipeline. Okay. It's going to put a bunch of boxes on the screen that are going to be too big for my display. I'm probably have to
19:56
shrink it back down. Oh, no. They all fit. That's good. Okay. So, we've got the terminal window. We've got the LLM that's going to have
20:03
Brad talk to it. We've got the thing to go see that Brad wants a price and pass it off to the price thing. And we've got
20:10
the the result coming back. So, just to make sure everything is working. Hi, you
20:15
saw my message, right? I did. All right. Well, use the slashpric command to get a price for something
20:22
like Bitcoin. I did. Oh, well that's that's fast. And the
20:28
difference is that this response didn't come from an LLM where it had to go offline and think about it and come back
20:34
and generate some tokens. It went directly to the tool that knows how to go get prices. So Brad sent price ETHUSD
20:40
in the chat. The get price function responded almost immediately. Here's the price of of ETH. Do it do it for Bitcoin
20:47
just so people can see. Oh, she saw you asking about the price of Ethereum.
20:54
That's awesome. She knew you asked it, but she didn't see the price. That's hilarious.
21:00
Well, ask for ask for the price ask for the price of Bitcoin and see if she Oh,
21:07
the only thing that matters to me is the price of your heart. And it's priceless. Oh, and the price of Bitcoin is still at
21:13
It's still below still 110 to it last. You can see but but let's look at
21:19
this for a second because this is the data that the Kilroy tool is responding. So it gives you an idea of the richness
21:25
of the tools. You can see you know it's time stamp so you know exactly when the request happened. You get to bid and ask
21:30
price from Coinbase, the last trade, what did it close at for that trade. Um
21:36
and then you know some other information about is it a buy side quote or sell side or whatever. So
21:41
and it go ahead. Sorry. I was just gonna say it's important that people understand that this is feeding back the
21:48
raw JSON, but you could then wrap this into a UI for the end user if you were building this app to do this for them so
21:55
that they see a pretty presentation of the price if so you know we can do that. We have the
22:02
price coming in here from from get price into the public swarm and we haven't
22:07
talked about doing this but I'm going to fix it up so it's human readable. Okay. So, I'm going to add another AI
22:14
agent in here, which is uh human
22:20
prices. Okay. And let's see, we'll have we need
22:25
to show that. That's going to be number five. And human prices is gonna Come on.
22:35
Human prices is going to listen to the results from get price. Right. Right.
22:40
Um, so we only want it to listen for things from get price, right? And it's going to do its work and it's going to
22:46
put it back into this swarm for everybody to see. Our swarm's getting crowded here. Let me uh let me rearrange
22:54
the plumbing a little bit so the lines don't overlap. Okay, so human prices needs to say, "Hey, I'm going to get
23:00
some JSON and I need to put human stuff back in here that people can read,
23:07
right?" And hopefully your wife isn't gonna chime in about
23:13
break it. All right. So she may give it a prompt that says a system prompt that says you
23:20
will receive. And of course if we wanted to block
23:25
that agent from commenting as my wife, we can do that through the through the language
23:31
crypto pricing info. Please summarize. the data in
23:39
human readable markdown format.
23:45
Okay, so ideally what's going to happen and we're going to see a lot of stuff go
23:50
by in the chat window so we'll have to decompose it. You're going to ask for a price. It's going to fetch the JSON.
23:56
We're going to see the JSON. This agent over here is gonna uh get that same JSON
24:02
that we see in the terminal window and then mangle it up and put out uh we'll see maybe a maybe a markdown table,
24:09
maybe a narrative. Don't really know what it's going to do. We're letting it be creative
24:14
which is always dangerous over here and but we like living on the edge. Kill the old one. Yeah. You know, I I
24:22
didn't sacrifice a chicken, by the way. So, there's no guarantee happen. Great.
24:28
Um, we'll paste that in. Do submit. It's going to build all our stuff. Off the bottom down here is going to be our
24:34
other LLM. Let me zoom this back out a little bit so we can maybe see it all fit. Minus.
24:42
Oh, come on. Here we go. If I close this.
24:49
There we go. There's our chat window and our four little guys. Okay. Okay. Now,
24:56
just real quick to go back to the window, I made this thing that does human readable stuff, and I didn't change any of the settings. And you'll
25:02
notice that it doesn't have the context turned on. We don't want it remembering the previous JSON from previous
25:09
requests. This is agent with amnesia. It's only going to do the current request and then throw away its history.
25:17
Yep, we definitely want that. Keep it from hallucinating other prices. All right, Brad, ask for price of some
25:24
crypto. Let's see if we can get a human readable summary. It's still working. You see the one on
25:31
the right is still yellow. And there it is. There's your train. There it is. Nice. It went basic, but it
25:37
looks good. Well, it did it did some more stuff above it, right? Oh, I didn't see that. Okay. Nice. And
25:43
you could feed in sentiment analysis on social media into an app like this. You could have it look for price changes of
25:50
a certain percentage because you always want to buy on a specific dip or you want to sell at a specific profit level.
25:55
And in that case, you'd keep the context turned on so it could care compare the prices over time, right? Uh but the bottom line is is this
26:03
is just like the core functions of an app that could go far beyond this. You
26:08
could have 20 different agents trading on your behalf automatically. you could have um all kinds of other capabilities
26:15
built into this and integrations from other apps um of any kind
26:21
and and you know this is an example of doing command processing. It's looking for that slashpric command that you're
26:27
sending. Um you know we could look at let me make a new let me get a new
26:34
let me get a new display here. Let's we're done with this one for the moment. I'm going to kill this pipeline. Okay.
26:42
And uh actually, you know what? I'm just going to delete it so that we can redeploy it later if we need it.
26:48
Cool. We'll leave that one running. And then I'm going to get a new editor window
26:55
because I want to drag in that more complicated example that we showed last
27:00
week and see if it makes a little more sense. Now, that's a good idea. This is the one that was our Telegram
27:06
bot, right? If you remember, uh, we could, uh, send lots of different commands from
27:13
that particular bot. Let me, uh, zoom in a little bit. We'll start at the top. Um, this one had the chat agent where
27:21
you're talking as a human. Uh, that would look to see is is it a command or
27:28
is it something that just goes on downstream to be dealt with? And is it a command says that same regular
27:35
expression pattern match does the first character in the string match a slash?
27:43
If so, it might be some kind of command that we care about. This one down here says the exact opposite. It says, is the
27:51
first character in the string a slash and invert the condition? No. So for all the things that don't start with slash,
27:57
it goes down this other branch of the tree. Right? So where in that previous example all we had was one command slp price.
28:04
This thing assumes anything that starts with a slash might be one of our commands. Let's send it off to the bot
28:10
command swarm to be analyzed by the LLMs and see if it's something we want to do. If it's not, just send it on down to the
28:16
general purpose swarm and let it go flow out to all the participants in the chat. So, if we go down now and look at uh
28:24
what's going on with bot bot commands, bot commands down here is this swarm,
28:29
this swarm, and there should be I thought one below it. Maybe there's only two. Um, and it's looking for Oh, that's
28:37
right. It's looking for a command to reset the context, which resets the context for the LLM, or it's looking for
28:44
one that does help. And if it decides that it's one of the other commands that it knows that it's
28:51
processed over here, which is this persona parser, this thing's job is to
28:56
be an LLM that says, "Look, if you get a command that starts with a slash and it happens to be slashpersona,
29:03
you're going to change the persona that the LLM is using. We're going to under
29:08
kill bot control. If it's pirate, we're going to set the prompt for a pirate. If
29:13
it's baby or Karen, it's going to set it accordingly. And what these little things do is push out over a control
29:19
channel a new prompt string. You are a pirate sea captain. Always respond to
29:24
the user in character. Right? Or if it's the baby, it sets the prompt to be uh
29:31
you are a baby. Always respond to the user in character. And if it's the Karen, apologies to anyone listening
29:38
whose name is Karen. You're an angry middle-aged woman who's never happy and always wants to speak to the manager.
29:43
So, we've got really three commands. The persona command, which this LLM up here parses even further and decides what to
29:50
do with the reset context command and the help command. So, you can do slashresethelp
29:56
or slashpersona space one of those persona names. Well, that's go ahead. Sorry.
30:01
No, go ahead. And that's when we I you know kind of playing with this one initially was when
30:07
we kind of realized the capabilities that we had deployed here, right? because um not only can this chat happen
30:13
within Kilroy, this chat can happen in Telegram, it could happen in Discord, whatever you wanted to connect it to um
30:19
to be able to interact with it. So you could theoretically be changing the behavior of an agent and and its system
30:25
prompt by just sending one slash command to the system. And what that led us to
30:32
realize is that within this system is the ability for agents to do the same thing that we're doing with slash
30:38
commands, right? So based on things that are happening within a swarm in a pipeline, um an agent can then deploy
30:44
and change functionality of other agents and other components of a system and deploy it themselves. So, this not only
30:53
gives you the power to interact with your agents from anywhere, it also gives you the and change what they're doing
31:00
and how they're doing it, but it also gives you the ability to actually generate new functionality and
31:05
capabilities not within Kilroy, but from an external chat interface, right? And and this example just shows
31:12
changing the configuration of one LLM, but there are Kilroy commands to deploy
31:17
whole new agents, whole new LLMs, whole new tool chains, new swarms. So if
31:24
you're clever, you can build Skynet with this probably. Exactly. Exactly. Now we all we need is
31:30
some way to show the new ones that it generates. So So Brad, this is a this is a good little self-contained uh bit of demo. Do
31:37
we want to to show anything else or do we want to declare victory for today? No, I think let's declare victory. We
31:42
said we were going to try to keep these at 30 minutes. I think we're in good shape. That's excellent. Awesome. Well, this was fun. And if you
31:48
guys that are listening have any ideas or thoughts about other examples that you'd like to see, send them in the chat
31:55
or drop Brad a DM on any of the places that you know where Brad is. Well, yeah, Chuck's at Cshotton on
32:02
everything. Uh, Telegram, Farcaster, Twitter, etc. I am B05 Crypto on Twitter
32:09
and Telegram and B05 on Farcaster. If you would like to play with this, if
32:15
you're someone who's cool with beta testing early stage stuff, um, if not early stage, come on. Well, not
32:20
early stage, but if you're somebody who's likes to help people find things
32:26
to do with software and technology, you don't mind getting your hands a little bit dirty, um, getting set up, um, help
32:32
us, uh, build out new things and new ideas and be an early adopter that could potentially have some of the first apps
32:37
in the marketplace that people can buy and pay you for. Um, please feel free to
32:43
DM either of us. We'd be happy to uh, uh, chat about it and get you set up with it. So, thanks, Chuck. Thank you.
32:49
Awesome. Thanks, Brad. See you guys. Have a good day. Take care.
ROTP: How to easily build agentic swarms in Kilroy Part 1

Realm of the Possible
4 subscribers

Subscribe

1


Share

Save

Clip

14 views  Streamed live on Oct 24, 2025
Last week we did the high level overview! Today it's time to get into specifcally how to build easily in Kilroy. 

Learn how you can be empowered to build anything with a powerful decentralized automation and AI platform. 

We've created the most powerful decentralized AI and crypto automation/integration platform while being completely decentralized. It comes complete with no configuration peer-to-peer/swarming capabilities, the ability to easily build integrations with any other compute, and a Turing complete visual creation tool anyone can use to build functionality in the network while giving agents on Kilroy the ability to build and deploy on their own.
Transcript
Follow along using the transcript.


Show transcript

Realm of the Possible
4 subscribers
Videos
About

All

For you

18:59
I Built 4 SaaS Apps to $100K MRR: Here's My Exact Playbook
Starter Story
103K views
•
3 weeks ago


30:59
I Gave HACKED SCAMMERS a PANIC ATTACK On CCTV!
Scam Sandwich
10M views
•
9 months ago


14:31
How To Run Private & Uncensored LLMs Offline | Dolphin Llama 3
Global Science Network
832K views
•
10 months ago

17:35
The Future of Veritasium
Veritasium
3.4M views
•
5 days ago
New

18:57
The Infinite Software Crisis – Jake Nations, Netflix
AI Engineer
117K views
•
9 days ago

10:02
Can Magnus Carlsen Beat a Noob with 30 Queens?
Esports Spotlight
929K views
•
1 month ago

5:25
New Barista Training - SNL
Saturday Night Live
7.1M views
•
11 months ago

16:47
Office moments I think about way too often
The Office
1.7M views
•
4 months ago

13:24
LAWYER: How Cops Use Apple's NEW Tech to Spy on Your Phone
Hampton Law
381K views
•
8 days ago

13:39
I Built 3 SaaS Apps to $200K MRR: Here's My Exact Playbook
Starter Story
464K views
•
1 month ago

37:15
The Epstein Files are Worse Than You Think!
Patrick Boyle
1.3M views
•
2 days ago
New

25:57
Stop Rambling: The 3-2-1 Speaking Trick That Makes You Sound Like A CEO
BigDeal by Codie Sanchez 
191K views
•
7 days ago

16:44
THE CRAZIEST PART OF GETTING DIVORCED 😳 | Kelsey Cook Compilation
Spotlight by Laugh Society
424K views
•
2 months ago

12:35
How To Learn So Fast It’s Almost Unfair
theMITmonk
870K views
•
10 days ago

15:40
I Quit an AI Startup After 6 Months - Here's What I learned
Brian Jenney
191K views
•
2 weeks ago

33:14
Ethernet is DEAD?? Mac Studio is 100x FASTER!!
NetworkChuck
310K views
•
9 days ago

20:16
One Formula That Demystifies 3D Graphics
Tsoding
285K views
•
4 days ago
New

11:52
I shrunk down into an M5 chip
Marques Brownlee
 and Epic Spaceman
2.1M views
•
3 days ago
New

16:36
The Codebase Singularity: “My agents run my codebase better than I can”
IndyDevDan
11K views
•
15 hours ago
New

16:10
The Ultimate Linux Laptop is Only $120
Switch and Click
305K views
•
8 days ago


Show more
    


NL

Skip navigation
Search



Create


Avatar image
Transcript


Search in video
0:00
Okay. Telegram Chuck.
0:06
No, we got 30 seconds. 10 seconds. Oh, awesome. That's great. Right.
0:11
All right. We are live, sir. Okay. How are you? We are. How are you?
0:16
I'm I'm I'm ready for some holiday action. Some holiday action. What's uh what's
0:22
entailed in holiday action? A longass drive to Canada. Yeah, I've got the longass drive to
0:28
Georgia on Sunday. Yeah. Well, you'll be home and having some eggnog while I'm still somewhere in upstate New York.
0:33
Freezing your ass off, right? Yeah, exactly. Hoping the dogs don't claw their way through the windshield of some squirrel
0:39
they saw on the side of the highway. The dogs are making a cameo in today's show, by the way. You'll be surprised.
0:46
Oh, awesome. That's great news. All right. Uh, okay. So, uh, let's touch a
0:51
little news first. Um, I don't know if you saw this or not, but Appleind uh introduced this little um or model
0:58
called Sharp Well, it's Sharp Moninocular View Synthesis. Um,
1:04
essentially what it is, it lets you take a still photo and turn it into a 3D environment. Um, so
1:11
so it's actually pretty cool and it's actually seems to be um uh work pretty
1:17
well. I've seen a number of shots like this one um on um on uh Twitter uh of
1:24
people messing about with this. This is one I guess they took a picture of themselves on a bridge out in the wilderness and then it kind of turned it
1:31
into this 3D thing. I guess I wonder I wonder if it needs one of their lidar cameras because I can you
1:37
can kind of sort of see this in what is it the live motion pictures that are in your photo album on the new phones have
1:43
this motion in it but you can't go that deep into the pictures. Yeah. I wish they had more examples here on the uh on the website, but uh I
1:50
bet that's just I bet that's just playing with the big blob of data that they shoot with the LAR sensor when it
1:56
when you take a picture. Yeah, maybe. So people were raving about it on uh on um it says an approach to
2:03
photorealistic view synthesis from a single image given a single photograph sharp progresses the parameters of a 3D
2:08
gajian I don't know how to say that. I've never known how to say that representation of the depicted scene. This is done in less than a second on a
2:14
standard GPU via single forward feed forward pass through a new network. Wow.
2:20
New feature in iOS 26. Yeah. Yeah. Um we've talked about these guys before. This um uh product Skyurn,
2:28
which is a browser autom automation tool. They raised uh $2.7 million. So,
2:34
makes me want to get off my ass and raise some money. That's not the headless thing we were looking at last week. No, it's a different one. This is um Let
2:41
me see. I thought I had their GitHub repo open, but maybe I don't. Oh, here it is. Um,
2:49
this one adapts automatically to uh changes in pages, so you don't have to
2:54
worry about it a page staying the same all the time. Oh, that's good because that was a problem with the other. Oh, give me the
3:00
fourth div inside of the fifth thing was the third paragraph and oh yeah, somebody put a I mean there's sites like
3:06
LinkedIn that actively randomly generate different structure every time you load the page just to screw with scrapers.
3:13
Yeah, Skyarn relies on vision LLMs to learn and interact with websites. So it
3:18
doesn't use DOM parsing or XPath or anything like that. It just uses the vis
3:24
off the screen. Yep. Yeah. So it's essentially trying to act like a human interacting with it. So
3:30
might be a cool and it's open source. So could be a cool integration for us. I I would assume they raised money on the
3:35
basis of providing a service uh SAS wise andor um maybe they would um do enterprise
3:44
integrations for Yeah. And that seems to be the model here. The other headless one we were looking at is oh well you could run this
3:49
but it's hard so use our cloud service. Exactly. Exactly. So it's interesting because they you can you know they
3:56
provide all the SDKs for all the primary environments and um you can uh use your
4:02
own browser use theirs etc and enter your credit card here. Exactly. Exactly. So anyway it's um uh
4:08
browser tasks browser actions data extraction validation for loops file parsing sending emails text prompts HTTP
4:15
request block. Anyway um looks like something could be really interesting for us to play around with a bit. Um, I
4:22
don't want to show that. That's boring. I found this API. Um, and you know, again, it's kind of a centralized thing,
4:28
but it's not an it's not AI first thing, although it does talk about AI on their
4:33
website. This is a simplified integrated social media data basically scraping
4:40
system. I guess it's not scraping. I guess they have API connections for everything, but uh every major um social
4:48
media platform including Twitter, Instagram, Tik Tok, you know, that's good. You know, the whack-a-mole game we've had to play with
4:53
trying to get feed data out of Twitter, right? Yeah. Yeah. And Twitter charges a fortune. So, the average person can't
4:59
really um make use of it. So I guess these guys are paying the whatever it is
5:05
25 or 50 grand a month to Twitter for the for the API calls and they parse it all together for you so you can uh so
5:12
you know ranges from a free trial to $399 um for uh for access to the data. But it
5:22
actually was pretty cool because it you can do all kinds of uh twisting and turning of the data and making sure that
5:29
um you can find the latest engagement. You can do sentiment ratings. You can do all the good stuff that we would have
5:34
liked to have done with it. So yeah, here analytical dashboard, sentiment analysis, ad intelligence, influencer
5:39
research, market research, ANI machine learning. Those are just all examples. But um I can see this be a very valuable
5:46
tool. So um yeah, come by the social media streams for us so you can train your LLM on live
5:51
data. Exactly. Uh Salesforce launched um a framework for LLM prompt optimization.
5:59
Um and I haven't given this a What's that? Promptomatics.
6:05
Yeah. Um but um you know it's what I do
6:10
like about here's what I like, right? Um so Salesforce is obviously doing centralized AI. It's running it all
6:16
through your CRM data, etc., etc., and they want to integrate with corporations and they have a big corporate install
6:21
base. So I'm not a big fan of that part. Um, but I do like when companies like this
6:26
release um, open source products uh, that others can use to build with.
6:31
I don't want to jump ahead to our impending fear-mongering segment that we're going to do, but this seems like,
6:37
hey, let's frontr run the AI cloud people and we'll get all the prompts and all the results through our stuff so
6:44
that a OpenAI and others aren't the only ones that are scraping all this data out of our corporate customers. Well, it
6:50
could, but I mean, if you're running this locally on your own machine, you know, then you're not giving them anything unless
6:56
you know, that's just me being paranoid, I guess. Well, you know, it pays to be paranoid these days about everything. So, you
7:02
know, um and you brought this to my attention. I hadn't seen it that chat GPT is loaded up um a simple image
7:09
editing capability into um chat GPT itself instead of being a separate uh
7:14
image editing tool. And um it looks like it does a pretty good job. I mean, you know, you've got um uh painting in the
7:22
image, you've got editing out of specific things. Um I haven't played with this because I don't use chat GBT.
7:27
I mean, I just saw the announcement yesterday, so it's pretty new. Yeah. Um but it it does look like it's a
7:33
pretty good product. Um I would say that there's probably similar functionalities available in all the major stable
7:39
diffusion editing applications, including things like diffusion B for the Mac, which is open source and local.
7:46
But um you know they're really trying to up their game right now and they're under a lot of heat and pressure and as
7:51
we mentioned last week uh they see um Apple as their top competitor. Uh so
7:58
which I think is a smart move. Anyway, um let me see if Oh, this one is great that you sent too. So I I did read
8:05
through this a bit after you sent it over and um this is a system to remove
8:10
all of the AI stuff that's forced upon users in Windows 11. um, Copilot, etc.
8:16
And why would we do that? Um, we would do that because Apple, I mean Apple, Microsoft is storing all of your data.
8:23
Um, they're taking everything you do and as we mentioned last week, we had a
8:29
chart that they released from their research team um, showing how people are using it. Well, the only way for them to
8:35
know how people are using the AI, what they're looking for, what problems they're trying to solve, what tasks they're trying to do is because they
8:40
track everything you do. So having a tool called rem called remove Windows AI
8:48
um is a great thing because it removes co-pilot um it disables recall or not
8:53
removes disables I should say um takes copilot out of edge um gets rid of the
8:58
image creator just basically removes all the AI related services or disables them. So I'm really glad to see a
9:05
platform like this. Yeah, I'll try it on my on my PC here. The one that sits over here quietly in screen saver mode that I touch once a
9:11
month. Yeah, of course we don't have to worry about that. Why? Because we use Apple products, but
9:16
Well, I do have to build the occasional version of Kilroy over there, but that's it.
9:22
Yeah. Yeah. Otherwise, the only time I deal with it is when my mother has an issue and I have to go do a Google
9:27
search for it because she doesn't seem to be able to do that. You're giving her an iPad for Christmas, right? I should. I should. She still teaches.
9:34
She's a substitute teacher now. She's a retired teacher and so she has to have a device.
9:40
Yeah. Yeah. Unfortunately. Unfortunately. All right. What else? What else? Uh that's what I have for the for the
9:46
day, sir. Where are we? Oh, yeah. Just in time. 10 minutes for fearongering.
9:53
So So let me let me let me say why I'm saying that because it's it's
9:58
serendipitous or not depending on your perspective. This week I've had three separate conversations with three
10:04
different people who are either extremely wellconed in the corporate universes of Fortune 100 businesses or
10:11
otherwise have been around the block several times all about how they're seeing people in the industry using AI
10:18
and in in their industry. These are Fortune 100, Fortune 500 businesses, thousands of employees. What are they
10:25
what are they hiring for? Right? how are they hiring and what are the people doing after they're hired? And and the
10:33
universal theme which just frightened the heck out of me that they didn't understand the ramifications were, oh,
10:39
these big companies are buying like corporatewide site licenses for all
10:44
their employees to have chat GPT on the desktop. Woohoo! Great. Look, we're leading edge. Everybody's AI enabled.
10:50
And I asked this guy, I said, 'You know, did you ever wonder if the company has
10:56
any policies about what you should or shouldn't do with those AIs on your desktop? And he looked at me like, why?
11:02
And I said, well, suppose that I'm in the CFO's office and my job is to put together the quarterly report for the
11:09
upcoming shareholder meeting, right? and it's a month out and I'm getting the numbers and I'm putting them together
11:15
and I'm asking chat GPT to help me put together the PowerPoint and I'm asking chat GPT to write the investor memo and
11:21
I'm writing chat having chat GPT write the press release and I'm doing all these queries and I'm saying chat GPT
11:27
but you pick your poison of choice whichever cloud-based service it is and you're feeding them all those prompts
11:33
and they're generating all the results and you're going to be a hero and the CFO is going to be happy with your work and it's going to be a great investor
11:39
meeting except you just put all the data about your company's performance, your product strategy, everything that's been
11:45
going on into somebody else's cloud. And I don't care what the privacy policy
11:51
says. There are some little somebody's up in there that are looking and front running your stock because they know
11:56
what you're going to report or your annual report. They're selling your data to your competitors so they know what
12:02
your product strategy is going to be, blah blah blah. and and the people that are using these inside these businesses
12:08
have not been trained, have not been told, nor have their bosses or their bosses had a clue get caught about the
12:15
type of information leakage that's going on with these things. I mean, I after you think about it a little bit, why
12:21
would you ever use one of these things for anything other than generating goofy pictures and telling yourself a story?
12:27
Because it's just it's corporate espionage to the nth degree.
12:32
Yeah, it's insane. And I it's the the the the
12:40
delusion of these people who are so desperate to have AI in their corporation
12:46
um to think that this is going to be safe and protected for them is just insanity. It's it's absolute insanity.
12:54
Front running somebody could be paid to divulge information from within the corporation. I mean, how many time
12:59
freaking Coinbase like had a problem with support personnel stealing
13:04
identities of people because a support person in Asia doing with Grock.
13:11
I mean, right, he wants to put and there's no doubt that the whatever you've got as a as a privacy policy or a
13:17
terms of service says we own everything, including your retina scan for looking at us on the screen.
13:23
Exactly. Exactly. Well, I I mean, look, it's funny because I was talking the other day about um Grock signed a deal.
13:30
Look, we already know Elon has manipulated Grock to his political viewpoints, right? Whatever your
13:36
political viewpoints are, that should not ever be okay with you. It should not
13:42
be okay that your artificial intelligence tool that will eventually when all of computers is AI um
13:50
has a has a founder who's controlling the information that you get out of it.
13:56
The knowledge and doesn't tell his media companies what to say. It's kind of the same thing once
14:01
removed. Right. Right. Of course. Of course. And that's what you're going to get centralized no matter what, you know. And then we got
14:06
we've got Sam. Go ahead. Sorry. Well, I was just going to say we have for media
14:11
outlets now, there are clearing houses that do evaluations of media bias,
14:17
right? And you can go look and see who's middle of the road, who's right, who's left, you know, whatever, and see that
14:22
and use that to maybe make a decision on where you get your your news from. It's going to end up having to be the same
14:28
way with the AIS, right? They're going to be no more different, more or less like a a you know, a a news network,
14:36
right? They're going to have their bias. They're going to have the things that they do well or not. And and it's funny
14:42
because I remember if you'll remember from last week when we were trying to get the
14:47
stupid little LLM to decide whether to buy or sell, right, on the numbers. I tested some stuff
14:53
across four or five different LLMs from four or five different projects, right?
14:58
Quinn and Mistral and some stuff from Meta, several other models. And they all
15:05
suddenly had this same weird failure mode. When you gave them the same prompt and you gave them the same inputs, they
15:10
all got it wrong the same way. Every time I was comparing two numbers and the first number was the number one, they
15:17
all got it wrong. So they've all been trained on the same math data, right? They've all used the
15:23
same training set. They're all screwed up the same way. And imagine when that starts to creep into the the generated
15:30
training data when they're using synthetic data to train these things. Now they're using wrong synthetic data, right?
15:36
Using the looking at these these things as sources of knowledge or sources of truth or sources of accurate information
15:43
is frightening because if you don't know what the answer is before you ask it, you have no way to know if the answer is
15:48
right. Yeah, absolutely. Absolutely. Well, look, I mean, here's my biggest, well,
15:53
not my biggest, one of my biggest concerns is school districts choosing
15:59
these to educate our children, right? And that's going to happen. And last week or week before last, El Salvador
16:06
announced Elon and and President Dictator um in El Salvador announced
16:12
that they are now going to institute Grock to educate all of the children of
16:17
El Salvador with AI. You know, I'll tell you, this is interesting. This conversation just came up with my daughter yesterday. She just
16:24
finished her senior year at at VCU and she's going off into the real world. And somehow we got off on the subject of
16:31
using AI for schoolwork. And she said, "I am so glad that this stuff did not exist when I was in high school because
16:38
I learned how to write. I learned how to review my stuff. I learned how to think about whether or not I was communicating
16:44
clearly and I didn't just turn around and rely on some tool." And she said, "You don't know how many of my peers in
16:50
this university can't write a document longer than two paragraphs if they can't
16:56
use chat GPT to do it." I believe it. I totally believe it. Well, my fear is indoctrination. I I
17:02
just don't want for my own personal political beliefs, I don't want Elon Musk teaching my
17:08
children from 5 years old everything they need to know about the world. Just not what I want. And and that
17:14
Well, you're going to be a homeschooler then, aren't you? I I would have to. Thank God my kids are almost done and have I have
17:20
indoctrinated them properly. So, so, so, so along our theme of of of
17:25
fear-mongering, um, one of the things that, that was, well, you know, we've
17:31
talked all along about them swiping your data, but here's a a thing that people aren't going to realize, and I pointed
17:37
this out a while ago. I don't think we've done it on the show yet, but um, imagine if you are a software engineer
17:43
in an organization, and you're tasked with coming up with some comparable product to some competitor, right? And
17:49
and there's all sorts of of development shops out there whose job it is just to clone something, right?
17:54
Sure. Sure. And at least in the US market, you better be doing a clean room implementation of that because otherwise
18:00
you're going to have all sorts of copyright infringement and lawsuits and willful infringement damages and all
18:05
sorts of other things that happen when you rip off somebody else's IP. But here's the problem.
18:11
Every developer in the world is using Visual Studio Code now has C-pilot hiding in the background feeding in
18:17
stuff. I I don't I try not to use it, but sometimes it just offers up stuff that's too convenient not to hit the tab
18:23
key and use. It's like, oh yeah, I was gonna type that. You typed it for me. Great. Good. Good job. Yeah. But if you're in a clean room scenario
18:30
and you're asking this thing, I need to I need a routine that does X, Y, or Z, right? Or works with certain API or does
18:35
a certain thing with a database or some certain function that's a key critical piece of that competitor's software.
18:41
Because Microsoft owns GitHub and because they fed the entire contents of GitHub into these LLMs when they trained
18:48
them, there's a nonzero chance that when it offers up some code for you, it's offering up an unadulterated snippet of
18:54
your competitor's source code for you to merge into your product. I just saw this the other day. I asked it a question and the answer that it
19:01
gave me and the chunk of code that it gave me was already open in another window on my desktop from a Stack Overflow comment. It was line for line,
19:09
variable for variable, indentation for indentation, identical to the Stack Overflow answer, and it dropped it in my
19:15
code. So now you are infringing on IP whether you know it or not. No clean room implementation is ever going to be
19:22
clean if you don't if you don't ditch the AI and run it by build. Who's in who's the infringer? Was it
19:29
OpenAI for giving you the infringing code? Was it Microsoft for giving you the infringing code? Was it you for not
19:34
knowing that it was infringing? How are you going to know? Right? you can't see the original code. So people I think that there are all
19:40
these little subtle gotchas that are coming down the pike that nobody's even really thought about. And if I was an
19:46
executive in a big corporation right now, I would have 10 people in a room all day every day thinking of all the
19:52
horrible crap that's going to happen because I'm not managing this stuff properly because it's going to wreck a lot of people.
19:58
All right. Yeah, it is. It is going to wreck a lot of people and it's going to make it um it's going to do really interesting
20:04
things in terms of intellectual property. Right. I think it's a I think
20:09
the battle I think that this idea of protecting
20:15
everything is probably going to end up falling by the wayside is at least digital things because it's going to and
20:21
that could include everything. You can see the writing on the wall has kind of been there for at least in the software
20:27
world with things like the Alice decision for patents, right? If all you're doing with the software is automating something that you can
20:34
already do, that's not a patentable thing, right? Exactly. And so, so yeah, there's a copyright
20:41
issue, but I didn't copy out of your original source code and I didn't willfully infringe. So, too bad. So sad.
20:49
Yep. Right. Yeah, you should have you should have hidden your source code away and not let it be visible to the AIS that scraped it.
20:56
One more quick tidbit and then I have a really cool toy that I want to show you that we did in Kilroy over the last 24
21:01
hours. Um, I had discussion last night and I don't want to name names because I
21:06
don't know how confidential this was, but a big networking vendor is rolling out this product which you know we're
21:12
we're champions of of local LLMs doing all the compute on your own your own AI or it will Yeah. and and
21:20
these guys are rolling out a product which is well they'll deliver to your place of business a rack full of AI
21:26
ready machines allworked together so that you have a cloud looking service on
21:32
prim that looks like a chat GPTP or a claude or whatever right and so the
21:38
their argument is oh yeah well we'll just keep all our our data in house and you will except now you've got the
21:43
scalability issue if you're a fortune 100 company and you want to replace an open AAI subscription with a rack of
21:49
your own gear. Well, okay, go stand up the AWS size data center that you need
21:55
to support your 15,000 worldwide employees doing this all day every day because that's not going to be enough.
22:00
Exactly. And he was all excited about this product because, oh, it's going to be killer. We're we're, you know, sticking it to the guys who are in centralized
22:07
cloud mode. And I said, let me tell you what we're doing. you you could put our software on every
22:14
PC on every desktop in every office in your in your organization and not just
22:20
have a hundred machines in a rack for everybody to use. You could have 10,000 machines all swarmed together.
22:26
Maybe they can't all run LLMs, but a bunch of them can. And when they're idle, they can help their buddies. And
22:32
so now you've got a swarm of 5,000 AI boxes instead of a hundred. And Oh,
22:37
yeah. It's just a double clickable app. Exactly. and and it's easy to access, easy to utilize. I mean, I think later
22:44
on we'll add some functionality for combining GPU capabilities because there already exists on the market. There are
22:49
open source products that allow you to combine GPU resources resources for multiple machines together to help an
22:55
LLM. There there's unending possibilities and it wipes out the need
23:00
for all this data center crap. So, I definitely think it's uh it's uh
23:08
powerful model for for people to utilize our stuff. Let me show you my cool toy that I made. Yeah, I am so excited about this, but
23:15
also a little [ __ ] overwhelmed by it. Well, the thing is this is something
23:20
that's been in Kilroy for a long, long time, and it was just because of a comment you made a few days ago about, well, why can't we put web pages and
23:27
have a website in our in our in our cloud swarms? and it's like, well, there's no reason why we can't. Let's do
23:33
it. So, today live on the air, we're introducing, tada, mini apps.
23:40
Apologies to Farcaster and Tell, right? Well, well, I mean, they Farcaster and and Telegram and Discord
23:47
and a bunch of these other uh social platforms have had this idea of mini
23:52
apps for a while. It's a little self-contained piece of HTML and code and buttons and logic and whatever that
23:58
can get shipped around to the clients and run to do game things,
24:04
cryptotransaction things, fill-in- thelank form things, polls and messages and whatever. And and this certainly
24:11
doesn't look like one of those on the screen, but I just wanted to show this first and I'm going to go run the mini
24:17
app and then we'll come back and look at what this does. But great. If you're familiar with the inards of a lot of different uh web backend applications,
24:24
you have a router in there that handles requests from the front end. And look, we have a router here in here for page
24:29
one, page two, page three. And in some cases, they're returning static data. In some cases, they're going off to an AI
24:35
swarm to compute the page. And so what we're able to do now with this little
24:41
tweak, this little conceptual tweak in Kilroy is have embedded little miniature
24:50
HTML sites inside of Kilroy widgets, inside the dashboard that act just like everything else that you've ever played
24:56
with on the web. So I'm going to start this up and it's not going to be exciting because I just did this yesterday in about 45 minutes, but
25:02
yeah, we could walk through some ideas, right? So, so this is this is a little mini app. It starts out, oh, merry
25:08
Christmas. TW was the night before Christmas, right? And it's got a couple of links in it. Now, this is the interesting thing because this page is
25:15
being served through our swarm from a pipeline. These links go back into the
25:20
swarm to say to the pipeline, go find me the content that's related to this message. Right? Everything is going in
25:27
and out of the swarm basically as plain text. These are not HTTP requests going out over the internet. It's not the
25:33
browser fetching the code. It's the agents in the pipeline fetching the page data and showing it to you in the in the
25:39
viewer here. That might sound weird. I'll show you in a minute how it works. But if we go off to page two and like
25:45
you would in a normal little website or mini app, tada, here's page two.
25:51
There's kids in there. I love it. Making their holiday appearance. This was a stable diffusion uh image, by the
25:57
way. These are Chuck dogs. And and and so it's it's all self-contained. It's all in here. We
26:03
haven't messed with any of the other Kilroy apps. All the other stuff that's running is still running. But this is its own little universe in here that's
26:09
being served up by this pipeline called test links. If I go back to the first
26:14
page, we're going to do something that's going to be painfully slow. So, you know, we'll play the hold music here. I'm going to ask one of the links is
26:21
will it snow for Christmas? Great. Well, what that's going to do is actually go fire up Llama 3. And while it grinds
26:27
itself to life and loads up Llama 3, it's going out to the network, pulling in the weather data that we've seen it
26:33
use before, sending that JSON blob over to the LLM. The LLM is turning that into a HTML table and a nice little weather
26:41
app. It's great, right? And that was all done inside of a Kilroy pipeline. It didn't require any
26:48
code. It didn't require any special ability to write HTML or do anything.
26:53
Um, and you've got a little live website that you can navigate around between and do things.
26:59
Yeah. And I I think it's really important to think through this, right? So the this does a couple of things.
27:06
First of all, at at a very simple level, it gives a nice little HTMLbased interface to pipeline, swarms, agents,
27:14
etc. Right? It allows you to easily create um a UI to access swarms. And the
27:21
cool thing is it could be multiple swarms. This could be a a dashboard that accesses all the things you're keeping
27:28
track of all day. And you could have data feeding into the dashboard. You could expand the size of this frame that
27:35
is holding the HTML. Um it can um allow you to interact with
27:41
itself. Yep. It can it can allow you to interact with and um execute different swarms and
27:47
pull different swarms into things that you're working on. This opens the door to even more access.
27:54
And here's one of the things that we have grappled with building Kilroy. We
28:00
have all these great little swarms and apps that we create, but we end up with like the little box to the right of the
28:06
white screen and the little box to the left of the white screen. We often end up with dozens of these little boxes on
28:12
our screen. Sometimes that's great because it operates as a dashboard and that's how some people would prefer to
28:17
operate. But with this kind of an interface, we can now eliminate a lot of those little boxes and allow almost all
28:24
of the functionality that somebody might be utilizing different agents and swarms for all to happen within HTML. I I think
28:32
the I think the major pardon the the trit phrasing the major
28:39
paradigm shift here is that clicking on these things is not telling the browser
28:44
to go to this URL like correct 100% of all the other websites clicking on this thing is actually pushing a
28:51
message into the swarm for all the other agents to see. When I click this it
28:56
actually pushes a message into I'll do it on this one because you'll see it up there for us a minute. It pushes a message into the swarm that is slashpage
29:04
three. And you can see it happen right here. That's the message that went into the swarm. And if you look over here at
29:09
the logic, there's a handler for seeing a message coming in that h that matches
29:15
the regular expression page three. Right. Right. And when it gets that, it says,
29:20
"Oh, let me just go over here to the weather swarm and kick things off." It's
29:25
going to send out a static message that's actually my zip code, right? 2076
29:31
and it sends that zip code down here which goes off to our get weather tool which takes a plain text zip code as an
29:38
argument. It goes out and gets a JSON blob of data that's the weather and it sends it back into the swarm. This
29:43
weather agent has a cool prompt, right? It says you're going to receive some JSON that contains weather data. Convert
29:48
it to HTML. Also put a small image 64x 64 using the URL from in the JSON,
29:54
right? follow that HTML with a table that summarizes the current conditions and don't output anything else. So, it
30:00
does that and jams it back into the swarm. This little guy's job is to listen for the finished weather report.
30:06
It wraps it with a little bit of other HTML here, a blank line, a heading that
30:11
says, "Hey, this is the Leburg weather report and the link that you can click to go back to page one." And if you look
30:18
at the text that it's in here, this is the magic guju of what we're doing. Kilroy exposes one little function in
30:24
JavaScript back here in this chat window that anybody can call. And if you say send one message from Kilroy and send
30:32
the text to the message, it puts it back into the swarm that it came from. So you can have pre-anned uh queries that go
30:40
into the LLM. You can have commands like this that are structured to do navigation and state change in the in
30:45
the pipeline, whatever you can imagine. So it's not just a URL. It's a call potentially to a back-end service. I
30:52
mean, this is the equivalent of a web service that goes out and makes an API request for weather data and then
30:57
somehow renders it out as an HTML table. But we did it all with a single a single
31:02
prompt to the LLM. Yeah. So now I'm writing I'm writing backend web service code with three sentences in an
31:09
LLM. Yeah. It's amazing. And and and and not only so so this is an example of you
31:15
know you see here on this diagram three swarms, right? Well, you could chain multiple swarms together and have them
31:22
be the next step and the next step and the next step that you're clicking through to. So maybe maybe you scraped
31:28
use that social data API that we talked about earlier, right? And it pulled down, you know, the highest engagement
31:34
tweets and maybe we we create a methodology to actually pile into the
31:40
accounts of those tweets, right? Or or pull in other data related to it or find
31:45
data about that person that tweeted it. It may be that this swarm is on my
31:51
machine and it handles some static pages, but I've got a special page that goes off to your machine where you have a swarm that generates pages for me to
31:58
look at. And the same way that I can navigate from website to website with links and cross links and and whatnot
32:04
between regular websites, you'll be able to navigate around between Kilroy installations and people's agent swarms
32:11
the same way. Yeah. Well, and and what we didn't even really get into was was the reason I made that comment. Well, I think it was
32:18
the reason I'm I'm I'm going to remember it that way right now. Um was that we were talking about the fact that built
32:24
into Kilroy is everything you need to create
32:29
your own personal social media network that's completely peer-to-peer, right?
32:34
And so, how do we have profiles or how do we have feeds of your posts that
32:39
other people can tap into or subscribe to and get, right? And so, that was kind
32:45
of the the origin of, hey, how do we make this happen? How do we create this? But now, think about it. You can now
32:52
utilize this functionality as a person that's doing a social feed publicly or
32:59
privately in Kilroy to provide functionality to people to expose access
33:04
to apps and agents that you've built to um share information with them. So it it
33:10
opens up so many possibilities to have a live stream like this coming into it, right? Um so it really um really
33:18
You're excited, Brad? I am so excited. It is just ridiculous. I I I I just need to abandon everything
33:25
else in my life and just Well, so so we've run out of our 30
33:30
minutes. We'll leave Yeah. Well done, though, man. I'm excited we have this now. It's great. We'll leave Maple and Coconut up to take
33:36
us out. But beautiful. I'm kick clicking the instream button now. Hope everybody has a great day. If you're interested in
33:42
playing around with, building with, integrating with um whatever for Kilroy,
33:47
we want to hear from you. B05 crypto on Telegram. B05 Crypto on Twitter. Chuck
33:52
is Seshotten on all uh social networks and I am B05 on Farcaster. We'd love to
33:59
hear from you. DMs are open. Please.
ROTP: Building in Kilroy Part 5

Realm of the Possible
4 subscribers

Subscribe

0


Share

Save

Clip

No views  Streamed live on Dec 19, 2025
Learn how you can be empowered to build anything with a powerful decentralized automation and AI platform. 

We've created the most powerful decentralized AI and crypto automation/integration platform while being completely decentralized. It comes complete with no configuration peer-to-peer/swarming capabilities, the ability to easily build integrations with any other compute, and a Turing complete visual creation tool anyone can use to build functionality in the network while giving agents on Kilroy the ability to build and deploy on their own.
Transcript
Follow along using the transcript.


Show transcript

Realm of the Possible
4 subscribers
Videos
About

All

Computers

For you

53:44
Building the PERFECT Linux PC with Linus Torvalds
Linus Tech Tips
4.5M views
•
4 weeks ago


14:19
What Sam Altman Doesn't Want You To Know
More Perfect Union
2.3M views
•
10 days ago
Fundraiser


13:39
I Built 3 SaaS Apps to $200K MRR: Here's My Exact Playbook
Starter Story
464K views
•
1 month ago

20:30
Instagram Pilot's Emergency Caught on Camera!
Pilot Debrief
1.2M views
•
8 days ago

25:32
Navy SEAL Destroys Karate Black Belt In SECONDS
Jesse Enkamp
1.7M views
•
12 days ago

23:42
My project was stolen and sold for profit
Nerdforge
1.3M views
•
3 days ago
New

30:59
I Gave HACKED SCAMMERS a PANIC ATTACK On CCTV!
Scam Sandwich
10M views
•
9 months ago

13:24
LAWYER: How Cops Use Apple's NEW Tech to Spy on Your Phone
Hampton Law
381K views
•
8 days ago

24:19
NEW Walking Method 2x Better Than Running?! (Tested)
Jeremy Ethier
620K views
•
8 days ago

25:57
Stop Rambling: The 3-2-1 Speaking Trick That Makes You Sound Like A CEO
BigDeal by Codie Sanchez 
191K views
•
7 days ago

10:02
Can Magnus Carlsen Beat a Noob with 30 Queens?
Esports Spotlight
929K views
•
1 month ago

5:47
Chef Show - SNL
Saturday Night Live
12M views
•
2 years ago

9:23
"AI Can’t Replace Juniors" - AWS CEO
The PrimeTime
262K views
•
2 days ago
New

17:35
The Future of Veritasium
Veritasium
3.4M views
•
5 days ago
New

17:08
When AI Gets an Innocent Man Arrested
EWU Bodycam
3M views
•
2 weeks ago

21:43
Why tech billionaires are quietly bankrolling Europe’s far-right | Pinch Point
Al Jazeera English
313K views
•
13 days ago

18:55
the creator of Claude Code just revealed the truth
Wes Roth
7.4K views
•
3 hours ago
New

21:10
The Attention Economy Is Everywhere. Self-Hosting Is the Escape.
Nathan Laundry
312K views
•
2 months ago

11:05
Will Ferrell Refuses To Discuss The Bird On His Shoulder | CONAN on TBS
Team Coco
1.4M views
•
3 months ago

5:25
New Barista Training - SNL
Saturday Night Live
7.1M views
•
11 months ago


Show more
    


NL

Skip navigation
Search



Create


Avatar image
Transcript


Search in video
0:00
How are you? Good. Welcome to Realm of the Possible by Kilroy.
0:06
Um, real quick, you just got back from New York. I did. It was uh Did you make Did you make it to the most
0:12
important eery in uh Oh, it's using my
0:18
other camera. What the fing the most important ery? The most important ery. I'm wearing my
0:25
uh t-shirt from you know no we we opted for the other the other brand that's the most
0:31
important to read the Eminem store. Oh well the Eminem store. I thought you were going to tell me Dunkin Donuts and
0:36
then I was just going to kill this stream right now. Anybody that eats Dunkin over Krispies.
0:42
Well you know I used to go to Dunkin Donuts all the time in Penn Station and they've completely destroyed the Penn
0:47
Station vibe and tried to make it respectable with this Moahan train palace thing that they built. So
0:53
there is no more crispy cream before the train. It's too bad. It's too bad their donuts suck because there is an amazing book
1:00
about the story of the founder of Dunkin Donuts that's just incredible, how it
1:06
was built, um his father started it, blah blah blah. But it's just really impressive, really cool story. But
1:13
anyway, I'll always I'll always be a fan of Dunkin Donut Crawlers because nobody else makes them. But
1:18
well, there you go. The rest. The only doughnut I need in the world is a hot fresh crisp crispy
1:25
cream glazed. That's it. That's all I need. That's all. And I can eat I could easily eat I know I can eat six because I've
1:31
done that multiple times. But I could probably eat a dozen in one sitting. Dude, in college every Thursday night
1:37
was Hill Street Blues night and one of the fraternities would come around with a pickup truck full of hot now don hot
1:43
now donuts and we would have a six-ack of Jennese Cream and a dozen donuts while we watched Hill Street Blues.
1:49
That's awesome. And that is dating your ass with Hill Street Blue. Totally. I just nail it all. But that was that was from Crispy Cream Store
1:56
number three, my man. Holy [ __ ] Original stuff. OG crispy cream.
2:01
Awesome. Oh, we have one here. That's probably one of the originals. But anyway, um yeah,
2:08
let's talk about Well, today we're going to um go further with our trading
2:14
pipeline, right? But um before that, let's talk a little bit of AI news and
2:19
stuff if you're good with that. Uh let me share screen. Probably should have done that
2:25
beforehand, but whatever. Uh
2:32
can I do an entire window, please? Yeah, everybody can see the four million
2:37
tabs I have open. Okay. Uh is it on screen? It is. What are you seeing?
2:44
Almo. Perfect. So, you know, I am a fanboy of
2:49
Allen AI and that is Allan of Paul Allen, uh, one of the original Microsoft
2:55
bros, and he left a [ __ ] ton of money to create an AI organization years and
3:02
years ago. And uh Allen AI or AI2 as
3:07
most people know it um has been creating really amazing powerful
3:14
excuse me open-source LLMs open source to the point where they disclose every
3:21
detail of every piece of the process from data creation all the way through to training and every bit of how they
3:29
build every model they release. They're pure open- source. No strings attached,
3:34
no IP API calls you have to hook up to or anything else. And they're they have
3:39
a lot of really cool tools. They built one of the first um iPhone apps that allows you to utilize LLM on your phone
3:47
uh integrated. Yes. Yeah. So, really impressive stuff. I encourage anyone um that is interested
3:54
in decentralized AI and open source AI to go to allenai.org.
4:00
or I will tell you the only centralized AI tool I use is their product called
4:08
ATA. If there is anything I want to know about anything from supplements to
4:14
illnesses to education to any topic, this is an AI engine that has sucked in
4:23
hundreds of millions of research papers and keeps it up to date so that you can come in here and ask it any kind of
4:29
question you like and you will get tons of research references and summaries of the research. I would encourage anyone
4:36
that's doing any kind of research or any kind of work to definitely check it out as you can see on my left sidebar here.
4:42
I use it almost every week, a couple of times a week. Great tool, fantastic to
4:47
use and that's at asa.allen ai.allen.ai. Um really amazing powerful powerful tool
4:55
and great uh example of utilizing um AI the right way. Um, this week we looked
5:02
at uh we looked at this research paper that came out um of this uh uh
5:09
the the one that documents the Kilroy approach to building pipelines. Exactly. Uh and I posted I said, you
5:15
know, it's great when the scientists catch catch up to you, but it also kind of sucks, right? because essentially um
5:21
this was um a research done to uh solve what is used on a bunch of different
5:27
LLMs for testing and benchmarking but it's a millionstep LLM task and they accomplished it with zero errors using
5:34
only small small very small off-the-shelf models so 7B and lower
5:40
models and the reason it worked you want to explain it Chuck well it doesn't have room for error when
5:48
it's a single little step of the prompt and you're really doing some natural language processing and some simple
5:53
calcs right there in the model. It it doesn't have room for error. It's not going to hallucinate because it's got a
5:59
context that's three miles deep. It's not going to hallucinate because you've said the same thing five different ways and it's confused. and and it makes it
6:07
really easy to set up for tool calls between the steps to clean up the arguments or to do mundane things like
6:14
go look up a piece of data that uh can be used by the next step. So that step by step process of breaking it down,
6:22
it's how people work. You don't you don't get like the full instruction manual for your Lego kit that you're
6:27
going to put together and do it all at once. Right. Exactly. Exactly. And and again, it's
6:34
all about breaking pieces down into the smallest component possible so that you never need, you know, chat GPT level
6:43
LLMs. And you can solve the same kinds of problems that the big ones solve, but
6:48
you lose all of the hallucination problems. You lose all of the inefficiency. You lose all of the
6:54
expensive cost. And that's exactly how we're building Kilroy right now. You're working on that this week, making it
7:00
even more efficient. that is on the plate and we can have a little tiny demo of it today later when
7:06
we're ready for that. Beautiful. Um, this is another one I think we'll move into the thing because I talked too much about crispy cream,
7:13
but um, this is another one that I'll put in the show notes. Um but it's again
7:19
utilizing the same kinds of models that we use in terms of breaking things down into small pieces and performing uh
7:26
beautifully because what we do is break everything down into singular tiny
7:32
context agents that um never have to learn uh what they're supposed to be
7:37
doing again because they always know what they're supposed to doing. They have a context around what they do. So
7:42
had a bunch of other stuff but I don't want to run out of time for what you want to show us today. So, we'll Oh gosh, you're going to make me fill
7:47
the solid 20 minutes that we have. Okay. Oh, I can give you another five minutes of stuff here. Like, here's a
7:53
cool stuff. I mean, some of this stuff about yet, so let's look at the this new robot that
7:59
came out this week is pretty The interesting thing is, um, this is
8:04
actually, I guess, shipping now. Uh, it's called Memo and it's a home robot.
8:11
Um, and they give you examples of making sure your children are the most spoiled
8:17
possible children on the planet and that they will never have to clean up after themselves again because you can have
8:23
this robot in your house. Um, man, that robot looks like that thing they sent on a cross-country tour across
8:29
Canada and it made it all the way and when they tried it in the US, it made it 10 miles down the highway in
8:34
Philadelphia before they found it beaten to scraps in the gutter.
8:40
Exactly. Needs to be punched. This one does dishes, folds clothes. Um,
8:45
you know, the whole bit. Now, I'm sure it's a little slow, but this is the video I was talking about of the example. They have this uh young girl uh
8:53
just throwing her crap all over the place and leaving a mess everywhere, and dad just watches it and waits for Memo,
8:58
the robot, to come pick it up and clean it up. So, um, I thought this was hilarious. It's I think this is the prequel to
9:04
Murderbot. That's awesome.
9:11
I never did finish that series. It's worth it. The books are really
9:16
good, too. Start with the books. The books are a better start because it makes them makes the show make more sense.
9:21
Got it. Got it. Got it. Uh, one more thing. You know, it's funny. You know me, Chuck. I'm like a GitHub addicted AI
9:28
guy and so I'm constantly looking at all the new open source projects and then I'm dumping all these ideas on you. Um,
9:34
but here's the thing that's the continuing pattern that I keep seeing. Um, and that is just to toot our own
9:41
horn and say I have yet to find any of these cobbled together Pythonbased
9:48
applications that require, you know, node level installs or Python level installs that require multiple
9:54
applications, 200 dependencies. I I've yet to find one that we couldn't
10:00
recreate in Kilroy. So I think one of the things I want to start doing is taking some of these research papers
10:06
that we're seeing and some of the projects I see in GitHub and just video me scary creating the pipelines that
10:15
recreate the functionality that they're doing with arduous coding and programming. So um this isn't a this is
10:21
a perplexity clone. um you know, it's supposed to find answers to your questions, search engine, blah blah
10:26
blah, but at the end of the day, it's really just a bunch of components and pieces put together um trying to force
10:33
them to work together, whereas with us, they're meant to work together. So,
10:38
that's it. That's all. All right. All right. You know, I I it's really it
10:44
really is sort of uh I don't know. We we we need to we need
10:50
to do exactly what you said because us saying that it works like that without showing it is oh yeah sure guys. But it
10:56
really is the case that with a double click on a desktop grade machine on your
11:02
desktop, you can do what otherwise would be two or three days worth of installing Docker images and and updating Python
11:10
libraries and generally fussing around with, you know, some Linux install
11:16
that's ultimately going to get destroyed because you do something wrong. And Kilroy is all in one bundle. It's all
11:22
one thing and it has all the pieces that you need. And if you don't like it, you drag it in a trash can.
11:28
And anything that you want to integrate has the potential to be integrated. If it can generate a web hook, if it can do
11:34
it, there is there is nothing out there I can think of that could not be a module integrated into the pipeline in
11:41
Kilroy for you to utilize by your agents. So, well, let's look at what we've done this
11:47
week. Okay. You want me to show it? Yeah. I I'm a little bit mad at myself for adding all these clown colors in
11:53
here, but it'll help me describe it by Oh, no, no. It makes it so much easier to look at. I'm glad you did.
11:58
Talking to the colors. Okay, so if you remember last week, what we were working on was integrating uh the pipeline, the
12:07
AI pipeline in Kilroy with the existing apps that we have for dealing with
12:12
exchange trading, right? So, we have an exchange application out there and you can hook it up to any of 120ome
12:19
different exchanges. Uh, and it can do buys and sells, market orders, limit orders. It can get the
12:25
value of your portfolio. It can see positions that you've got. It can look at your order history. That app is all
12:31
out there to be driven by you and your mouse clicking. Um, but what we've done
12:36
is add several of our tool blocks in the AI pipeline that also do those same
12:42
functions, which now means that your LLMs can do the clicking instead of you with your mouse. Uh, which we want to
12:50
proceed to do safely and carefully because you don't want it to spend all your money and find out that it what was
12:55
that robot? It just bought one of those for you. Memo. Yeah. Yeah. you've got a memo showing up in
13:00
your uh on your front doorstep instead of you know in your portfolio. So last
13:06
week what we had done is is we were showing how you could just type out sort of a command that would have uh the
13:13
Kilroy exchange code go out and get you your positions in a particular uh in a
13:19
particular exchange. And and so I'm going to go over this diagram in gory detail, but right now I'm I'm using an
13:27
exchange object that's hooked up to Kraken. Kraken is my my sandbox exchange because
13:33
it's only got a little bit of uh holdings in there. And if I mess up, well, no harm.
13:39
Well, we don't want to show the world your massive wealth, my massive stacks, right? Yeah.
13:44
I want to see those, too. Um, so what we had what we had last week was a user
13:49
input uh, you know, scrolling chat window where the user typed and it took
13:54
a command that went off and got used the positions tool over here in light blue uh, to go off and fetch the position
14:01
that you had and then format it as some uh, markdown and put it back into your
14:06
display. What I've done since last week was make this a little cleaner. Even though it looks more complicated, we're
14:13
separating out handling commands from the LLM's doing work and the users input
14:19
and output. So, we have a section over here on the left for dealing with commands. A section over here on the
14:24
right that's a work in progress for the work that the LLM are going to do to do buyell decisions and figure out what
14:30
your holdings are and the percentages you've got and whatnot. Then the orange stuff is just basic input and output to
14:37
kind of sanitize the inputs and outputs from the user's perspective. So that user box is a chat engine
14:43
connected to an LLM that can process. And this could be this is the this one
14:49
currently is the chat window inside of Kilroy, but it could be a Telegram channel. It could be whatever instant
14:55
messaging service you want. I mean, I guess you could talk to it with email if you wanted to do the integration, right?
15:01
So the user puts in a command. It's either a slash command like in this case we've got slash positions and we've got
15:07
slashlimit. These two blue boxes are checking for those commands and sending them off down the right branch of the
15:12
tree or it's not a command and we're just going to stick it straight into a swarm over here for all the LLMs to
15:18
listen to and do work on. And and so right now we've got two commands,
15:23
positions and limit. And I'll just pop back over here. This thing's already instantiated and I'm sure we're going to have to wait for Oama to wake up as
15:30
usual, but we'll do the positions command first. This is actually not
15:35
really using an LLM for very much. I'll show you what it does when it comes back with its answer. It's gone off to Kraken
15:40
and said, "Give me all of the positions in the portfolio." And then it's going
15:45
to pretty it up and show it to me as a table. Nice. So, it came back as a big JSON
15:51
object with all of the different holdings and how much was free of each currency, what was, you know, used or
15:56
part of a pending order and what the current price is. Well, there's a lot of stable coins in there. So, the prices
16:02
are one, but I have my Doge stacks in here. There you go. Got that Doge stacked up. You know, you brought up something just
16:07
a minute ago I really want to quickly touch on. Um, and that is is, you know, we don't need an LLM for everything. And
16:14
I just read a really long post over on Farcaster today. um of a guy saying that
16:20
he was he took the the the functionality of a centralized AI app and he basically
16:28
took it and removed all of the AI [ __ ] from it and then recreated this thing
16:34
using actual software and actual code and then had a really tiny piece where
16:39
the LLM played a role. And I think that's one of the strengths as well of Kilroy is that we're not um we are not
16:48
um expecting every step of the equation to be a gentic or to be an AI. We really
16:53
want it's not look at what we just did. I typed slashpositions here. It went into the swarm a piece of code. These boxes
17:01
are not LLMs. Looked at it and said does it match the pattern? Hey, it starts
17:07
with a slash. Right. Right. That's the That's the pattern that's in there for the regular expression. You can see it right there
17:12
is start of the line as a slash, right? Well, it did, right? It was slash positions. So, it flung that over the
17:19
fence into the commands swarm. And this little guy down here is looking for
17:25
messages from is a command. And if it matches the word slashpositions, it
17:31
says, "Yep, I want to get the positions using the positions tool." So, so far, no LLMs have touched this, right? Right?
17:38
Position tool goes off to Kraken and says, "Give me the positions." Right? And it comes back as a JSON object that
17:44
gets dumped into this swarm cleverly called input to formatter. And formatter is finally an LLM,
17:51
right? And a really robust awesome system prompt format JSON that you receive as a
17:56
markdown table and output to markdown. So I think the point is though that
18:01
there are plenty of programmatic things that we have learned over the last however many years we've been writing code in computer applications that are
18:08
built into Kilroy that don't require an AI LLM to actually process it. And you
18:15
use the LLMs when they need to be. If it's anal analyzing sentiment or if it's
18:20
formatting, you know, text that's coming in or if it's identifying things, whatever. But there doesn't always have
18:26
to be an AI in the loop. And I think that's the point that that poster was making this morning is is that it's such
18:32
a waste of energy and time and hassle to try to get AI to do everything when you
18:39
don't need it for a lot of things. Right now, and I'm going to show you something right now that's going to strip everybody's gears because it's using an
18:46
AI to do something that nobody uses them for, but they're really well suited for,
18:51
and that's being a dumb variable store. I freaking love this. A lot of people try to drag in tools
18:58
that go talk to key value stores or I'm looking stuff up in some vector database or I've got some, you know, GraphQL MCP
19:06
tool that my agents can talk to. But the reality of it is is that the context of an LLM is a perfect place to store
19:13
stuff. Yeah. And as long as you can manage the context and you can read and write the
19:18
context, it's a perfectly good data store. And that's what this green stuff up here in the work in progress is
19:24
actually showing. So, as part of our trading strategy, we want to be able to tell our trading AI pipeline the limits
19:32
that we're willing to work within, right? We want to spend this much money. We don't want to buy stuff over this price. We want to sell stuff below this
19:38
price. There's lots of limits that we want to set as a human before the AI goes off and and runs a muck with her
19:45
bank balance. So, this is an example of setting a limit variable. In this case,
19:50
I'm ultimately going to use it to set the the buy price for my fat Doge stacks
19:57
that as long as the Doge price is below this, I want to I want to do dollar cost averaging. You know, buy in once a day,
20:03
once an hour, however often as long as it's below this limit that I set, right? So, we've got a command called
20:10
slashlimit, which lets me, the human, say what the limit should be. And it
20:16
flows up here into this work in progress box where it goes into a swarm. A little AI little LLM up here's job is to just
20:24
pull out what I'm telling it that the limit is. And it's going to pass it over here to another box which is going to
20:30
set the prompt on this LLM down here called limit variable. And it's just going to set the prompt to say the limit
20:36
value is blah, whatever that is. Right? So now any other AI, any other tool,
20:43
anything else, any other human in the swarm can say what's the limit and this thing is going to respond with the
20:48
value. So let me show you, let me let me show it to you in action first and then I'll we'll come back and we'll look at
20:55
how it's working real quick. So over here, yeah, we should talk about when you get back, let's talk about the fact that what else that opens the door to. But go
21:01
ahead. Oh yeah. So all right. So first thing I want to do is just say I don't know what is the limit
21:10
and it's going to come back and say well right now the limit is one right
21:15
okay well I want to change that I want to make the limit be uh 12
21:22
now you can see over here the little parser figured out that the number I wanted to set was 12 and it generated a
21:29
a new system prompt for the other LLM that that's down here responding to what the limit is. So now if I ask what is
21:36
the limit, the limit's now 12, right? And that's a variable being held
21:43
in an LLM, right? Related to its context, right?
21:49
So So now the limit's four. So I doesn't seem like a big deal except that it it
21:56
really is because you're using exactly the same data formats, the exactly the same message formats, everything that
22:03
already flows around in these swarms and you're letting the LLM be a storage
22:08
mechanism for you. It could be storing JSON. It could be storing a complicated JSON object with lots of fields in it.
22:13
It could be an array. It could be a string. It can be whatever. It could be a markdown document. It could be a task project. Yep. Yep.
22:19
Whatever you're asking it to store is going to go into that limit ver LLM as
22:25
its context. And if we go back and look, you can even see that in this dumb little LLM down here, its context value
22:33
is set to the value of limit is four. That's the last thing I told it. Whenever you receive input, output the
22:38
value of limit. Only output the single message like limit is X. Do not output any additional information, you dumb
22:43
LLM. Um, so that's how we're always getting the limit is four out of this
22:49
thing. Regardless of, you know, I could say, you know, bake a cake and ideally it should still tell me the limit is
22:55
four or the limit is four. There you go. Right? It doesn't care what I whatever I tell it, it's just going to always tell
23:00
me what the limit is. So, if you look at what goes on back here, I'm typing slashlimit and some value. And that goes
23:07
up here into this swarm. And this little guy up here has one job, right? And his one job is you will receive a command of
23:14
the formlimit number, extract the number of the command and output only the number. Right? And that's why we see
23:20
back here in this display, it's just putting out the number four. Right? This little guy's job is to just take
23:26
whatever input it gets, find the number, and spit the number back out. That number goes over to a tool, right? It's
23:32
not going to another LLM. And this tool is really dumb. It says no matter what you get, it's going to be some value
23:38
that you want to insert into the system prompt of this guy. So, if we look at what this thing does, it says, well, set
23:45
the system prompt of whatever you're wired up to with this red wire to be the value of limit is whatever text you just
23:52
received. Well, in that case, it just got the number four. So, the value of limit is four whenever you receive the input blah blah blah. And it's setting
23:59
this guy's system prompt to be that. So now whenever this guy gets text from the
24:05
user or from any other LLM or any other input source, its whole job is going to be to put out whatever the current value
24:11
of limit is. So now go ahead. No, go ahead. Uh so this little dumb
24:18
function set prompt is probably one of the most powerful and empowering things that
24:26
we've discovered in in building this system because that is now a a a module
24:33
of functionality that can be utilized by an agent to create more of those limit_var LLMs those kinds of things
24:41
right we can utilize agents coordinating agents manage managing agents to
24:47
actually generate more agents with prompts that are related to whatever the current task is at hand. Right? So this
24:55
when we when you when we saw this and we and you had this you deployed this and
25:00
it clicked for me that we now could actually have agents creating agents and
25:06
pipelines on the fly. That was just everything right. This context storage with our agents is absolutely
25:15
probably the most powerful thing in this system because then it allows us to basically build anything or have an
25:20
agent that can build anything. Well, and it's selfmodifying which you know we're gonna build Skynet here.
25:27
Yeah, exactly. We're just doing Doge trading today, but in a couple of weeks, we'll be working on Skynet with this
25:32
because it's it's like you say, Brad, it can if it understands that it needs to
25:38
generate a subtask, it can conceivably build the system prompt and the pipeline tooling to do that subtask. Right. So,
25:47
and and it can even try it and see if it gets an answer that it likes. And if it doesn't, it can kill that pipeline off and try again.
25:53
Yep. Right. and and we'll get to the point where we have a chat window where we can tell an agent to deploy a pipeline that
26:01
does A, B, C, and D to get to goal X. And it can deploy these pieces. It can
26:08
deploy interconnected swarms. It can connect to other swarms. I It's
26:13
every time we talk about it excites. I I know it's it's going to be fun. So, so the next step to do here is instead
26:21
of just talking to this limit thing directly just for demo purposes like we're doing when we ask what is the
26:26
limit, we'll have a whole another chunk of pipeline out here now that periodically is going to wake up and say
26:33
what's the price of Doge? What is the limit? What's my budget? If all those
26:39
things line up, the price is less than the limit and I have some spare cash, go buy some Dogecoin.
26:45
Right? That's the next step of this. So, we're we have the ability to set the variables and the bounds for the agent.
26:51
You know, one of the things we're going to set, how often do you check the price? Once a minute, once an hour. That's something we haven't even touched on in uh the Kilroy universe. Right now,
26:59
there's a block over here that is actually called time and it can do
27:05
things on a periodic basis like tr send text into the pipeline to do something, see if it's time to trade. It could just
27:11
send that prompt in repeatedly once every 10 seconds. Is it time to trade? Is it time to trade? Is it time to trade? So these things can become
27:18
automated and run on a periodic basis without a human in the loop. I mean that's the that's the
27:24
ultimate peril and and crippling limitation of almost every interaction that people are doing with LM LLMs right
27:31
now is that you got to start with somebody typing something, right? Or you got to figure out how to set up crown jobs, which is nut,
27:37
right? And all that stuff. So you know that's just another block you drag on the screen and every 10 seconds send out this message. you know, get the price of
27:44
Doge, and that's going to get dumped in a swarm. And whenever the LLM sees the price of Doge, it's going to say, "What's the limit?" And then it's going
27:50
to compare the price of Doge to the limit, and then issue a buy command, and something else is going to be listening for the buy command, and when it gets
27:56
it, it's going to go off and buy it. Yep. Well, and again, we've done this already. We can build in, you could
28:03
build in Trading View signals into this system. Oh, absolutely. one of these swarms. We have a swarm right now that receives
28:09
real-time updates from Trading, you know, Trading View, well, material indicators, versions of Trading View
28:16
signals. And and it's actionable data. It's got, you know, deviations from, you
28:23
know, the the running price. It's dip, go buy, it's, you know, at at a peak, go
28:28
sell. Um, and it just shows up in a swarm as another message that the LLM's going to work on, right? Well, and the be the
28:35
beauty of that is you can have multiple signals coming in and have the LLM compare them and say, "Do all of these
28:40
signals match up? The variable I have is A is green, B is green, C is yellow or
28:47
green, whatever it is, right?" And and and that LLM then says, "Yes, go. Okay, great. We're good to go." Right? And you
28:53
could have variables within the system that says, "Yes, you can go, but only buy X if X is whatever." I mean,
29:00
well, and I mean, I want to get that just because I watched Bitcoin dump two nights in a row where I'm dead asleep
29:06
and there's nothing that you can do about it, but if you had your little LLM buddy out there watching for you.
29:12
Yep. Right. Not only would it have been get out before it dumps, but it would have been get back in when it's at the bottom
29:18
and you could have slept peacefully all night long. Well, and then there are, you know, look, I mean, there have been at least 20 20 of signals from the
29:26
material indicators, MTF indicator over the last 48 hours for Bitcoin and Ethereum. Um, that were all turned
29:33
profit, right? So, there's anyway, there's a there's a million options here. Whatever you like to do from a
29:40
trading perspective, you could be analyzing Twitter sentiment, right? You could take a follower uh you could take
29:46
a an influencer that you trust who you've made money on in the past on Farcaster or Twitter or wherever and
29:52
every time they do initiate a buy, you're executing a buy. And that's a really simple thing to do, right? We're
29:58
just looking for data from that person's feed and the token and then we go buy it. So, um yeah, there's a lot of power.
30:06
Well, you know, it's interesting. We were we were playing around with the copy trading in Blow Fin last week. Not
30:12
to great effect at the end of this week, but you Well, it was good until we didn't take profit, right? Well, but but you could do some
30:19
stuff that they don't let you do on their platform, which is you could follow the trades of five traders and
30:25
pick the best three out of five. Yep. And follow what they're doing. right now you you're you're all or nothing with a
30:31
traitor on blowof fin. But this way you could say, well, let me see what everybody's doing and we're going to
30:36
throw out the Russian judge and the you know, whatever and pick the one. Well, and that was one of those research
30:41
studies was a process of voting, right? The the one that solved the million task problem not only was breaking everything
30:48
down into small pieces, but was having every single step voted on by three different LLM. I can't remember if we
30:53
showed that, but we have a demo of that where you're asking the LLM to compare two numbers. And they're always bad
30:59
about comparing numbers. Is 0.11 bigger or smaller than 0.1. Right.
31:05
Right. And so you ask that three different ways to three different LLMs. You get their answers back. You pick the
31:11
two that match and that's the answer. That's the easiest, quickest way to do that. And we've got lots of examples of
31:17
that. You know, is this price of Doge outside my limit? Well, I'm going to ask three times because I don't want to be
31:23
buying because the LLM hallucinated a price and made the wrong decision. Right. We'll put that
31:28
we'll put that in there in this at some point. You'll see the sanity check module come in. Well, and the the nice thing about that
31:34
model of the copy trading is is once we've onboarded a lot of folks with a lot of experience and skills in trading,
31:40
they're going to have their own swarms in Kilroy and you'll be able to copy them and follow them and ch pick and
31:46
choose what you want to take part in and not take part in. And then your agent can be comparing them to like you said
31:51
to everybody else. But it can all be happening in Kilroyce forms peer-to-peer. No latency, no issues, no
31:57
worry about, you know, whether Binance's uh API is down because too many people
32:03
are hitting it at once, whether, you know, Coinbase is down because Cloudflare is down. This is pure
32:10
decentralized peer-to-peer agentic platform that allows you to
32:15
collaborate and share information and data and agents with each other as well as GPUs and LLMs. So, it's um
32:23
I'm just it's blows me away. I you think by now I wouldn't be that excited about
32:28
it, but it's just ridiculous. You know what's really exciting is I mean the more people that use it the
32:34
more exciting it gets too because they do things that we haven't figured out yet. Yep.
32:39
So this is time to pitch people on using Kilroy Brad. Yes. So we are looking for folks that
32:46
want to be building and trying and testing and helping us improve everything about this system. If you are
32:53
someone who's uh willing to give it a shot and think logically and experiment
32:58
with some power of this, please um you can DM me on Twitter, B05 Crypto, Chuck
33:04
is Cshotton on everything. And I'm B05 on Farcaster, Brad Nickel on LinkedIn,
33:11
Chuck Shottton on LinkedIn. Um please do Oh, same with the Telegram. B5 crypto
33:16
and Cshotton on Telegram. Uh and oh well, we have our group. I'll put our group link in the show notes for um Kil
33:24
the Kilroy Group Telegram. Yeah, you got a new Kilroy group. Absolutely. Yeah. Um but
33:29
it's it's closed right now. If you want in, send a DM to Brad or me.
33:35
Yeah. Yeah. And we are, you know, open arms. We would love to have you. We need more brains on this. You will unleash
33:42
incredible power for yourself by utilizing the platform. And you'll also be helping the only truly decentralized
33:49
uh agentic platform uh touring complete development environment that exists. So
33:56
we would love to is that a tagline? Is that a whole word bingo for the week? That's it. I didn't say revolutionary
34:02
yet though. So you know that was awesome, Chuck. I love that.
34:08
Thank you very much. I hope everyone has a great day. Take care. We'll see you. See you.
34:13
Bye. Bye. Now, let's see if I can go off the air. It says we're not live,
34:18
but we were. I checked. No, I checked. Mine says live and recording. Mine does not.
34:25
It's like, what the Okay, whatever. Uh, now, well, the problem is I can't uh
34:31
take us down. I'm going to refresh the screen. We'll see what happens.
ROTP: Building an automated trading app from scratch in Kilroy Part 2

Realm of the Possible
4 subscribers

Subscribe

0


Share

Save

Clip

1 view  Streamed live on Nov 21, 2025
This week we're going to continue walking you through building a trading app in Kilroy. 

Learn how you can be empowered to build anything with a powerful decentralized automation and AI platform. 

We've created the most powerful decentralized AI and crypto automation/integration platform while being completely decentralized. It comes complete with no configuration peer-to-peer/swarming capabilities, the ability to easily build integrations with any other compute, and a Turing complete visual creation tool anyone can use to build functionality in the network while giving agents on Kilroy the ability to build and deploy on their own.
Transcript
Follow along using the transcript.


Show transcript

Realm of the Possible
4 subscribers
Videos
About

All

Computers

For you

Recently uploaded

20:30
Instagram Pilot's Emergency Caught on Camera!
Pilot Debrief
1.2M views
•
8 days ago


14:19
What Sam Altman Doesn't Want You To Know
More Perfect Union
2.3M views
•
10 days ago
Fundraiser


16:47
Office moments I think about way too often
The Office
1.7M views
•
4 months ago

13:24
LAWYER: How Cops Use Apple's NEW Tech to Spy on Your Phone
Hampton Law
381K views
•
8 days ago

25:57
Stop Rambling: The 3-2-1 Speaking Trick That Makes You Sound Like A CEO
BigDeal by Codie Sanchez 
191K views
•
7 days ago

9:23
"AI Can’t Replace Juniors" - AWS CEO
The PrimeTime
262K views
•
2 days ago
New

21:43
Why tech billionaires are quietly bankrolling Europe’s far-right | Pinch Point
Al Jazeera English
313K views
•
13 days ago

24:19
NEW Walking Method 2x Better Than Running?! (Tested)
Jeremy Ethier
621K views
•
8 days ago

17:35
The Future of Veritasium
Veritasium
3.4M views
•
5 days ago
New

14:36
Why Spaghetti Code Beat Clean Architecture
Eric Roby
107K views
•
3 weeks ago

12:35
How To Learn So Fast It’s Almost Unfair
theMITmonk
870K views
•
10 days ago

11:05
Will Ferrell Refuses To Discuss The Bird On His Shoulder | CONAN on TBS
Team Coco
1.4M views
•
3 months ago

17:00
Downfall of the 7-Hour Coding Tutorial
Boot dev
160K views
•
1 month ago

20:36
An atheist explains the most convincing argument for God | Alex O'Connor
Big Think Clips and 2 more
373K views
•
11 days ago

13:34
Virus disabled their Option to Shutdown
Scammer Payback
912K views
•
3 days ago
New

17:54
I Quit Every Streaming Service… Here’s What I Use Now.
Switch and Click
1.6M views
•
1 month ago

32:32
The Strange Math That Predicts (Almost) Anything
Veritasium
10M views
•
5 months ago

13:39
I Built 3 SaaS Apps to $200K MRR: Here's My Exact Playbook
Starter Story
464K views
•
1 month ago

53:44
Building the PERFECT Linux PC with Linus Torvalds
Linus Tech Tips
4.5M views
•
4 weeks ago

33:14
Ethernet is DEAD?? Mac Studio is 100x FASTER!!
NetworkChuck
310K views
•
9 days ago


Show more
    


NL

Skip navigation
Search



Create


Avatar image
Transcript


Search in video
0:00
episode of Realm of the Possible by
0:02
Kilroy. How are you today?
0:04
Doing well yourself, man?
0:06
Yeah, it's fun to pretend like we
0:07
haven't actually spoken for an hour,
0:10
10 minutes.
0:15
Um today we're going to talk a little
0:18
bit about Apple Foundation models and
0:21
we're going to show folks um uh Kilroy
0:25
agents inte I have got to fix this
0:28
camera stability. Uh Kilroy agents
0:32
integrated pipelines integrated with
0:35
Telegram
0:37
and Mac shortcuts. Is that correct sir?
0:39
Yep. gonna make it put Mac OS to work
0:43
today
0:44
and we're going to show them how to
0:45
build, how to integrate and um some
0:48
functionality, but basically how the
0:50
pipeline builder works. And if you are
0:52
interested in playing with Kilroy and
0:56
you don't mind getting your hands a
0:57
little bit dirty, getting things set up,
0:59
um we would love to have you join us.
1:02
technical people, developers,
1:03
non-technical people that uh have a
1:06
little experience with software, we
1:08
would love for you to uh join us as we
1:12
build out functions for the system. If
1:14
you're a developer,
1:15
yeah, I think we're going to start doing
1:17
some some build along, you know, sort of
1:19
some Bob Ross painting style stuff.
1:21
Build your Kilroy pipelines along with
1:23
Brad and Chuck.
1:25
Exactly. Exactly. So, um if you're
1:28
interested in that, please let us know.
1:30
If you're a developer, we're looking for
1:32
developers to uh participate in
1:35
integrating your favorite DeFi protocols
1:38
as well as other modules for the system.
1:41
Um, your skill set can encompass all
1:44
kinds of things. So, DM me on Farcaster
1:47
on Twitter. Um, Telegram B5 crypto,
1:51
Farcaster B5, Twitter B5 crypto. Chuck
1:56
is chatton cs h o t o n on everything.
2:01
So um we would uh love to connect with
2:04
you. Um all right, Chuck. So uh this
2:07
week I was attempting I installed this
2:11
handy dandy here. I'll go ahead and
2:13
share this. I can share this my shaking
2:16
camera.
2:18
Uh
2:19
so while you're fumbling around, this is
2:21
this is going to be the uh Apple
2:23
Foundation model, right? Correct. So we
2:26
um Oh, this is interesting. Our
2:29
streaming thing does not show that
2:32
window, that little map. Oh, there it
2:34
is. Okay. Uh so we um wanted to test out
2:39
AM uh the uh Apple Foundation model. Can
2:44
you see the screen, Chuck?
2:45
Yep, it's up.
2:47
So this is an LLM uh 3 billion parameter
2:50
LLM that's built into Mac OS. um and
2:55
fairly capable from the demos that Apple
2:58
has been doing in their developer
3:00
channel. Um very fast and you know
3:05
freely available for anybody that's on a
3:08
Mac or an iPad or iOS um to be able to
3:11
utilize the model. And so I was really
3:15
anxious to give it a shot. you and I
3:17
built out that little plans app that we
3:19
showed last week that allows us to
3:21
record what plans we're working on and
3:22
keep track of them, etc. in Kilroy and
3:25
in an AI chat window. And so I was
3:27
excited to um try out the foundation
3:30
model. So I installed this little app
3:32
that lets you configure um and make it
3:35
work with the OpenAI compatible
3:37
endpoints. Um, so just like if you were
3:39
using a llama uh to to infer LLMs um on
3:44
your desktop, this allows you to connect
3:47
to the Apple Foundation model and
3:49
interact with it like you would any
3:51
other catchy PT like model. So I was
3:55
excited it worked. I got it connected
3:56
up. Everything worked. And then we ran
3:59
into a little bit of an issue.
4:02
So, it's uh it's uh Apple's extreme
4:06
cover your ass um safety guard rails on
4:10
their foundation model. Um they um they
4:15
have made it next to impossible to do
4:18
anything except system related stuff or
4:22
uh writing code. It did write code very
4:24
rapidly. I did a couple of little um uh
4:28
Vibe apps in our Vibe uh module and our
4:31
pipeline and it worked really well for
4:34
that. But if you want to just interact
4:36
with it, send it your plans for the day
4:40
that you're going to uh serve up so your
4:42
business partner can look at them. Um
4:46
sometimes it'll work, but most of the
4:48
time Apple says, "I am an ethical LLM. I
4:52
am not permitted to answer questions
4:54
like this. Now this
4:55
and it was clearly right. It was it was
4:58
like choking on the slash commands,
5:00
right? like the slash sounds. Well, it
5:02
was choking on the slash commands and
5:04
then I tested it without the slash
5:06
commands in our little AI chat pipeline
5:09
LLM
5:10
and it rejected just innocuous tasks
5:15
like set up Trello board for bisdev
5:19
like nope can't do it unethical I'm not
5:22
it didn't understand it didn't it didn't
5:25
have enough context to realize that
5:27
you're just making a list you're not
5:29
actually asking it to try to do stuff.
5:32
Yeah. And well, and and just this idea
5:36
that it's blocking it as a safety issue
5:38
is just ridiculous. It could just say,
5:40
"I don't know what you mean." Right?
5:42
Like any other LLM would if you just
5:44
Let's put this in context, right?
5:45
Because we've been we've been building
5:47
to the Llama 3 thing for forever. And I
5:51
have to say I was a little gratified by
5:53
that news that news article or those
5:55
benchmarks that you found uh yeah last
5:58
week
5:59
because they basically vindicated the
6:01
fact that hey that's the the least
6:03
hallucinating most sensible small model
6:06
that's still out there and it's what a
6:08
year and a half old now.
6:09
Yeah man it's like 400 years old in AI
6:12
time. So that's uh you are definitely
6:15
the retro LLM dude.
6:17
So the goal was to try to find some some
6:20
stuff. The goal was to try to find some
6:21
stuff on people's desktops that was
6:23
already there. So, they didn't have to
6:24
install stuff. Didn't have to run O
6:25
Lama. Didn't have to get
6:29
and that's the power of what Apple has,
6:31
right? All of these devices have GPUs.
6:34
All the newer ones since the M1, the
6:37
iPads have it, the iOS, iPhones have it,
6:40
the they all have the ability to process
6:43
on device with these LLMs. This is a
6:46
fast integrated LLM built-in and it fits
6:51
our philosophy perfectly with Kilroy,
6:53
which is you don't need monolithic
6:55
prompts, gigantic prompts to tell an
6:58
agent how to function. You can break and
7:01
have the agent break tasks into small
7:04
context, small prompts so that it will
7:07
be more efficient with models like Llama
7:09
3 and or the Apple Foundation model.
7:12
Well, so here's a here's an interesting
7:15
thing to me about this, and I think this
7:17
is where there's going to be a huge
7:18
divergence in the industry real soon.
7:21
You know, you've got all these companies
7:22
like OpenAI and Meta and Google who are
7:25
building these giant
7:27
enduser human focused chat bots, right?
7:30
I mean, that's really at the end of the
7:32
day, that's what they're doing. You want
7:33
to be my therapist? You want to be my
7:36
homework mentor? You want to be my, you
7:39
know, sidecar code buddy? whatever, you
7:42
know, they're they're making it be, you
7:45
know, a one-sizefits-all Swiss Army
7:47
knife. And what Apple's done is said,
7:49
you know what, we really need the
7:51
natural language processing and the
7:53
ability to understand
7:55
uh context for system related stuff. And
7:58
that pushes the LLM down into the realm
8:01
of being operating system software, not
8:04
application software. Right? and all
8:06
these other companies that are doing
8:08
these giant monolithic cloud LLMs are
8:10
all thinking, you know, this is going to
8:12
be the beall endall
8:15
one one app to rule them all kind of
8:17
environment where Apple's kind of taken
8:20
the other tag, which is let's break it
8:21
down into little pieces that it can do
8:23
capably with the OS functions that we've
8:26
already got. And I got to tell you, man,
8:27
from painful personal experience, what
8:30
Apple does at this point in the in the
8:32
market is probably not wrong. They did
8:35
it to me back in 1997 where we had we
8:39
had a really successful commercial web
8:41
server product. It was really popular
8:43
webar and its predecessor Mac HTTP and
8:47
you know we had like 95% market share
8:49
and then one day Apple shipped a version
8:51
of the OS with web serving baked in
8:54
killed your business in about 10 seconds
8:56
right and and they are going to do that
8:59
at the at least some level of the LLM
9:01
world. There's no reason for these
9:03
things to be monolithic, standalone,
9:05
external from your system applications.
9:07
They should be operating system
9:09
software. That's where they're going to
9:10
end up.
9:11
Yep.
9:12
Yep. Well, it's going to be interesting
9:14
because um frankly, I think that um once
9:18
we've done a lot more of this uh Mac OS
9:20
shortcuts work and integration work that
9:23
um it's going to be really interesting
9:25
to see how Apple reacts to what Kilroy
9:28
is capable of doing with Mac OS that
9:29
they are not doing. It's funny because,
9:32
you know, you'd like to be able to talk
9:33
to Siri and have Siri act like a front
9:35
end to an LLM and then go do some system
9:39
level stuff. Well, it's not doing that
9:41
right now. It's not even doing that in
9:43
in their in their betas. But if you skip
9:46
past the fact that it's really hard to
9:48
talk to your Mac and have it go to Siri
9:51
and just end up with the text that you
9:53
want, it's real easy to tell Siri to run
9:55
shortcuts. And it's real easy for
9:57
shortcuts to talk to Kilroy. And it's
9:59
real easy for Kilroy to use its LLMs to
10:01
go do all the stuff that Siri can't do,
10:04
including talking back into the OS. So
10:07
maybe rather than talk about it, let's
10:09
show a little bit of it and see, you
10:11
know, show people how it works in one
10:13
direction and we'll do the other
10:15
direction a little bit later.
10:17
Sweet segue there, B.
10:19
Right. All right. So, I'm gonna share my
10:21
I'm gonna share my whole screen, so I
10:23
apologize if you got to squint to see
10:24
the text. You're gonna have to
10:27
zoom it up on your screen.
10:28
Oh, one more thing while you're doing
10:29
that. Apple uh announced this week that
10:32
they are doing they already have a deal
10:33
with Open AI. not open AI um
10:38
Google
10:38
to and utilizing chat GPT with users,
10:42
but they also um are um now signing a
10:46
deal with Google to do Gemini uh with
10:49
Siri uh to the tune of a billion
10:51
dollars, which um to me, I mean, I know
10:55
it's a stop gap measure for them because
10:57
eventually they'll dump both of those,
10:59
but I feel like they already have all
11:01
the pieces and I guess we'll just have
11:02
to show them they do and we can do it
11:03
for them. So,
11:05
well, hopefully they like it. Um,
11:07
yep.
11:08
So, all right. So, here's what we've
11:09
got. We've got Kilroy running on the
11:11
desktop down here in the corner as a
11:12
little background application. And we've
11:14
got uh our old buddy O Lama running in
11:17
the terminal window over here on the
11:18
left running Llama 3. And then this is
11:21
the Apple Shortcut Shortcuts app back
11:24
here that we're going to look at in a
11:25
minute. So what we wanted to do is do
11:28
some nice tight integration between a
11:30
native Mac OS application and uh
11:35
and and Kilroy. And so what we decided
11:38
that we would do is kind of a standard
11:40
little pipeline that that other people
11:42
have done, but it's requires a lot more
11:44
moving parts and I don't think most
11:46
people can set it up. And that is uh a
11:49
URL clipping service, web page summary
11:52
service, all built in to your desktop.
11:55
So, the premise is that you would go to
11:57
a web page. I'm going to use RS Technica
11:59
as the guinea pig today. And we'll pick
12:00
one of these articles and say, "Hey, I
12:02
want to share that with Brad in our in
12:03
our shared Telegram group." So, what I'd
12:05
like to be able to do is just be on the
12:07
page, click a button, have that page go
12:10
off through the shortcuts infrastructure
12:12
to Kilroy where Kilroy can ask Llama 3
12:15
to summarize the text, pull out the URL,
12:17
and push that over into Telegram for us.
12:20
So, that's going to be what we build.
12:22
And so let's just start with uh a a new
12:26
pipeline. And I'm gonna cheat a little
12:28
bit because some of this stuff I don't
12:29
want to throw up on public screen things
12:32
like my Telegram bot ID and stuff. So
12:34
I'm going to
12:35
start with the idea that Kilroy uh
12:38
already has built into it a way for
12:41
outside world applications to talk to it
12:43
into its swarms. And if you look in the
12:46
AI application that we have in this uh
12:49
wonderful little piece of user
12:50
interface, you can say, "Hey, Kilroy,
12:52
listen for messages from other people on
12:56
a particular web hook, this listener web
12:58
hook." And I'll show you the URL for
13:00
that in a second, and then take
13:03
everything that you get from that web
13:04
hook in the payload of the web request,
13:06
and jam it over into this swarm called
13:09
listener swarm. any Kilroy pipeline
13:11
that's listening to that swarm is going
13:13
to get that input from the outside
13:15
world. So, so let's start with uh making
13:19
that swarm inside of the pipeline
13:22
editor. We'll just drag this over and it
13:24
was called
13:26
listener swarm.
13:29
And it needs to be a public swarm,
13:31
right? So that
13:33
everybody that needs to can talk to it.
13:35
Now, normally this would be a more
13:37
private name than listener swarm, but
13:38
but there it is
13:40
for now. That's great. Well, and so the
13:42
idea is with making it a public swarm is
13:44
so that other pipelines and swarms can
13:46
connect to it and make utilize resources
13:48
back and forth, right, by name.
13:51
Okay. So, so the goal is going to be to
13:53
have the shortcut come to Kilroy with a
13:55
web request and then push that data into
13:59
the swarm and something's going to have
14:00
to pull it out. So, let's uh let's get
14:03
an an AI agent blob from over here and
14:07
put it in. And uh this one's going to
14:11
have a particular system prompt because
14:12
we know what we're going to get from
14:14
shortcut is going to be a blob of JSON.
14:16
And uh we're going to want to uh do some
14:22
stuff with the pieces of that JSON. So,
14:23
let's look at how it's getting here.
14:25
Right. So, in in the Apple OS is
14:29
something that a lot of people probably
14:30
don't even know about or even use. Uh
14:32
they may use it more on their phones
14:34
than they do on their desktop, but it's
14:36
the it's the Apple Shortcuts
14:38
application. And you can uh you can see
14:42
all of the all the shortcuts that you
14:44
have if you pop open a window and look
14:47
through all the ones that you've built.
14:49
And I've got a bunch that I've built to
14:50
play with. And there's this one in
14:51
particular that we have that's uh called
14:54
browser clip. browser clip has these
14:57
five steps in it, right? It says uh get
15:00
the current when this thing is invoked,
15:02
right? And we'll talk about how that
15:03
happens in a minute. Get the current web
15:06
page that's showing in Safari, right?
15:08
We're going to use that RS Technica page
15:10
back there in a second. And after you
15:12
get that page, get the contents off the
15:14
page. Now, this is a really important
15:15
feature of shortcuts in Safari. Most of
15:18
the time when you go to a URL on a
15:20
website, what you get is a blob of
15:22
JavaScript that then proceeds to go back
15:24
to the website and pull down article
15:26
pieces and stylesheets and whatever else
15:28
is needed to draw the display on the
15:31
screen and you don't get meaningful
15:32
content that's loaded after the fact.
15:35
But what this lets you do is after
15:37
Safari's already pulled down the page
15:39
and rendered it and is showing it to you
15:40
as a user, then it can run through its
15:42
own internal storage and pull out the
15:44
actual content of the page. So that's
15:46
what we're doing here. And we also want
15:48
to get the URL of the current web page.
15:51
Real quick, I want to make a point,
15:53
Chuck. Sorry. Yeah. Um, so this is
15:56
something that normally people would
15:58
either have to use a tool call or in in
16:02
via the LLM that supports it or some
16:04
kind of MCP server to um be able to
16:08
access a browser that's running
16:10
somewhere else or on your machine to do
16:12
this process. What this does, what this
16:15
kind of integration with things like Mac
16:17
OS shortcuts does is it allows us to
16:21
utilize the functions of the user's
16:23
desktop in Kilroy um and be able to pull
16:27
out text because Safari probably pulls
16:29
and uses Reader to pull out the text
16:32
without having to install additional
16:35
servers for for for interacting and
16:38
utilizing LLMs with with browser with
16:42
Windows with text with information. So,
16:44
right. So, what we're going to do with
16:46
the contents that we just got and the
16:48
URL that we just got is we're going to
16:49
turn that into a JSON object. Now, in
16:53
Apple's
16:53
Can you zoom a little on that, Chuck?
16:54
Sorry.
16:55
I I don't think I can because this is
16:58
Apple's this is Apple's UI. Um
17:00
Yeah. Yeah. Yeah.
17:01
The um
17:04
this is a dictionary, right? A key value
17:06
store. But Apple has the ability to
17:08
present that as JSON, right? So you make
17:11
a dictionary and we have a URL value
17:14
which is going to be the page URL that
17:15
we got from the previous step and a data
17:18
value which is going to be the page
17:19
content that we got from the previous
17:21
step and we're going to use the other
17:25
shortcut step which is get contents of a
17:27
URL to call Kilroy with that JSON object
17:30
that we just created. So it's a little
17:33
bit it's a little bit esoteric but once
17:36
you have this done you never have to do
17:37
it again right. So the we're saying talk
17:39
to Kilroy local host on port 3000. Call
17:42
the endpoint for calling web hooks
17:45
inside of Kilroy and call the web hook
17:47
called kileroy.ai.web hooks listener.
17:50
That's the thing that we configured over
17:52
here where we said turn on the listening
17:55
web hook. That's the actual path to that
17:58
thing. And so that's going to pass that
18:01
JSON into Kilroy. Kilroy is going to get
18:02
that JSON object. they're gonna it's
18:04
going to turn it around, format it
18:05
correctly as a message for an AI
18:08
pipeline and put it into the swarm for
18:10
the other agents in the in the swarm to
18:13
operate on. And that's the end of what
18:14
the shortcut does. So in Mac OS, there's
18:18
a lot of ways to invoke shortcuts. You
18:20
can have keyboard commands, you can have
18:22
menu choices, you can have services in
18:25
the service menu. Um, and one of the
18:28
ways that you can invoke it, and this is
18:29
how Kilroy talks to shortcuts, is with a
18:32
URL that's a deep linking URL that
18:34
starts with uh a shortcut colon slash
18:39
URL. And if we looked at if we looked at
18:42
this, I don't you'll never see it, but
18:43
this is the this is the shortcuts:
18:47
runshortcut. Uh, the name is browser
18:50
clip. That's the name of our shortcut.
18:51
And then it's just going to fire that
18:53
thing off and it's going to do its job
18:54
which is to talk back to Safari and then
18:56
talk to Kilroy. So let's finish making
18:59
our pipeline. And for uh for expedient
19:03
sake I think I'm just going to take the
19:05
one that I already have uh up and
19:07
running over here rather than build it
19:09
again from scratch piece by piece. We're
19:11
going to end up with one that looks like
19:12
this. Right. So we've got the listener
19:15
swarm feeding its messages from uh the
19:18
Mac OS into the agent. The agent's going
19:20
to do some work on that and put it into
19:22
a swarm that's going to go out to
19:24
Telegram. Right? This is a Telegram
19:26
agent that's configured to talk to a
19:28
specific channel in Telegram and as a
19:31
specific bot user. And it's going to
19:32
take what this agent makes and show it
19:34
in the shared chat between Brad and
19:36
myself. So just for completeness, let's
19:39
look at how this agent is uh set up. Um
19:44
the system prompt is all of your output
19:46
should be markdown format as supported
19:47
by Telegram. Only output the raw
19:49
markdown. do not add explanations or
19:51
introductions. Right? That's the system
19:53
prompt. There's no context being kept.
19:55
So, every single transaction with his
19:57
agent is a new one and it's going to be
19:59
a JSON object. So, we've got a a prompt
20:03
prefix for every single message that
20:04
comes in that says extract the URL field
20:07
from the following JSON and output it as
20:09
a link in a small markdown heading.
20:11
Extract the data field from the JSON,
20:13
summarize it, and output a single
20:15
markdown paragraph. Do not add any
20:16
introductions or explanations. just
20:18
output raw markdown text. Now, to the
20:20
extent that Llama 3 can follow
20:22
instructions, it's basically going to
20:24
take that JSON object, pull out the URL,
20:26
make a title out of it, pull out the
20:28
body of the article, summarize it, and
20:30
put it out as a paragraph, and then send
20:32
that all to Telegram. Right? So, if I uh
20:35
export this guy and we go over here and
20:39
build that pipeline,
20:42
um we should get a couple of boxes on
20:45
the screen, right? And these are just
20:47
for us to watch for debugging purposes
20:50
because the normal way that you would
20:51
use this pipeline is I'm looking at a
20:54
web page. Oh, let's go look at uh Ford
20:57
says no exact date for F-150 Lightning.
21:01
Let me get Telegram over here so we can
21:03
see Telegram work, too. This is going to
21:06
be a lot challenge to tile the screen up
21:08
here.
21:10
All right. So, shortcuts is going to pop
21:13
up over over top of all of this when I
21:14
hit go real quick, but you'll see the
21:16
message show up in here in Telegram in a
21:18
second. I'll scroll it over so we can
21:19
see. All right. Here we go. So, I'm
21:20
reading this article. Hey, Brad would
21:22
really like to know about this electric
21:23
truck thing. He's in the market for
21:24
electric truck. Let me just hit Kilroy
21:27
web clip and allow it. And off it goes.
21:29
It's running the shortcut right now. The
21:31
LLM, if we switched over to Kilroy was
21:34
doing its work there. And in just a
21:36
second, it's going to pop over the
21:37
message here into Telegram. So, as an
21:39
end user, I would normally just click on
21:41
the link in the browser, click on the
21:43
button, and in a few seconds, here's my
21:46
summary posted to Telegram that has my
21:48
headline as a clickable link. And the
21:52
summary that the LLM made out of the
21:54
whole big article and then Telegram does
21:57
this little instant summary where I can
21:59
look at it in line with Telegram instead
22:00
of having to go off to a browser.
22:03
That makes sense.
22:04
I totally I love that, man. I absolutely
22:07
love that. And the beauty of this thing
22:08
is is that this can be birectional. You
22:11
can be sharing stuff into Telegram or
22:14
interacting with the LLM into Telegram
22:16
and controlling what Kilroy is doing.
22:20
You could theoretically operate your
22:22
entire life from Telegram and have it in
22:25
the background processing and doing
22:27
things on your Mac um or your other
22:30
computer if with with another model for
22:32
this um and never leave Telegram. So you
22:36
could even be like a comm a command line
22:38
junkie but in Telegram where you're
22:40
controlling and interacting
22:42
like I mean if you think about the
22:44
things that LLM are bad at, right? Like
22:46
what's one of the things that comes to
22:48
mind right away when you think about
22:49
what LLM suck at?
22:51
I don't know. Pick one. Math.
22:53
Math. So theoretically, you could be
22:57
utilizing this with the calls to the
23:00
calculator capabilities on the Mac or
23:03
you could be using numbers um on on Mac
23:07
OS. I have interact
23:10
I've made an agent that sits in the
23:11
background and gets the Bitcoin price
23:13
every five minutes and sticks it into a
23:14
numbers spreadsheet and graphs it out.
23:17
Right.
23:17
Right. And then you could have the LLM,
23:20
right?
23:20
Yeah. And then you could have the LLM
23:22
that's been trained on financial data
23:24
analyze the market conditions at any
23:26
time based on the prices that have come
23:27
in over time.
23:29
Or somebody's putting buyell signals
23:31
into your swarm that you share and then
23:33
you can use that to go compare against
23:35
your, you know, mean reversion algorithm
23:39
that's off in your spreadsheet and
23:40
decide whether or not it's really time
23:42
to buy or not. Um, so what we just
23:44
showed was going from the Mac OS world
23:47
into Kilroy and then back out into sort
23:50
of a different world into telegram that
23:53
didn't involve Mac OS. One of the things
23:55
that you can show and we did that by
23:58
making this web hook call into Kilroy.
24:01
We'll go the other direction. Maybe the
24:02
LLM goes and decides I mean for a while
24:05
I had a light on my desk that I would
24:07
turn the light bulb green when Bitcoin
24:09
was up and turn it red when it was down.
24:11
And so all day long I'm watching you
24:14
know Christmas show over here on the
24:16
wall as the light toggles between green
24:17
and red. But that was an example of
24:20
Kilroy asking uh Mac OS to use the home
24:25
automation functionality in Mac OS to
24:28
set the color of the light bulb. Right?
24:30
So it's just as easy for Kilroy to talk
24:32
out to shortcuts as it is for shortcuts
24:34
to talk into Kilroy. And there's a block
24:37
that's built to do that which is the the
24:40
opener block which we talked about a
24:41
little bit last time. And if you look at
24:43
what it does, it really is just calling
24:45
the shortcuts URL that we talked about.
24:48
and it uses a function inside of Kilroy
24:50
that's able to send deep links into the
24:53
local OS. So on Windows, this could be,
24:56
you know, running a different automation
24:58
environment. On uh Linux, it could be
25:01
invoking shell scripts, whatever it is
25:02
that you want to tie in with uh the deep
25:05
links that are supported by the
25:06
operating system. So it's completely
25:09
birectional, which means that Kilroy
25:11
could invoke Siri to do things. Siri can
25:13
invoke Kiloy to do things. And the end
25:16
result is is that you've basically
25:18
extended all the functions that you
25:19
could do with Siri and other uh
25:22
automations on your Mac to include all
25:24
the stuff that Kilroy can do with DeFi
25:26
and blockchain and integrations with uh
25:30
exchanges and outside services like
25:33
Telegram or WhatsApp or whatever else
25:35
you have tied in.
25:37
Yeah. I mean, the the bottom line is is
25:40
that
25:42
this gives Apple an agentic
25:46
operating system into the Mac OS
25:49
operating system, right? And so,
25:51
well, don't tell them, but it also does
25:53
it probably for Windows and Linux.
25:55
Well, yeah, of course it'll do the same
25:58
thing for all of those operating
25:59
systems. But um the the you know the the
26:03
beauty of this is that you can unleash
26:05
all kinds of power with the computer and
26:07
capabilities you already have and you
26:09
don't have to pay chat GPT or OpenAI or
26:14
Anthropic or anybody else for your AI
26:17
processing capabilities. And if you need
26:20
more powerful beefier LLMs for some
26:24
task, the swarms in the network will
26:27
allow people to share GPU and LLM
26:29
resources with you seamlessly, right? So
26:32
you may have those resources and then
26:34
you um want to make money on them. Well,
26:37
you'll be able to do that through
26:38
transactions within Kilroy. So lots of
26:41
opportunities here. Um and we have a
26:44
guest on the show,
26:46
Maple. Maple, you know how she is, Brad.
26:49
She hears your voice and she decides
26:50
it's time to go outside. So,
26:52
well, you know, at least this time she
26:54
just wants to see you. That's it. So,
26:55
that's fantastic, Chuck. Anything else
26:57
you want to cover?
26:58
No, man. I think we I think we did it
27:00
and we got it done in 30 minutes again.
27:01
Yay.
27:02
That was beautiful. Beautiful.
27:04
Beautiful. Nicely done. Nicely done. All
27:06
right, folks. C Shottton Ct
27:10
on any network that you may be on. Um,
27:14
B05 on Farcaster for me. Uh, B05 Crypto
27:18
on Telegram and Twitter. We'd love to
27:21
hear from you. We'd love your ideas.
27:23
We'd love to work with you. We'd love to
27:25
integrate your protocol with our
27:27
platform. Um, and we're looking for
27:30
people to help us test and build and
27:32
grow the ecosystem. So, um, please reach
27:35
out. We'd love to hear from you.
27:38
Awesome.
27:38
All right. All right. See you next time.
27:40
Later, folks. Thank you. Bye. Bye.
ROTP: How to easily build agentic swarms in Kilroy Part 3

Realm of the Possible
4 subscribers

Subscribe

0


Share

Save

Clip

5 views  Streamed live on Nov 7, 2025
Last week we dove a little deeper into the mechanics of building agentic pipelines with Kilroy and this week we're going to get into integrations with Telegram and Mac Shortcuts. 

Learn how you can be empowered to build anything with a powerful decentralized automation and AI platform. 

We've created the most powerful decentralized AI and crypto automation/integration platform while being completely decentralized. It comes complete with no configuration peer-to-peer/swarming capabilities, the ability to easily build integrations with any other compute, and a Turing complete visual creation tool anyone can use to build functionality in the network while giving agents on Kilroy the ability to build and deploy on their own.
Transcript
Follow along using the transcript.


Show transcript

Realm of the Possible
4 subscribers
Videos
About

All

Computers

15:40
I Quit an AI Startup After 6 Months - Here's What I learned
Brian Jenney
191K views
•
2 weeks ago


13:55
this tiny $100 pc replaced every streaming service
Switch and Click
436K views
•
13 days ago


18:57
Russia Challenged NATO Jet — Big Mistake
Beyond Military
795K views
•
3 weeks ago

14:31
How To Run Private & Uncensored LLMs Offline | Dolphin Llama 3
Global Science Network
832K views
•
10 months ago

13:24
LAWYER: How Cops Use Apple's NEW Tech to Spy on Your Phone
Hampton Law
381K views
•
8 days ago

5:25
New Barista Training - SNL
Saturday Night Live
7.1M views
•
11 months ago

13:11
We need to talk... about the Proton ecosystem
The Hated One
169K views
•
2 weeks ago

24:19
NEW Walking Method 2x Better Than Running?! (Tested)
Jeremy Ethier
621K views
•
8 days ago

25:57
Stop Rambling: The 3-2-1 Speaking Trick That Makes You Sound Like A CEO
BigDeal by Codie Sanchez 
191K views
•
7 days ago

11:05
Will Ferrell Refuses To Discuss The Bird On His Shoulder | CONAN on TBS
Team Coco
1.4M views
•
3 months ago

17:35
The Future of Veritasium
Veritasium
3.4M views
•
5 days ago
New

9:23
"AI Can’t Replace Juniors" - AWS CEO
The PrimeTime
262K views
•
2 days ago
New

17:54
I Quit Every Streaming Service… Here’s What I Use Now.
Switch and Click
1.6M views
•
1 month ago

15:30
Ex-Google CEO Warns "AI is Becoming Conscious"
AI Upload
32K views
•
13 days ago

12:35
How To Learn So Fast It’s Almost Unfair
theMITmonk
870K views
•
10 days ago

33:14
Ethernet is DEAD?? Mac Studio is 100x FASTER!!
NetworkChuck
310K views
•
9 days ago

16:04
Mr Bean does 'Blind Date' | Comic Relief
Comic Relief
22M views
•
16 years ago

18:55
the creator of Claude Code just revealed the truth
Wes Roth
7.4K views
•
3 hours ago
New

18:36
What's BETWEEN the Atoms? Feynman's Answer Will Break Your Brain
Physics The Feynman Way
267K views
•
6 days ago
New

20:36
An atheist explains the most convincing argument for God | Alex O'Connor
Big Think Clips and 2 more
373K views
•
11 days ago


Show more
    


NL

Skip navigation
Search



Create


Avatar image
Transcript


Search in video
0:00
today, sir. Hey, Brad. Back from the wilds. You're back in the wild.
0:05
Back from the wilds of uh Where were you? Colonial Williamsburg. Colonial Williamsburg. Did you wear did
0:12
you wear a colonial costume for that trip? Um yeah, my colonial ski jacket and my
0:17
colonial stocking hat. Did uh did your um did your wife wear uh
0:23
that hat I gave her? No, I wouldn't let her.
0:31
I sent Chuck's wife a hat that says, "I'm sorry." Does it say, "I'm sorry
0:38
about my husband or for my husband?" I'm sorry about my husband. I'm sorry about my husband. Which is
0:43
just perfect for for Chuck. It's perfect. It's perfect.
0:50
Uh all right. So, uh, today we're going to continue on a little bit with, uh, uh, trading, uh, demonstration and
0:57
Kilroy, but let's cover a little bit of, uh, AI news today. So, oh, I should
1:03
share screen, shouldn't I? I should take this off the main tab and close that
1:10
one. Really, probably should have done this before we started.
1:15
Uh, all right. Share screen. uh screen
1:23
window. There it is. All right. So, uh on
1:29
Farcaster, um I uh shared yesterday that Microsoft released um a study on uh user
1:37
behavior on C-Pilot. Um and they showed this chart that um shows all the things
1:44
that were people were looking up, searching for, asking about, needing help with etc. And kind of the
1:50
percentage break the breakdowns of you know what the rank was for the types of
1:55
things that people were doing. Um and you know people were sharing this and
2:01
fascinated with the data and what people were using AI for. And I'm like, does
2:07
anybody understand what this friaking means? Because, you know, we all think
2:13
about the fact that and accept that basically all the social media giants invade our privacy and use our data and
2:20
use our content and make money from our content, etc., etc. But if you just
2:25
think for a minute about the fact that AI is going to replace most of compute and
2:32
you're feeding your entire life into an AI, well, the centralized component of
2:37
that where someone has your entire life in their databases and in their systems
2:43
um to be able to run a study like this um really tells you how much they will
2:48
know about you and how much they can control what you know, what you learn, what you do, and how you do it. Um, and
2:55
to me this was an indictment of centralized AI more than, oh, isn't this
3:01
fascinating what people are using AI for? I mean, certainly that data is valuable in terms of building things in
3:08
AI, but the if people can't see what's coming for them, I just I really I really have
3:15
a difficult time why understanding why people continue to feed their lives into these things. But, you know, that's just
3:22
Well, I'll I'll tell you one that's similar to this that I just ran into two days ago that's just absolutely
3:28
frightening, right? So, now you've got forever we've had SEO manipulation, right? People playing with search
3:34
results and trying to game where they show up in Google's rankings, all sorts of games inside of their web pages.
3:41
Well, now they're doing it with what's clearly going to be training data that the agents gobble down and use to
3:47
present results. So now people are practicing at gaming what the AIs are
3:53
going to eat and return and produce when the user asks for something after it's
3:59
been trained on that source data. And the really amazing thing is that they're
4:04
doing it with commerce sites so that their products show up higher and get recommended. And so
4:09
we're gonna have this arms race between producers of content being catalogs on
4:17
Amazon or whatever other e-commerce crap is going on and the AI people trying to
4:23
or not trying to produce factual results. So now you're going to go ask uh you know chat GPT what you should buy
4:30
grandpa for Christmas and you're going to get a bunch of branded results that have been jammed down the throat of the
4:36
training agents. Yeah, exactly. And it won't even look like advertising. It'll just be the agent found the best product for you.
4:42
Oh, you need this? We found you the best deal. Don't worry. Trust us, man. I just You're right about the
4:48
centralized thing. You're just willing to say, "Okay, I think big brother up in the cloud is smarter than me, so I'm
4:54
going to just give it my whole life and let me tell it what to do." I mean, it's kind of like that old Star Trek episode
4:59
of the two planets that were fighting each other and and once a month it would just print out the list of people who
5:04
had to send themselves into the disintegration chamber because it was time to die because their simulation said so.
5:11
Chat GPT could tell, "Hey man, your time's up. Go down to the crematorium. The box is ready."
5:16
It's amazing, man. It's absolutely amazing. Have by speaking of Well, have
5:21
you are you caught up on Plurbus? Yes, I am. Okay. Okay, cool. Um, so anyway, and you
5:28
know what it reminds me of is I was flabbergasted. I don't know remember was it was like four or five months ago when
5:34
OpenAI launched memory in their agents and
5:39
everybody was oh so excited. Look how much better it is. It remembers everything that I want that I that I've
5:46
put into it. And I'm thinking, you're excited that they are going to
5:51
store everything you're doing in their database and use it to make
5:58
their better. And by the way, control what you get. I I'm I'm just
6:05
Anyway, okay. I I Well, look, man. It used to be that you could at least look at the URL and see
6:10
the source of the information that you were getting and tell maybe from the URL is it a scam site or is it not or is it
6:17
old data or is it you know where is it coming from. Now when you're just flinging text into a text box and
6:22
looking at the chat info that comes back there's no provenence on that data at all. No, not at all. Not at all. Uh Microsoft
6:31
has released a competitor to Olama which um I found to be uh really cool and this
6:38
this actually has some pretty cool functionality. The one thing I haven't been able to find, um, you know, they have a homebrew installation for Macs,
6:46
but it doesn't list anything related to optimizing for Apple silicone, and I'm I
6:52
doubt they do because this core code comes from uh their their Azure um
7:00
inferencing capabilities, but essentially it's a free open-source local inferencing engine. I'm glad to
7:06
see it coming from Microsoft since, you know, we know they're collecting all our data if you're using Copilot. But, um,
7:14
it's actually pretty cool. But the really cool thing about this is that something that Olama doesn't have, and
7:19
it's actually a pain in the ass about Lama. Every time I see a new um, LLM
7:25
come out that I want to give a shot, and I'm going to show you uh, one right now. um I have to wait for Olama folks to
7:34
convert it or make it available um in the system or I have to use the hugging
7:41
face connection which sometimes works and sometimes doesn't work if it's compatible. This inferencing engine
7:48
allows you to load things from hugging face and it will do the quantization conversions for you to work within
7:55
Microsoft Foundaries uh inferencing engine. So, um, really cool, really impressive. That looks like they're
8:01
planning to really build on this. They've got full documentation for it. They, um, uh, obviously are making it
8:08
freely available, but I'm kind of I kind of wonder when I see moves like this is maybe it's just covering their ass, but
8:15
I also kind of wonder if they're starting to realize that, oh, you know what, we may really need a solution for
8:20
this. And it could be when they start thinking about the advantage Apple has
8:26
with um local uh inferencing capable
8:31
with Apple Metal and even on Apple phones and iPads that they may start to
8:36
realize that a competitive issue for them is going to be the privacy issue because Apple has that in the bag. I
8:43
mean, nobody's going to question Apple's privacy commitment anyway, but the fact that I can now run these models on my
8:48
machine and Apple is going to be optimizing for these small models is really impressive. That also brings up
8:54
something else. Did you Did I send you the piece where Sam Alman uh last week was doing interviews because their
9:00
models aren't performing up to snuff and so they're kind of in panic mode and so he's trying to talk to the press about how great they are.
9:06
Well, that was the 5.2 release frenzy yesterday, right? Yeah. Well, and and everybody making fun
9:12
of it. Um and and um I saw one that said
9:17
uh they asked it 5.2, how many letters in garlic? And it said zero. Um but um
9:26
Sam Alman said last week he doesn't care about Google or Microsoft or any anybody
9:32
else. Their number one competitor in the future is going to be Apple and that's the one he fears the most.
9:39
So, well, there was a there was a a similar article, same flip side of the same coin
9:44
that came out which said, "Look, as it turns out, Apple's slow out of the gate start on AI is going to turn out to be
9:50
the right strategy, and they're going to be the biggest winner because they didn't go down these wrong paths."
9:56
Exactly. And And I'm I'm I'm sorry, but you know, I'm going to pat myself on the back. For two frigin years, I've been
10:02
saying Apple is the best position company in the world to dominate I AI. They have hardware. They have integrated
10:09
a OS. They already had developers building functional components of their apps to make available to use in their
10:16
OS. And between the hardware, the OS and the apps that nobody else has that
10:23
integrated combination. Well, certainly not a startupish business like OpenAI. They're lacking
10:29
the hardware. They don't have the OS. Well, they're trying to do it now. That's why he called hired Ives. They're
10:34
developing a a an a screenless AI box, but it's like, dude, you're so far
10:41
behind. I'm not sure what Tony Ives does if you don't give him a screen to play with. It makes it happen. Well, I mean, he's a UI designer, so
10:48
it's like, dude, I mean, it's got a great box. How creative are you going to get?
10:53
Ultimately, it's just going to be an interface to wellpt.
10:58
At the end of the day, if you use a strategy like we're using in Kilroy with
11:04
the swarm-based models, you're not using, you don't even know where the AI is that you're using. It's certainly not
11:09
going to be in the box on your desk, right? It's going to be whoever is in your peer up swarms that has a tool that
11:16
you can use at that moment. That's right. Right. I mean, I'll tell you though, in the in just
11:21
talking about centralized a AI holding on to all your goodies, there is a use case for a local box, which is that's
11:28
the box that holds all of your profile information and makes it available
11:34
through some something or I don't know, maybe some swarms and a bunch of maybe some peerto-peer, right? Because if you don't put if you
11:40
don't put that info in the cloud, you're not going to be able to act on that info. That's right. Unless you've got it somewhere else. So
11:46
if you've got it somewhere else, i.e. locally, then you can work with it. But most users aren't going to have the
11:52
wherewithal to stand up that stack themselves right now. That's right. That's right.
11:58
So it does make sense to think, well, the same way that I would buy an iPad or a, you know, a desktop PC for home use
12:04
or I've got a a home automation hub like an Apple TV or a, you know, Fire Stick
12:11
or a Google Home Box. I might now want to have uh an AI box with a bunch of
12:18
storage and enough agentic interfaces in there that my other stuff can talk to
12:23
it, right? Oh, that's what you and I guess we should get to work on that because I'm not sure that's what they're building and we don't have to do much
12:29
but add some hard drives. Exactly. Um, I have a setup subscription, which is a Mac OS service
12:34
that lets you use all these apps that are in it. And they came out with the people that build setup, MacPaw, came
12:40
out with an app that's supposed to be an integrated desktop companion for your Mac. Uh, looked really cool. So, I
12:46
thought, oh, I'll test it out. Maybe we can learn some things from it. You know me, I'm constantly trying out everybody else's crap so that we can figure out
12:52
what we could utilize and I can throw more stuff at you that we need. And, uh, I was impressed with the way this looks.
12:59
So, I installed it, but then I was about to agree to the privacy policy because
13:04
it's local. It's supposed to be local. They emphasize in the in the thing that it's local only, that it works with your
13:10
local LLM. And I'm like, okay. But I looked at the I looked at the privacy policy and guess what?
13:17
They're taking they're sucking back every piece of input that you put into
13:23
the app for improvements. I get it, but I don't want you to take all of my
13:29
out. What's the point of having a local inference except to save you money?
13:34
We're gonna get all your Hey, Tom Sawyer, let's paint my fence, right? Exactly. Exactly.
13:40
You run our AI agent and it'll send us all of your data and we just have to sit here and catch it. Yep.
13:46
You guys, you and I got a big kick out of this big announcement. Oh, wow. Yeah. Neuro symbolic resonance
13:52
console. Neuro Neuro symbolic resonance console. So I sent this to Chuck because
13:58
in Telegram because I'm like, you know, I'm not the most technical person when it comes to the latest in machine
14:04
learning and and neural networks. And so maybe I'm just missing out and and my
14:09
because my initial response to the the rhetoric in this thing was like, "Oh my god, this is such a bunch of freaking
14:15
bullshit." And it is It's just
14:20
I mean, what was your theory, Chuck? So anyway, let me quickly read some of this and you will tell your theory. So you know we stand at the precipice of an
14:26
evolution of artificial intelligence. You know the standard startup revolutionary
14:32
uh thing to be introduced. This is a next generation's neuros symbolic AI platform that shatters this paradigm.
14:39
It's not just another LLM plat uh rapper. It's a fundamental reimagining of how intelligence is structured using
14:45
prime resonance as its semantic and operational backbone. And I'm thinking who is this targeted to? like who knows
14:51
these terms re AI researchers like is that who's going to buy your product so
14:57
um and you continue to read through it and it's just this crazy concept of how
15:02
they are identifying what a particular query is about whether it's duality and
15:08
existence structure and interaction change in dynamics identity and recursion and then that somehow improves
15:13
the AI's response it's just but anyway what was your what I asked you
15:18
whether you thought it was legit it or if I Well, I'm gonna I'm gonna say I'm gonna say three things. First of all, the
15:25
irony of all this is that we're going to find out in a year or two that this is some amazing breakthrough and we were just realizing.
15:33
But my my comment this morning, I think when we were talking about it was this is somebody's PhD level thesis that
15:39
should have stopped with a BS. Well, there you go. Because they're dispensing too much BS
15:46
here. Well, here's why it's BS, right? Conceptually what they're trying to do is cool. They're using I mean there's like gigantic prime numbers. There's
15:53
like an infinite supply if you can figure them out. And they're trying to map prime numbers to some ontology or
16:00
taxonomy or whatever that says this thing is in this group. And by multiplying the primes together, you can
16:06
say this thing's in these two groups and if I factor it, I can see which two groups that are it's in. And then somehow you associate that with the the
16:13
vector. But the problem I I think the author doesn't understand that LLMs don't tokenize at the word or the
16:20
concept level. Sometimes it's down at the individual letter or symbol level or symbol sequence. And so there's no
16:25
mapping of the the weights and the vectors in an LLM to something that a
16:31
human thinks of as a word or a concept. And even if there was, who's making that join? because the LLMs are just looking
16:38
at statistical patterns in the input that they are trained on, not the
16:43
semantic understanding of it. So unless you've got some other process that's layering semantic and understanding over
16:49
top of the the dumb vector math, this is this is one of those things where I got
16:54
this great idea and some magic happens and then over here's Nirvana. Yeah, exactly. There's nothing in here about how the
17:00
magic happens of how do I really apply the semantics to the data that the LLMs are understanding. Even if you could map
17:06
it to prime numbers and get it into the the data set, the question is who's doing that? Because none of this stuff
17:12
is smart. It has to be a person, of course. How are you how is that automatically classifying if the AI
17:18
engine it doesn't make sense. There's there's no cart before the horse here. It's it's
17:23
insane. And this is the thing that really keep me off to the BS. ALF is tomorrow's AGI platform here today. So
17:31
you're telling me you have released superhuman level human level intelligence
17:38
in this platform and it's ready to go like no I'm sorry
17:44
there's no way. So well like I said we're going to find out we would have heard about your we would
17:50
have heard about your half a trillion dollars in funding in your in your seed. When Skynet calls you up and tells it's
17:55
time for you to go in get into the death chamber and it's driven by this guy's stuff. You're going to be sad.
18:03
Okay, I am going to stop sharing my screen. Let me start sharing mine because we
18:09
have to we have to resume our uh our our tutorial in creating
18:17
spiffy cooly bot. So, um last week, oh wait, I gotta add there you go.
18:22
You gota let me in. Yep. Um, so last week we were working on automating the process of trading
18:30
Dogecoin, right? See, and we had made this thing and this
18:35
thing starts with some input from the user and that's either uh a command or
18:40
not a command. And the command stuff that we could do last week was get me my positions in the portfolio, set a buy
18:48
limit for what I want to buy. And then we talked about how we are using uh the LLM's context as a storage mechanism to
18:55
be able to be an oracle providing values to further downstream compute. So if I want to know what limit the user has as
19:02
a price limit for Doge, I can ask this LLM and its sole job is to say the limit
19:09
is X, whatever X is, right? And and so what we need to do after that
19:14
now is we need to be able to get the current price of Doge, compare it to the users's limit, and then decide whether
19:21
or not we want to buy or sell, right? And so the act of doing these
19:27
comparisons is one of the things that a lot of LLMs have trouble with. Compare
19:33
two numbers is, you know, 1.1 larger or smaller than 1.11.
19:39
Well, I don't know. Is 1.1 larger or smaller than one,
19:45
right? you know, is it are do does it get confused and think you're talking about the length of the string, the ordinal
19:51
values of the characters, the numeric value, you know, it's it's interesting
19:57
to watch them fail. And so, one of the things that we did, and just to remind people, I'm going to run this one real
20:02
quick just so you can see it go. This one had some commands. Uh, position,
20:10
I think it was positions, and it goes off and fetches uh, that right? Yeah, it
20:17
goes off and fetches the positions from my exchange agent. And this one's I think running against my uh, my Kraken
20:23
account and it comes back and says this is what I've got in positions. And Doge right now, it's interesting Doge is
20:29
dumping because earlier when I was playing with this, it was at 0.141. Um so uh if I want to set the limit I can
20:36
say the the limit is uh for purchases is anything less than 0.14
20:43
right and that's going to go off and feed it into the LLM and I get no confirmation back but I can say what is
20:50
the limit and it's going to tell me oh the limit is 0.14 right that's that's so
20:55
that's that little Oracle LLM out there holding on to that value for anybody in the form to use
21:01
right that's all it knows how to do is tell me what the limit is. And so the next thing we want to do is do some comparisons. So
21:08
I'm going to throw this one away. And while it's throwing away, I'm going to switch over here to the this little
21:14
pipeline segment. This thing's job is to is to take some input. And in this case,
21:20
we're just going to use a a chat agent to do it. Take some input from the user. Two numbers. And we've got three
21:27
different LLMs, AI 1, AI2, and AI3. They've all been given different system prompts. It could be physically
21:33
different LLM. One could be chat GPT and one could be Claude and one could be O Lama running my my old pal Lama 3 here
21:40
locally. And they can all come back with different answers. I could give three different LLMs the same prompt. I could
21:46
give one LLM three different prompts. And in this case, I've done the latter. And for instance, AI1's system prompt is
21:53
you're an agent that compares two numbers and outputs the largest. Only output the number by itself. Do not generate any explanations or additional
21:59
text. format the number output as plain text. That one's a pretty explicit uh
22:05
system prompt. This one is a little different. It's simple. Compare two numbers and output to largest. Only output the number by itself with no
22:11
other output. And then the third one is uh you're a very tur quiet agent. Your
22:18
job is to compare two numbers from the user and output the larger of the two. So it's three different ways of saying
22:23
the same thing. And what's going to happen is the input from me, which is just going to be a pair of numbers on
22:28
the command line, is going to go to all three AIS in parallel. They're all going to do what they were asked to do and
22:33
output the one that they think is bigger into this swarm. We have a a fourth
22:38
agent, a fourth LLM called the evaluator whose job it is is to take three numbers and find the matches. Right? If there's
22:45
two that match and there's a third that's an outlier, then the two that match are probably the right answer. So
22:51
this little guy's prompt is you'll receive three separate inputs from the user. Each input is a number. When you have received all three numbers, compare
22:57
them. So it knows that it's going to get three in order. The context is going to build up before it actually does the
23:03
work. So we've got three agents feeding into one, they're going to come in as three separate messages and then it's got some instructions that if two or
23:09
more of the numbers are the same, the answer will be that number. If no numbers are the same, i.e. none of the
23:14
previous LLMs got any kind of answer consistent or they all screwed up, the answer's error, right? and and it's got
23:20
some some examples of proper output. We want to see the answer is some number so
23:26
we can use that further downstream to see uh whether those numbers compare correctly or not. So let me let me um
23:33
instantiate this pipeline real quick run. Um
23:38
don't die. Excuse me. And so we've got where I'm going to type
23:43
and we've got AI1, AI2, AI3, and then the evaluator. And so what we should see
23:49
is if I put in, you know, three and seven, it should they should all realize that seven's larger than three. You'll
23:55
see the three uh LLMs on the right go into compute mode and then the fourth
24:01
one will decide, you know, it's throw out the Russian judge and take the other two or whatever. So here we go. So they
24:08
all return seven. That guy decides that seven's the answer. The answer is seven, right? And I can do some weirder stuff
24:14
like 2.1 and 2.01. Right. I I think the answer is 2.1.
24:20
Right. Let's see what happens. So, I gotta tell you, man, Llama 3 is
24:26
really good with numbers. I'm still impressed with this two-year-old model. It's incredible. I I don't like I keep
24:32
messing around with other ones just because, you know, me, I got to always try the other ones and, you know, ADHD brain. But, um I I I'm just
24:40
flabbergasted with how good it is. It's just ridiculous.
24:45
I don't think Meta realizes what they had. I don't know. I don't They They should have quit while they were ahead. This thing does everything that I ever wanted
24:52
to do. Well, they're not thinking about breaking it down into small contextual agents. They're thinking about
24:57
I'm going to show you something because I did this. I did this. I said, "You know what? We don't want to know which number is bigger. We've got the current
25:03
price and we've got the limit price. So, we really want to know whether we should buy or sell, right? If the current price
25:08
is less than the limit, we want to buy. If the current price is greater than the limit, we want to sell." That seems that
25:14
seems straightforward, right? So, let me uh let me nuke this pipeline and we'll go to a second version of the same
25:22
pipeline. Looks identical, right? It's the same thing except I've changed the prompts. The system prompt for this one
25:27
is you are an agent that compares two numbers. If the first number is larger than the second number, output the word
25:32
sell. Right? If the second number is larger than the first number, output the word buy. Right?
25:37
And it's the same for all these others. I just changed the comparing the numbers and outputting the number to comparing
25:43
the numbers and outputting buy or sell. Right? They're all they're all roughly similar. Output buy or sell. And then
25:48
the last guy down here just says, hey, you're going to get three words. If two of them are the same, put it out, you
25:55
know, buy or sell. And if they're different, then it's an error. So the interesting thing, first of all, we want
26:01
for a buy to come out, we want the first number to be less than the second number, right? And for a sale to come
26:07
out, we want the current price to be more than the the limit price.
26:12
And I got to tell you, I for all my glorification of Llama 3 just a minute
26:17
ago, wait till you see how this works. And it's it's totally about the prompt strings. I have to go back and play with
26:23
the prompt strings a little bit because I didn't I obviously didn't figure out yet what this thing is doing. Let's get
26:29
let all these guys get in the swarm together and get their green lights on and then we'll use some prices that are
26:35
roughly similar to uh the Doge prices. So the current price of Doge was 0.13 my
26:41
limit is 0.14 right so um
26:47
that should be a buy right the current price is 13 the limit price is 14 the answer should be buy right
26:53
right watch what this does one of them decided buy and two of them
26:58
said nope sell oh right so the arbiter and and what I've
27:04
found out is that my whatever I told it it consist consistently works exactly the opposite. So, if I reverse that and
27:10
do 14 and 13, it's going to tell me that I should buy.
27:18
So, I don't know what it is about the language choices I chose in the prompts, but it's it's consistent enough that all
27:24
I have to do is invert the output. Right. Right. I one one and five. Right. Let's see.
27:30
And I think that's just going to carry on through with all of them. So obviously this is a great thing for us
27:36
to learn about context and prompting um within the context of breaking this down
27:42
into small pieces is understanding, you know, better what what makes that
27:48
happen. Doing it. We've got three different prompts. We're getting different results every time. This time it was actually uh
27:55
only one of two of them said buy this time and I don't maybe it doesn't like the decimals. I don't know. I'm gonna
28:00
have to Well, I think it's the decimals up. It might be. Let's see if it 44 and 55 should be a buy, right? One of them's
28:08
stupid. One of them The first guy up there keeps saying buy, right? This one hilarious. I don't know what's wrong with this one.
28:14
But and and they both the majority answer was sell. So these two are these two are wrong. And if I do I don't know
28:21
66 and 12, which should be a cell, right? Oh, they got they all got that
28:26
one right. So, I wonder if there's like a like if you put a differentiator like a um some
28:31
keystroke between the numbers if that would help them differentiate better. I wonder
28:38
it knows that it's it's seeing two numbers. It's good at that. Okay. I could even put them on separate lines.
28:43
I was gonna I was just gonna say that. One and two. Oh, okay. We have to drop for a call in one minute, don't we?
28:49
Yeah. We're uh Well, this is what I wanted to show is this technique of using Oh, no.
28:55
Three agents. Yeah, that calls at 12:30. Three agents to decide the veracity of their respective outputs with a fourth
29:02
guy. You know, it's funny. Back in the day when I worked on the worked for NASA, some of the space station stuff.
29:08
It was great fun to look at how the computers on the space shuttle worked. And they had four computers that would all come up with an answer. And if they
29:14
couldn't agree and there was a deadlock between the two, should we, you know, fire the thrusters or not or whatever,
29:20
there was a fifth one that was built by a different company with a different software that would tie break. Aha.
29:26
Right. And so this is a really similar model to what they used to use to figure out hard navigation problems on the
29:32
space shuttle. Yeah, that's cool. That's very cool. You know, it's funny. Uh I had another thought spur off of a call we did
29:40
yesterday. So we're working with a guy on potential tokconomics models for the Kilroy launch. And um really smart. He's
29:48
got a mathematics PhD, right? Yeah. Yeah. Really smart guy, really nice guy.
29:53
Enjoy working with him. And when we got off the call yesterday, I posted on Farcaster that if you're launching a
29:59
business, you should have someone who's able to ask you
30:06
tough questions that make you think in a professional way, right? Because that's kind of what he was doing with us
30:12
yesterday. You and I are deep in the weeds and building Kilroy and thinking about what we're going to do and we've had big picture conversations about it
30:18
strategically, etc. But he was asking like at no point in the conversation did
30:24
I feel like and I'm not sure why I'm bringing this up except whatever. Um but it was actually it made me think about
30:30
this structure this agent and I'm going to parlay that into what I want to build. Um it it actually made me really
30:38
think through things that I hadn't been thinking about lately and and those sat with me a lot yesterday and I thought
30:44
you know every business person should have that and then I thought well actually we should all have that in our life in general like I need somebody ask
30:50
yeah I need somebody asking me the difficult questions in a polite way so that I'm thinking about what I'm doing. So anyway that's what I was thinking
30:56
about. I thought we we should build an agent that actually takes, you know, and that took me down the path of continuing
31:03
on the agent I was trying to build with just nicely ask, "Hey, dumbass, do you really want to do that?"
31:09
Exactly. Well, and you know, but I think that the the agents give us the opportunity to give them context to what
31:15
we're thinking about. So if we're like putting in there uh strategy documents and text I mean tasks and projects then
31:23
you we can build up that context over time and then it can like even product based on what we said before. So,
31:29
just back on our earlier conversation about centralized AI, if you're a company and you're using centralized AI
31:35
to figure out your business strategy, you are a flatout dumbass because if I'm
31:40
your competitor, I'm going to open AI and I'm buying every query that every one of your employees has ever jammed
31:45
down the throat. That's right. That's right. It that's exactly it, man. And I guarantee you
31:52
or I'm paying I'm paying open AI to train on a set of documents that sends that company all the wrong suggestions
31:59
there. Even better. I mean those are just I mean those are some of the possibilities, right? And and how many
32:04
times have we seen uh crypto exchanges or DeFi projects or people at Amazon or
32:11
people whatever where you pay off the support staff that happens to have access to the wrong thing to get you the
32:18
information you want and or to to slant the information that you want going out.
32:23
I mean, that's a beautiful example. That's that's fantastic. I'm adding that to the list. So, I don't know. We may be in the wrong
32:29
business, Brad. Well, I'm telling you, maybe we just need to be arch criminals or something. I don't know. But
32:36
just bribing the staff at centralized AI providers. Um anyway, th those are the
32:42
things that can happen to you. That's a great example. I love it. Um um uh I guess we'll figure out what we're going
32:47
to go through next week. Yeah. So, I'll put this in I'll put this in this week and ne and next week we can actually see it do a proper buy or sell
32:55
recommendation based on the real price. I love it. We should probably have a Christmas themed day.
33:00
project next week or something. We can make it. We can do that one and make it draw you make the uh Kilroy AI
33:05
draw um Christmas trees for everybody. There we go. That's lovely. We could vibe code an animate it. Well, let's
33:10
vibe code anode and animated Christmas tree. Okay, we'll do it. Blinky lights. All right, blinky lights. All right. Um thank you
33:16
very much everybody. Appreciate you listening and watching and um have a great weekend. Reach out to us if you want to get
33:22
involved with the project in any way. Uh B5 crypto on Telegram. B I'm sorry.
33:28
Yeah, B 05 Crypto on Telegram and Twitter. Um, B05 on Farcaster. Chuck is
33:34
Cshotton on every platform. So, reach out. We'd love to have you join us. We're getting people set up to using it
33:40
and playing with it and building their own stuff, and you can, too. Thanks. All right. See you. Bye. Say it.
ROTP: Building in Kilroy Part 4

Realm of the Possible
4 subscribers

Subscribe

0


Share

Save

Clip

1 view  Streamed live on Dec 12, 2025
Learn how you can be empowered to build anything with a powerful decentralized automation and AI platform. 

We've created the most powerful decentralized AI and crypto automation/integration platform while being completely decentralized. It comes complete with no configuration peer-to-peer/swarming capabilities, the ability to easily build integrations with any other compute, and a Turing complete visual creation tool anyone can use to build functionality in the network while giving agents on Kilroy the ability to build and deploy on their own.
Transcript
Follow along using the transcript.


Show transcript

Realm of the Possible
4 subscribers
Videos
About

All

For you

1:36:04
Ilya Sutskever – We're moving from the age of scaling to the age of research
Dwarkesh Patel
1M views
•
1 month ago


26:14
Run YOUR own UNCENSORED AI & Use it for Hacking
zSecurity
96K views
•
11 days ago


53:44
Building the PERFECT Linux PC with Linus Torvalds
Linus Tech Tips
4.5M views
•
4 weeks ago

59:26
He Built a Privacy Tool. Now He’s Going to Prison.
Naomi Brockwell TV
431K views
•
11 days ago

20:30
Instagram Pilot's Emergency Caught on Camera!
Pilot Debrief
1.2M views
•
8 days ago

14:10
How AI is Changing Manufacturing
Future in the Making
68K views
•
3 weeks ago

17:18
Laid Off After 25 Years in Tech: The Anxiety, Sacrifice, and Reality No One Talks About
Asian Dad Energy
663K views
•
10 days ago

17:08
When AI Gets an Innocent Man Arrested
EWU Bodycam
3M views
•
2 weeks ago

14:19
What Sam Altman Doesn't Want You To Know
More Perfect Union
2.3M views
•
10 days ago
Fundraiser

21:40
Neuralink Overview, Fall 2025
Neuralink
384K views
•
3 weeks ago

5:25
New Barista Training - SNL
Saturday Night Live
7.1M views
•
11 months ago

17:35
The Future of Veritasium
Veritasium
3.4M views
•
5 days ago
New

8:01
Current AI Models have 3 Unfixable Problems
Sabine Hossenfelder
944K views
•
2 months ago

19:47
How John Deere Robs Farmers Of $4 Billion A Year
More Perfect Union
6.7M views
•
1 year ago

9:23
"AI Can’t Replace Juniors" - AWS CEO
The PrimeTime
262K views
•
2 days ago
New

24:19
NEW Walking Method 2x Better Than Running?! (Tested)
Jeremy Ethier
621K views
•
8 days ago

16:47
Office moments I think about way too often
The Office
1.7M views
•
4 months ago

12:27
AI vs Oscar Winning Actor (Same Scene)
Tristan Spohn
855K views
•
4 months ago

21:43
Why tech billionaires are quietly bankrolling Europe’s far-right | Pinch Point
Al Jazeera English
313K views
•
13 days ago

16:44
THE CRAZIEST PART OF GETTING DIVORCED 😳 | Kelsey Cook Compilation
Spotlight by Laugh Society
424K views
•
2 months ago


Show more
    


NL

Skip navigation
Search



Create


Avatar image
Transcript


Search in video
0:00
possible. How are you today?
0:01
Good. How are you, Brad?
0:03
I am well. I am excited about what we're
0:05
going to show people today. I think it's
0:07
going to be really
0:08
gonna be cool, I think.
0:09
Yeah, definitely. Definitely. Um,
0:12
so, uh, one of the things that, um, you
0:15
and I or I'm working on, um, is the idea
0:19
of, let me share screen here. um the
0:23
idea that there are a lot of folks in
0:24
the research community doing really cool
0:27
models with prompting, with context,
0:30
with memory, with um LLMs that they're
0:34
using, etc. And all of the time they're
0:37
doing this, they're having to do it um
0:40
in Python
0:43
with 500p.
0:46
Yep. Exactly. And so, um, I wanted to
0:49
quickly talk about, uh, this actually
0:52
happens to be,
0:54
um, I guess it's not showing on screen.
0:56
Can you see that?
0:58
I can see it.
0:59
Okay, cool. I didn't know if it
1:00
immediately went for me or I had to show
1:02
it. Anyway, um, this is a um, a model
1:05
called Swarmsy. Uh, was put together by
1:07
a bunch of researchers, most of them out
1:10
of, um, Hong Kong and New York. And the
1:14
reason I reached out to these folks um
1:17
to try to get them uh to build their
1:20
research within Kilroy is because it's
1:22
an entire model of improving agent
1:26
outcomes using swarms. Um, and you know,
1:30
when you look at kind of how they built
1:32
this thing, the fact that they're
1:34
utilizing probably, you know, five or
1:37
six different Python libraries and
1:39
scripts and piecing it all together to
1:42
get their research, they probably spent
1:44
a huge percentage of their time
1:48
just building out the functions to
1:51
actually test their hypothesis. Well, if
1:54
they built it utilizing our capabilities
1:56
and then integrated the algorithm in as
1:59
a module, then they would be able to
2:02
significantly um speed up the research
2:04
they're doing and tweak and make changes
2:07
to the way they're doing research um if
2:09
they were using Kilroy. So,
2:11
you know, there's I hadn't even thought
2:12
about it till you mentioned it, but that
2:14
one of the things that's always a
2:16
problem is being able to replicate
2:18
somebody's research or duplicate the
2:19
work that they're doing or try to have
2:21
theirs as a starting point. And it's
2:24
always a pain. You've got to set up the
2:25
same environment. I've got to get the
2:26
same tooling. I've got to know the same
2:28
programming languages and stuff that
2:30
they're using. And that's problematic.
2:32
It's a little it's a little more
2:34
standardized in the computer science
2:36
world, but I spent a lot of time off in
2:38
the human physiology world at University
2:40
of Texas, and that's a lot harder to set
2:44
up the same lab conditions and all that
2:45
stuff. And it just dawned on me that if
2:47
you set up some pipelines and some
2:49
logic, add a few uh customized uh
2:53
workflow tools and uh a couple of Kilroy
2:56
apps, that's just sharable data. I can
2:58
give you the entire pipeline. And I can
3:00
give you all the tooling and I can give
3:01
you everything that you need to
3:03
duplicate what I just did in a little
3:05
zip file.
3:06
Yeah. And then you've got peerreview
3:08
processes made incredibly easy and fast,
3:11
right? Whereas, you know, any other way
3:14
of doing it, it's a manual difficult
3:16
process to set up. So it makes
3:18
works on my machine.
3:20
Yeah.
3:22
Yeah. So, and that's the beauty of
3:24
Kilroy, right? that that when you're
3:27
using Kilroy, it doesn't matter what
3:29
operating system you're running on or
3:32
what's installed on your computer
3:34
because
3:34
or even where your LLMs are.
3:37
Exactly. Because Kilroy is running as a
3:39
layer and it's not going to require um
3:43
uh all of the same configurations. So,
3:46
um it's really cool. This research is
3:49
actually great. I'll put it in the show
3:50
notes. Um but the idea was that they
3:53
created these ar agentic swarms that
3:56
allowed for um agents to have better
3:59
outcomes because as we've talked about
4:01
they had multiple agents working on
4:03
multiple tasks um and then find the best
4:06
solution from those tasks. It's a lot
4:08
more detailed than that but um that's
4:10
the general gist of it. So um something
4:13
else that came out uh this week was this
4:17
platform called Glyph also um another uh
4:20
research uh more research work done um
4:25
and the idea was that they could take
4:28
really long context length which is
4:31
something that happens with all of these
4:33
LLM and agent systems is you end up with
4:35
this massive context and we have one way
4:39
of solving that with Kilroy of breaking
4:41
context into smaller pieces through
4:43
multiple agents within a swarm that can
4:46
self-generate and that can be stored.
4:48
And we're going to actually show an
4:49
example of storing
4:51
um context and changing context on the
4:53
fly, which leads to agent
4:56
self-programming. But this was pretty
4:58
cool because what they found was when
5:00
they took this massive context files,
5:04
these big giant text files. Um, they
5:07
could convert those text files into
5:10
images and then use a vision language
5:12
model to interpret and read the images
5:16
and they resulted in better performance,
5:19
higher compression, um, less tokens used
5:22
for those that are using centralized uh,
5:25
LLMs. um it just improved everything
5:28
dramatically to take this in this case a
5:31
novel of 180,000 words and put it all in
5:35
image files so that they can be read by
5:38
the LLM and they built infrastructure
5:40
around that but again something that
5:42
Kilroy can solve in terms of breaking
5:44
context down but also something that
5:46
could be implemented in Kilroy for those
5:48
instances when a really long context
5:51
window is is needed to process some
5:54
amount of
5:55
just to Be clear, Brad. Were they making
5:57
images of the text or were they making
6:00
images out of the text?
6:02
They were they were making images of the
6:06
text.
6:07
So, it was just like like scanned
6:09
images, but that was better performing
6:11
than tokenized text.
6:13
Yeah. And it dramatically reduced up the
6:15
the token requirements. I don't know if
6:17
it because
6:19
Yeah. I don't know if it's because the
6:22
um the LL the visual LLM performs VLM
6:26
performs better than the text LLM, but
6:30
for some reason it could process it much
6:32
faster.
6:33
Wow.
6:34
Yeah.
6:34
Kind of cool though.
6:35
Yeah, absolutely. Absolutely. And they
6:38
they used they they built a custom model
6:40
for doing it. So it's obviously it's in
6:42
hugging face. It's something we could
6:43
implement um as a visual LLM visual LM
6:47
in um in Kilroy. So um
6:50
so
6:53
as soon as we can do pictures, right?
6:55
Exactly. Exactly.
6:57
So um so do you want to talk about the
7:00
toy I made this morning?
7:01
Yeah, man. Because I love that thing.
7:03
Let me
7:04
Well, I don't want to call it's not
7:05
really a toy, but the but it stems out
7:08
of Let me give you a little bit a little
7:10
bit of background. So, for the last
7:12
couple of years, we had an ongoing
7:14
collaboration project with some folks in
7:17
the Bitcoin community to try to use
7:19
Kilroy to set up uh a decentralized
7:23
exchange for perpetual swaps for
7:25
Bitcoin. And that involves all the
7:27
things that a centralized exchange has,
7:29
right? uh order book and pricing and uh
7:33
bid ask uh order fulfillment and sort of
7:37
a consensus about uh what's going on
7:40
with your your peers in this network. So
7:44
it was to be decentralized. It was a
7:46
decentralized order book. It was
7:48
decentralized order processing and it
7:50
was decentralized order fulfillment
7:52
because with per there's all this
7:55
rebalancing and a bunch of other stuff
7:56
that has to go on. So, we spent a lot of
7:59
time doing a lot of extra code inside of
8:02
Kilroy to support the whole idea of per
8:05
contracts and how to keep the contracts
8:06
balanced and how to do the bid ask
8:08
pricing and the the order book work. And
8:12
it it just kind of got overcome by
8:15
events, right? And it dawned on me this
8:18
morning because I was watching the price
8:19
of Bitcoin going going sideways and
8:21
saying, "Man, I wish there was a way to
8:22
hedge this and why didn't we have that
8:24
stupid decentralized exchange?" I
8:27
thought, well, you know, we have all the
8:29
pieces to do that now with our swarms
8:31
and the collaborating agents in the
8:33
swarm. All the stuff that we were
8:34
building with custom code two years ago
8:37
is now drag and drop in the agent
8:39
builder, in the pipeline builder with
8:41
the stuff that we have now. So I thought
8:43
well what's a good example of how to do
8:46
uh sort of consensus document management
8:48
right it's the order book is a shared
8:50
document right it's sort of a virtual
8:52
spreadsheet or a virtual database that
8:53
everybody participates in with their
8:56
orders and this is going way back so
8:59
back in the you know 80s 90s early days
9:02
of of Unix pre- Linux even on on the
9:07
internet people used to have what was
9:09
called a plan file it was a little text
9:11
file that lived in a level of your login
9:14
directory and you kept information in it
9:17
if you used it about what you were
9:18
working on, what was your plan for the
9:20
day and you could query people's uh plan
9:23
files by using a plan command and their
9:26
uh host name and uh username on that
9:29
machine and it would tell you what their
9:30
plan was. So I thought well what if we
9:32
reimplemented that in Kilroy as a way
9:35
for a bunch of people collaborating in a
9:37
swarm to provide some shared information
9:39
on demand. Same thing as an order book,
9:41
right? What's in your order book? What's
9:43
your current price for the for the swap?
9:46
What you know, all of the details that
9:47
need to be shared amongst everybody in
9:49
the swarm to make sure you have
9:50
consensus. Well, let's just do that with
9:52
plans. So, I sat down this morning at
9:54
8:00 and by 8:30 we had swarm swarmed
9:58
plans for workg groups working in
10:00
Kilroy. And what does that mean? I can
10:03
sit down and Brad, why don't you pop
10:05
open the the screen for Kilroy if you
10:07
can do that.
10:09
Um, this is kind of a scary view.
10:12
Really, all you care about is this uh
10:14
text area on the lefth hand side. These
10:16
are all just uh views into the various
10:19
agents that make up the pipeline. And
10:20
I'll show the pipeline in a minute. But
10:22
the idea was I should be able to get
10:24
online and and uh I don't know. Let's
10:29
see. Let's see what what we can do with
10:30
the with the pipeline. I can type help.
10:33
It'll tell us, okay, pipelines's got
10:35
these commands on it that you can use.
10:37
help set some plan text and see what the
10:39
plans are on the network. So, right now,
10:42
um I just started this one from scratch
10:44
and I shouldn't have any plans set and
10:47
it'll tell me that. But the really cool
10:49
thing, and you can't see this on my
10:51
screen, but we're going to switch
10:52
screens back and forth in a minute, is
10:54
Brad is running this same pipeline on
10:56
his machine. And so when I ask for
10:59
plans, not only is it going to go fetch
11:02
what my plan is for the day, but it's
11:03
going to ask all the other participants
11:05
in the swarm, of which there's only one
11:07
today, Brad, what their plans are, and
11:10
their agents are going to return it. And
11:11
it's all going to be aggregated here in
11:13
this text window. So, let's see what
11:15
everybody's plans are. I haven't set a
11:18
plan yet, and it knows that. And Brad's
11:20
LLM is starting up, so it can tell me
11:22
what his plans are.
11:24
There it goes. and Brad's plans are
11:28
bisdev calls and make some contacts and
11:30
finish our tokconomics document. So, let
11:34
me set my plan.
11:40
And you can, if you're creative about
11:43
it, use markdown in here.
11:57
All right. So, I've set my plan and and
12:01
it's in the system now. And uh if Brad
12:05
now, if Brad on Brad's machine decides,
12:07
hey, what's Chuck up to? He can do the
12:08
slash plans command on his machine. My
12:11
machine's going to respond. I'm hands
12:12
are off the keyboard and watch the
12:14
blinky lights on the on the right hand
12:16
side of this screen as Brad's request to
12:18
the swarm comes in and my LLM's wake up
12:21
and figure out what I'm working on and
12:22
tell Brad.
12:25
There they go. So, it's offered up what
12:27
my plan is and then Brad returns, here's
12:29
his plan, too. So, anybody that asks
12:31
what a plan is, everybody gets an
12:33
update,
12:34
right? That's awesome. I love this
12:36
because this opens the door to a hundred
12:38
other things you could do with this. any
12:40
any collaborative shared knowledge,
12:42
right? We could be Bitcoin price
12:44
oracles, right? And somebody asks in
12:46
this form, what's your price for Bitcoin
12:47
right now? And you get 10 different
12:49
answers and you can do whatever math you
12:50
want to do to figure out what a fair
12:52
market price is
12:53
for that coin or equity or any other
12:56
asset that you want to work together in
12:58
a swarm to trade.
12:59
Yeah, we could we could also set all
13:02
kinds of keywords in the in the app
13:04
itself to store knowledge and data and
13:06
information, contacts. You could build a
13:09
CRM around this. I mean, there's a
13:10
there's a million things conceptually.
13:12
The storage the storage I mean I think
13:15
we're doing something that I have never
13:17
seen anybody else do with an LLM. And it
13:20
is a disturbing thing to force them into
13:22
doing, but we're actually using we're
13:25
actually using the system prompt in the
13:28
context of these LLMs to be variable
13:31
storage for the rest of the pipeline.
13:33
Yeah.
13:34
What does that mean? Right. So, normally
13:36
your system prompt is setting a
13:38
personality or telling it how to behave
13:40
or what kind of output format that you
13:42
want. But in this case, the system
13:45
prompts are really simple. And I want to
13:46
show you uh I'll show you what the
13:50
system prompt is right now for this LLM
13:54
agent. This is kind of a gross way to
13:55
see it. We'll go look at it in the
13:57
pipeline editor in a second, but in the
13:58
live running agent, you are an agent
14:01
that holds a single piece of data, the
14:02
user's plan. Whenever you see input from
14:04
the user, respond with only the
14:06
following text. And the text is set to
14:08
I'm working on new Kilroy startup code
14:10
th this cool plan app. Well, that's the
14:13
stuff I just entered in the text window,
14:16
right? And the LLM system prompt is set
14:18
to that. So, if I uh do another set
14:22
command.
14:30
All right. and just change my system
14:33
prompt. Now, if you go back and look at
14:36
that uh agent's system prompt now,
14:44
it's the stuff that I just typed.
14:47
Yeah.
14:47
Right. So, on the fly, we are changing
14:50
the configuration of the LLMs based on
14:53
what's going on in the swarm. Right.
14:56
This is a simple example. I type
14:57
something in the screen and it generates
14:59
and sets the system prompt. Uh but then
15:02
later on anybody can ask that LLM
15:04
because we keep no context for this LLM.
15:06
It's a zero contact context uh
15:09
discussion that goes on. I can say
15:11
what's the value of uh Chuck's plan and
15:14
it puts out I am demoing Keroy plans. Um
15:18
Brad said zoom the window there.
15:21
Yeah. Um, yeah. And go ahead. Describe.
15:25
All I was going to say is is that the
15:27
beauty of this is that not only can you
15:30
customize or on the-ly agents andor
15:34
users by entering context or information
15:37
can automatically customize what an LLM
15:40
has stored with it. I mean, an agent has
15:43
stored with it. But we can have agents
15:47
dynamically create more of these agents
15:50
that are storing other types of
15:52
information. So theoretically you could
15:54
take this thing to a level of
15:58
generating functionality that you want
16:00
in a collaborative or productivity or
16:02
project management or CRM app and
16:05
storing it in each of these agents and
16:08
keeping track of it and dividing it down
16:10
to the level of companies or people or
16:12
tasks or whatever. Um and it's pretty
16:15
much the sky's the limit. Another thing
16:17
that um we talked about on the last
16:20
episode that this makes this even more
16:22
dynamic and powerful is
16:26
we're and we're not going to show this
16:27
today, but you can integrate this with a
16:30
Telegram group. You could integrate this
16:32
into as a bot in a Telegram group, into
16:36
Discord, into Slack, into whatever chat
16:40
app you might want to integrate into
16:43
Kilroy so that all of your team can work
16:46
wherever they choose to work. They won't
16:48
have to be in the Kilroy app if you just
16:51
want people to access and send
16:53
information and interact with the agents
16:55
and update data. So,
16:57
I mean, it can listen to your email. It
16:59
could listen to, you know, it can be in
17:01
integrated with any other interface
17:04
format, right? You could have custom web
17:06
pages that talk directly to kill.
17:08
You could have a UI around it that
17:10
populates and says, "Oh, I see a tag A,
17:12
B, and C, so I'm going to put them over
17:14
here." You could have a UI that
17:16
self-generates. Yeah, you could have
17:17
anything. Um, and so it for me that
17:21
actually really solves um kind of one of
17:23
the problems in group work, especially
17:27
in the crypto space where everyone has
17:29
their own tools, their own ways of doing
17:30
things, their own way of tracking their
17:32
tasks, whatever. This allows you to
17:34
interact with Kilroy however you choose
17:37
and to be able to see the information
17:39
you want to see any way you want to see
17:41
it, right? And the engine will just give
17:43
you what you're looking for. So, um,
17:46
that to me is really powerful,
17:47
right? So, in this example, we're using
17:50
uh an LLM to actually store the data and
17:53
return the answer to the swarm of what's
17:55
my plan. I could have some other
17:57
calendaring or to-do list app or
17:59
something else that's integrated into
18:01
Kilway through a different block and
18:03
when that command comes in, you know,
18:05
what's your plan? It goes and hits that
18:06
application instead. This one just
18:08
happened to be all self-contained. But
18:10
let me let me this is kind of a scary
18:12
picture, but it's really not. Let me
18:14
describe what we're doing so people
18:16
understand because
18:18
this idea of using the system context of
18:22
the LLM as variable storage is huge
18:25
because for for decently capable models
18:28
you could give it a JSON object to
18:31
remember, right? And that JSON object
18:33
could have 50 fields in it, right? So,
18:36
you want to keep the you want to keep
18:38
the hands that were dealt and the the
18:40
bets that were made in an online poker
18:42
game. You just got one JSON object that
18:45
the LLM has to manage, right? You want
18:48
keep an order book for 50 people who are
18:50
participating in a swarm and one guy's
18:53
going to keep the master order book.
18:55
It's just one data structure that you
18:57
stash into the context of one LLM and
19:00
everybody can ask for it through the
19:01
swarm. So in in this case, a lot of this
19:05
stuff up here is about processing those
19:06
commands that you saw me type, the help
19:08
command, the set command, and things
19:10
that aren't commands. If you just type
19:12
some gibberish that it doesn't
19:13
understand, it gives you an error
19:14
message. And if we go back over here and
19:16
look, I can do that. If I just type junk
19:18
in here, it says, "Hey, these are the
19:20
commands that you do. Don't type junk."
19:22
And that's this this is just not a
19:25
command path. And so the the premise
19:28
here is that I type some stuff up here
19:29
in the interactive screen and it goes
19:31
into a private swarm where we figure out
19:34
locally what to do with it. Was it a
19:36
command that I can handle locally? Am I
19:38
telling the user some help? Am I giving
19:40
the user uh an updated system prompt or
19:43
is it a message that needs to go out to
19:45
the swarm?
19:47
I.e. go list the plans for everybody out
19:49
there in the swarm and and get their
19:52
results back. That's the plans command
19:55
over here uh slashplans and it goes
19:57
around this way and it it laers that
19:59
into a different command uh slashlist
20:02
plans so that that goes to the swarm and
20:04
there's no confusion. All the listeners
20:06
in the swarm see a list plans command
20:08
come in and they go up and say hey AI
20:12
you just got asked a question. Well,
20:14
this AI system prompt as we saw is
20:17
nothing but you're holding on to a
20:18
single piece of data whenever you see
20:20
input from the user respond with only
20:22
whatever the plan is. Right? And this is
20:24
the default system prompt which says I
20:25
haven't set a plan yet. But if I use the
20:28
set command and follow this path through
20:30
the swarm, we get over here to a place
20:32
where the thing that I typed slash set
20:35
whatever my plan is goes into this box
20:38
called set prompts. And set prompt's job
20:41
is to change the system prompt in
20:44
whatever LLM it's pointed at. And it's
20:46
going to change the system prompt to the
20:48
same system prompt that was there
20:49
before. You're an agent that holds a
20:50
single piece of data. That's the user's
20:52
plan. Blah blah blah. And here's the
20:54
data that they just said that is their
20:56
new plan. Pass that through to the
21:00
the LLM agent that's here. And what
21:03
happens is that replaces this value with
21:05
what came in over the red wire. and and
21:10
now the next time somebody asks for my
21:12
plan in the swarm, they're going to get
21:13
the updated plan. So, this is a really
21:17
twisted application of of LLMs in in so
21:22
far as it doesn't keep anything but one
21:24
piece of context data and that context
21:26
data is whatever the current value is
21:28
for the variable that it's responsible
21:29
for. Other agents can ask for that data,
21:32
other users can ask for that data. The
21:34
data can be set, it can be changed, it
21:36
can be deleted. It acts just like a
21:38
variable in a programming language. And
21:39
and we said this before, Brad,
21:42
this is a terrain programming language
21:44
with this weird picture stuff because we
21:46
have conditions and branches and we have
21:48
loops and we have
21:50
halting conditions and we have the
21:51
ability to change variables.
21:54
So, so you any literally anything that
21:56
you could come up with in any other
21:58
programming language you can do in here
22:01
given the limits of these gray these
22:03
gray blocks. These are the external
22:05
commands of things that kill can do for
22:07
you. In the case of this particular
22:09
application, all of these boxes are just
22:12
processing text streams that are flowing
22:15
between the swarms, right? They're
22:17
regular expression matches. They are
22:19
change the format of some text. They're
22:21
put out constant text. Like the help the
22:23
help command is is really tough. No
22:26
matter what it gets, if it's e/help, it
22:28
puts out a message that says your valid
22:30
commands are this, right? So you can
22:32
have hardwired text that goes in as
22:34
prompt parameterized text. There's a lot
22:36
of different a lot of different ways to
22:38
to use it. But the point is in 30
22:41
minutes this morning we had
22:43
collaborative workspace plan sharing.
22:46
Yeah. And the I mean the beauty of this
22:48
is that you could create your you could
22:50
create a private swarm that's for all of
22:52
your tasks and your schedules, right?
22:55
And then if you tag, you could set this
22:57
up so that the swarms are talking to
22:59
each other and you could tag specific
23:01
tasks as today and then have the ability
23:03
to read them into the plans and automate
23:06
from your task list in your schedule
23:08
what's in the plans for everyone else to
23:10
see. Right? So I mean there's there's a
23:13
million ways to use these.
23:15
Well, and there's a block over here that
23:17
we haven't shown that's really not very
23:20
exciting when you look at it. It's
23:21
called opener. But what that does is
23:24
open deep link URLs in your local
23:27
operating system. Which means these LLMs
23:30
can send messages to applications like
23:32
your calendaring app or your desktop
23:35
instant messaging app or your web
23:37
browser or talk to any application that
23:40
has
23:41
scriptable capabilities depending on
23:44
your OS. For instance, in the in uh Mac
23:47
OS, it's very very easy to use this
23:49
block to run shortcuts.
23:51
So you can have shortcuts that do, you
23:54
know, image cropping and rotating or,
23:57
you know, turning on lights in your
23:59
house or whatever. And these swarms can
24:02
talk to it and get results back because
24:04
there's a way for the things that you
24:06
deep link to in the operating system to
24:08
talk back to Kilroy with their results.
24:11
So
24:12
yeah, it's fantastic. I I haven't done
24:14
enough with uh opener yet. I need to
24:16
play with that some more because that's
24:18
uh opens a whole lot of possibilities.
24:21
Oh, that's so bad. Um well, anyway,
24:24
that's what we made today. And um we
24:27
really want other people to be able to
24:29
make these sorts of things, too. So, you
24:31
know, paid Kilroy advertisement. If you
24:33
want to play with this thing, get
24:35
involved, be an early stage developer in
24:37
it, participate in this project before
24:40
it blows up, send Brad a DM on Twitter.
24:44
Yeah, you guys can DM me at B5 Crypto on
24:46
Twitter, B05 on Farcaster, B05 Crypto on
24:50
Telegram. Um, we're looking for folks
24:53
that are doing research and want to work
24:55
in it. We're looking for folks that just
24:57
want to build things and try it. Um, and
24:59
I can tell you that um, the folks who
25:02
come in early
25:04
um, are going to see um, potential
25:06
upside um, from being an early um, a
25:12
contributor an early contributor to the
25:14
Kilroy ecosystem. Um, building
25:17
pipelines, doing referrals, helping us
25:19
with marketing, um, coming up with
25:22
modules in a chat room, right? Yep.
25:25
Helping other people.
25:26
Yeah. participating, helping, writing up
25:29
documentation, tutorials, whatever you
25:31
think you could bring to the table, we
25:33
probably need it. But what we need most
25:35
is people using the application and um
25:38
building in it and running into the
25:40
things that we haven't thought of yet.
25:42
Um and and that's really where we are
25:44
now. And we also need developers that
25:46
want to build integrations and modules.
25:47
If you have a D5 protocol, you have a
25:49
Farcaster app, um we would love to
25:52
integrate those. and we're very
25:54
interested in being tightly integrated
25:56
with um the Farcaster community as well.
25:58
So um please do not hesitate to reach
26:01
out to me. B05 on Farcaster and B05
26:05
Crypto everywhere else. Yeah, that's
26:07
great. Anything else, Chuck?
26:09
Well, if they can't get you, they could
26:11
get me at Cshot and just about
26:13
everywhere else. Um
26:15
but no, I think that's it for today. I'm
26:18
going to go play with this thing some
26:19
more.
26:20
Beautiful. Well, thanks, man. I love
26:21
this thing. I'm going to play with it,
26:22
too. um and probably be distracted and
26:25
not get anything on my plan done.
26:27
No, no, you as according to your plan,
26:28
you have some calls to me.
26:30
Yeah, exactly.
26:32
All right. Thanks, man. Uh everybody,
26:34
have a great day. Thanks for paying
26:35
attention. Thanks for watching. And uh
26:36
if you have any thoughts, feedback,
26:38
ideas, or you want to participate,
26:39
please don't hesitate to reach out to
26:41
me. Thanks.
26:42
Later.
26:43
Bye. Bye.
ROTP: How to easily build agentic swarms in Kilroy Part 2

Realm of the Possible
4 subscribers

Subscribe

0


Share

Save

Clip

5 views  Streamed live on Oct 31, 2025
Last week we covered the basics of building AI swarms. This week we get more detailed and functional and give an example of building swarms that can make you more productive with your team. 

Learn how you can be empowered to build anything with a powerful decentralized automation and AI platform. 

We've created the most powerful decentralized AI and crypto automation/integration platform while being completely decentralized. It comes complete with no configuration peer-to-peer/swarming capabilities, the ability to easily build integrations with any other compute, and a Turing complete visual creation tool anyone can use to build functionality in the network while giving agents on Kilroy the ability to build and deploy on their own.
Transcript
Follow along using the transcript.


Show transcript

Realm of the Possible
4 subscribers
Videos
About

All

Computers

For you

11:03
THIS is the REAL DEAL 🤯 for local LLMs
Alex Ziskind
406K views
•
3 months ago


18:57
The Infinite Software Crisis – Jake Nations, Netflix
AI Engineer
117K views
•
9 days ago


21:14
Why I Left Quantum Computing Research
Looking Glass Universe
1M views
•
6 months ago


18:00
The Hottest Thing Anyone Has Ever Said to Me | The Hustler
Kelsey Cook
2.8M views
•
1 year ago


5:52
I Made The Smallest (And Dumbest) LLM
Codeically
394K views
•
3 months ago


23:30
Decrypting SSL Traffic from a Chinese Security Camera - Hacking the Anran IP Camera
Matt Brown
81K views
•
10 days ago

10:02
Can Magnus Carlsen Beat a Noob with 30 Queens?
Esports Spotlight
929K views
•
1 month ago

14:19
What Sam Altman Doesn't Want You To Know
More Perfect Union
2.3M views
•
10 days ago
Fundraiser

30:59
I Gave HACKED SCAMMERS a PANIC ATTACK On CCTV!
Scam Sandwich
10M views
•
9 months ago

13:24
LAWYER: How Cops Use Apple's NEW Tech to Spy on Your Phone
Hampton Law
381K views
•
8 days ago

20:16
One Formula That Demystifies 3D Graphics
Tsoding
285K views
•
4 days ago
New

14:31
How To Run Private & Uncensored LLMs Offline | Dolphin Llama 3
Global Science Network
832K views
•
10 months ago

17:35
The Future of Veritasium
Veritasium
3.4M views
•
5 days ago
New

15:25
You Were The Smart Kid... So What Went Wrong?
Mark Manson
374K views
•
8 days ago

17:54
I Quit Every Streaming Service… Here’s What I Use Now.
Switch and Click
1.6M views
•
1 month ago

11:05
Will Ferrell Refuses To Discuss The Bird On His Shoulder | CONAN on TBS
Team Coco
1.4M views
•
3 months ago

22:47
THE most advanced city in the world?! How? Let me show you!
Heideexyz
1.2M views
•
3 weeks ago

5:25
New Barista Training - SNL
Saturday Night Live
7.1M views
•
11 months ago

11:52
I shrunk down into an M5 chip
Marques Brownlee
 and Epic Spaceman
2.1M views
•
3 days ago
New

32:32
The Strange Math That Predicts (Almost) Anything
Veritasium
10M views
•
5 months ago


Show more
    


NL

Skip navigation
Search



Create


Avatar image
Transcript


Search in video
0:00
Yeah, here we are. What's up, buddy? Well, long time no see.
0:09
Let's just hope my internet connection stays up today. I will be buying redundancy. Um, all right. So, we're uh
0:17
we're we're doing this show today because we wanted to introduce folks to a little bit of the philosophy behind
0:22
Kilroy as well as a little bit of what's possible um in building in Kilroy, at
0:29
least as it relates to AI. Um but why don't you quickly um tell a little bit
0:36
about uh what you think the best way to describe Kilroy is? I knew you were going to do this to me.
0:42
Um so, I'll give you my stock answer to when somebody asks me what is it and
0:48
then I'll tell you really what we're doing. So it's it's the elephant with the three blind men, right? One guy feels a rope, one guy feels a tree
0:55
trunk, and another guy thinks he's up against a barn wall because he can't really see the whole thing. And it's a big animal. So we've got this big
1:01
animal, but what it fundamentally is is a way for you to get rid of your dependence on centralized stuff and
1:09
build an internet the way that you want to do it. Right? So, it's an application that runs on your desktop and allows you
1:16
to pick what you see, who you talk to, how you interact with external services
1:21
down to even getting rid of the user interface on a centralized website or not having to talk to a big centralized
1:28
AI like chat GBT or cloud or uh being able to put together your own services
1:33
and monetize them for other people. So, it's all about really taking the
1:39
centralized internet that we've had enforced on us for the last 20 years and getting back to what it used to be,
1:45
which was everybody's on their own platform. I don't know. How was that? That was good. That was good. I mean, I
1:51
I I think it goes way beyond that, but I think that's a core
1:56
uh function of what it does. Absolutely. So yeah, look, I mean, I think um I I
2:02
think the decentralization piece is really critical. I think we've kind of identified the dangers of
2:08
decentralized money pretty efficiently in this space, but we don't have decentralized user interfaces to crypto
2:15
and um the dangers of of centralized AI to me are an even greater threat to
2:21
society. So um I think this is really important as well. And Kilroyy's peer-to-peer and swarming capability
2:27
makes it so like you said, anybody can build their own internet. But built into
2:32
that platform is automation and integration and agentic AI with the
2:39
visual creation platform um that is also turning complete. So um I think there's
2:46
a lot for people to understand about that. I I think one one point that raced
2:51
by there that it's worth going back to and talking about is the centralized AI piece. There are actually some things
2:57
that you can do on your desktop with Kilroy and a whole swarm of AI agents
3:03
that you couldn't begin to do with a web page talking to chat GBT for example. They're just things that you are not
3:09
able to build or interact with as a normal enduser. maybe as a, you know, a
3:15
computer scientist with, you know, ability to sit down and hack out your own code and glue the pieces together
3:21
and use some of the, uh, sort of janky offtheshelf open source pieces, you
3:27
could make it do the same thing. But for a normal user who wants to sit down and be a power user of AI, there's it's
3:34
night and day with Kilroy. I mean, there's things that you can build on your desktop that you could never try to do in a centralized service.
3:41
Yeah, absolutely. and and there are a ton of agent platforms out there and a
3:48
ton of um attempts at making AI more accessible and more powerful. But at the
3:56
end of the day, the ability for our system to not only be easy to build in,
4:01
but also to be able to connect and utilize resources is really powerful. For example,
4:07
um whether centralized AI or local AI, um the ability to easily share LLMs with
4:15
other people doesn't really exist in this world. There are blockchain solutions like AOS that that allow you
4:22
to to utilize uh LLM and servers that people make available and there are
4:28
obviously LLMs you can install on your machine. But with our swarming capability, it makes it so that you can
4:34
utilize LLMs on server infrastructure without jumping through a ton of hoops. It's just the swarmed connections are
4:40
together. I'm sorry. Go ahead. No, go ahead. I was going to say, let's talk a little bit because we keep saying this word
4:47
swarm and this peer-to-peer stuff, but we haven't really described what that means. Sure. Right. So, in a normal interaction with
4:55
an AI like chat GPT, you're using a web browser as a client to talk to a server
5:01
somewhere off in the cloud that's being run by OpenAI, right? And everything that you type on your screen is getting
5:06
sent to that server and it does some work and it sends you the answer back. And you can open two windows and talk to
5:13
two different agents, but that's sort of the amount of of interactivity that you get. Um, since Kilroy runs on your
5:20
desktop, Kilroy can talk to OpenAI servers, it can talk to Google servers,
5:25
it can talk to Microsoft servers, but it most importantly can talk to uh AI
5:32
execution platforms that might be running locally on your own machine or on an old laptop on your desk or on a
5:38
friend's computer, right? And the way that we do that is we have built into Kilroy this sort of seamless,
5:44
transparent, completely userfriendly way to hook any number of machines
5:51
together in a peer-to-peer cloud. If you've ever played with something like Bit Torrent or back in the day Napster
5:57
where there was hundreds of computers that were all collect connected together uh pushing information around between
6:03
them, um that's exactly what Kilroy does behind the scenes. If I'm in a a a swarm
6:09
called, you know, my spiffy AI, and I give Brad that name, Brad can join the same swarm, my spiffy AI, and now his
6:16
agents can talk to my agents and vice versa. We can set up chat rooms. We can build applications on top of this shared
6:22
infrastructure. And Kilroy handles all of the behind-the-scenes networking. You don't have to play with your firewall.
6:28
You don't have to set up port forwarding. You don't have to have a static IP address. You don't have to use a domain name that you've got to get
6:34
configured. they just talk. And so that actually becomes the mechanism that we
6:40
use inside of Kilroy for everything to talk amongst itself. Now Kilroy looks sort of like a a highlevel operating
6:48
system to all these agents. Kilroy can start and stop the agents. It can set up communications pathways between them. It
6:54
can run things on a periodic basis. uh it can integrate to the outside world with connections out to anything from as
7:02
simple as a chat service like telegram to a big backend database somewhere or to another AI in the cloud. So the the
7:10
whole point of the platform is let's get rid of all the hard stuff that makes
7:16
AI interacting with each other and with the real world painful. all the networking, all the configuration, all
7:21
the tool configuration, all the parameters that have to be passed back and forth. All that stuff is hidden and
7:27
instead you just get some nice little blocks that you can drag around on the screen and hook up. That's perfect. I love that. I love
7:34
that. That's a great explanation. That's excellent. So maybe maybe we should look at some
7:39
examples. What do you think? Good. Yeah, let's do it. Um, unfortunately, it looks like Pineree is
7:46
not working for us. It's a bummer. So, we're not getting Farcaster folks.
7:51
Nope. I'll just have to upload it. Okay. Well, they'll just have to wonder what they're missing. You know, I I
7:58
think while you're do getting that loaded up, I'm going to talk a little bit about kind of philosophically why, you know, um I I think one of the things
8:05
that for people to understand that's important for people to understand about why we are so concerned about
8:10
centralized AI providers like OpenAI, Google, Microsoft, whomever um is that
8:17
we believe that all of compute will eventually be AI or most of it. And so
8:23
what that what that results in is that every aspect of your life is then going
8:28
into whatever AI provider you choose whether it's your local provider you or it's a it's a service a centralized
8:35
service like open AI that also means that all the information all the news all the education that your children may
8:42
be getting also is going to be coming through these AI providers and at some
8:47
point you have to ask yourself who controls what is or is not in those
8:53
models and who controls what comes in and comes through and what we see and what we interact with and who controls
8:59
access to what you are putting into it and so that's one of the biggest concerns we have and if and you know I
9:05
firmly believe that if you think that decentralized money is dangerous um centralized AI is you know every
9:12
dystopian movie and book that's ever been made about centralized money right sorry centralized money um every
9:19
dystopian book or movie about AI and some horrible corporate o overlord or
9:25
government overlord um over controlling your life um only comes through
9:31
centralized AI. And so from from my perspective that's a really critical
9:36
thing and it's really in my mind more critical and dangerous than centralized money. So
9:42
now that you've scared everybody there you go. Show show them how not to be scared.
9:48
There you go. Excellent. empower the individual. All right. So, what you're looking at right now is my desktop and
9:54
everything that you're going to see is running locally on my Mac Studio. The the Kilroy application, the Kilroy user
10:02
interface, the connection to a local uh AI platform. All the stuff that you're
10:08
going to see initially is all locally locally provided um on one machine. Now, I've got other machines here that are
10:14
running other copies of Kilroy and doing things, and we've got some stuff that we're going to hook up to in the cloud, but for the most part, everything you're
10:20
going to see for the next few minutes is all on my machine. And it's not even a very beefy Mac Studio. I think just
10:26
about any normally modern, reasonably complete
10:31
CPU stack, whether it's Windows or PC, Windows, Mac or Linux, should be able to
10:36
do what you're going to see uh for these little dogging and pony shows here. Um, all right. So, Kilroy is, like I said,
10:43
sort of an operating system for little applications. And if I go look in the sidebar here on Kilroy, we have lots of
10:50
little applications that are running and doing things. uh things that are related to AI, things that are related to crypto
10:57
exchanges, RSS feeds, DeFi services like Vei, uh interacting with web browsers
11:03
and NFTTS, automated trading platforms, RSS reader down here called Miniflux,
11:09
some decentralized AI for the old Spirit Swap project on Phantom, weather services interacting with Twitter, uh
11:15
embedded web widgets. There's all sorts of stuff that you can do that we've already built that you can drag into
11:20
your copy of Kilroy and use. But what we want to start with uh because it's one
11:26
of the cooler toys that we have is the the AI platform. So I'm going to open up the editor that's built inside of this
11:33
uh Kilroy AI application. And it's a drag and drop editor, big white expanse
11:39
of of nothingness. And so let's start with something that's uh pretty simple
11:44
um which is just a little user interface to talk to Oops, did that wrong. A
11:51
little user interface to talk to um we don't want to do this one.
12:01
I have Llama 3 running on my desktop as a local LLM
12:07
and the may want to well I guess most people that watching would know what Ola is but
12:13
we'll we'll talk about it for a second and how Olama and Llama 3 work while I go find where I put my pipeline folder.
12:19
So Olama is an inferencing engine, an inferencing platform. Um, it's an open-
12:24
source platform and it's one we're currently using for model inferencing. And what that means is when you send
12:32
info to an AI, you're sending it to an inferencing engine that's interacting with the model. Um, that's a really
12:39
simple way to explain it. And OAMA can run locally on your own machine. So even if you don't have a beefy GPUbased
12:46
machine, there are a ton of small models that you can utilize um to utilize some
12:53
functionality of AI via Olama. There are a ton of others, LM Studio, um other
12:58
products that um allow you to do this as well. We like we like the way it functions. Um but the one of the things
13:05
in terms of small um LLMs that's uh will be important in the future for Kilroy is
13:10
is that because we're able to to distill the functionality of agents into small
13:17
tiny bitsized pieces in these um peer-to-peer networks and swarms, we actually have the ability to offload
13:24
or to put a lot less um work on the LLM in any given task that it's doing. so
13:32
that we won't need as much processing power. And Chuck's favorite LLM is a
13:37
really old and really small one called Llama 3, which by by Meta by the Facebook people. Um, and he uses that
13:44
for everything. And it's a small LLM that will run on a lot of different machines, and it's actually not even the
13:51
most up-to-date LLM that Meta has, but it does a really good job. It doesn't hallucinate, and it knows how
13:57
to do math. Those are two things that are majorly in its favor. And if you've used any of the modern newer LLMs that
14:04
are online, ChatGpt or Claude or any of those, they go to great lengths to explain to you what you're doing. And
14:10
for most of the tasks that we're doing, we know what we're asking the LLM to do. We don't need an explanation. We just
14:16
need an answer. So you might say, "Hey, go get me a quote for a particular cryptocurrency." It doesn't need to come
14:22
back and give you an explanation of what a cryptocurrency is and five paragraphs later show you what the current price is. We just want to see the current
14:28
price, right? or it might be, hey, you know, go add these two numbers together and if it's bigger than a certain amount, go do the next thing. the the
14:35
the difference and I think it's kind of it's hard to see until you see some of the agents that we're going to show in
14:40
just a minute or some of the pipelines is that what would normally be two or
14:46
three pages worth of prompts into a chat GPT like web interface to try to get it
14:51
to go through multiple steps and do what you want are all one or two sentence prompts that are really small that let
14:57
these little local LLMs do one little job and then hand off their work product through the swarm to the next one does
15:04
the next step in the job. And sometimes there may be tools or transformations in the middle of those steps, which is not
15:11
something that you can do with the big centralized models very easily. Do some work, transform it with a tool, do some
15:17
more work, send it off to an a cloud service, get an answer back, do some more work. That that pipeline style
15:23
workflow is is very challenging to do as an enduser with the existing consumer
15:29
focused interfaces. So, all right. So, let's switch back to what's on the screen now. Now that I found the folder with the example in it that I wanted. So
15:36
this is the visual editor for doing AI pipelines in on top of Kilroy. Now
15:43
remember just a second ago when I showed you all the apps that were running in the sidebar of Kilroy. This is just an
15:48
app on Kilroy. This editor, the building of the agents, the execution of the agents and letting them talk through the
15:54
swarms is all just what's the equivalent of a thirdparty app on top of the Kilroy platform. So if you don't like this, you
16:02
can build your own. If you want to change the way it behaves, you can modify it. But for right now, what we have is a pallet full of some pre-built
16:09
parts. Um, we have agents, AI agents like this, this one here, which are
16:15
frontends to talk to an external AI. It can be an external AI like chat GPT. It
16:21
can be a local AI like Llama 3 running underneath O Lama on your own local
16:26
machine. Whatever you decide to configure in these parameters for the URL and the API key and the model name
16:32
and the temperatures and other stuff like that is what the pipeline's going to be started with. And then we have
16:38
other things for interacting with the pipelines. We have chat agent which is basically a user input screen. It looks
16:44
like a terminal window. It looks like the web interface that you use to talk to uh a chat GPT. And then in the middle
16:51
of the two is a swarm. This one is just creatively named swarm. And its whole
16:57
job is to take messages from the user input agent and pass them over to the other things in the swarm that are
17:03
listening. In this case, there's only one. It's this AI agent over here. And what that's really going to do is whatever I type on the screen is going
17:10
to go to the swarm and from the swarm over to the agent. It's going to look like a prompt string. It's going to do its work. It's going to give me its
17:15
answer back to the swarm and I'm going to listen to it and see it on the screen. These yellow labels on the edges
17:22
are basically saying who am I listening to or who am I sending as? I'm sending as a thing called chat agent. The AI
17:28
agent's listening to everything. Don't care. The AI agent is sending as a thing called AI agent. The user input field is
17:36
is just listening to everything. So let's go make that so you can see what it looks like in Kilroy and run it so
17:42
you can see how it interacts with the LOM in the background. So, we're going to build that pipeline and it's going to pop open some windows
17:48
in Kilroy really quickly. This is the user interface for the interactive portion of the screen. And this is sort
17:56
of a little debugging window to show you that the agent's really doing work in the background when it talks to the LLM.
18:02
You can hide this, show it. It doesn't matter if it's on the screen. Normally, a user would just see an interactive
18:07
window, but um I'll just ask it who is this. Now, it's
18:13
going to take a while for it to start up the LLM because we've been talking and it's gone to sleep. But, okay, here it
18:18
is. It's llama, right? This is who we're talking to. And,
18:30
you know, tell me about chocolate cake. Right. So, that's the typical interaction that you would get with the
18:37
the LLM. And yes, it's barfed out a a recipe for a chocolate cake. Here we go.
18:43
Um, and so in a couple of mouse clicks and drags in the editor, we built sort
18:48
of the equivalent of a local chat GPT. I've got a window that I can type in prompts go over to the AI and I get
18:54
answers back. Um, in some more elaborate examples, you can have you can ask it to create code and show it in another
19:00
window where it's running. So you can do vibe coding on top of Kilroy. Um, but this is a this is a the simple example
19:08
of just one little uh interactive agent. What do you think about showing the
19:13
integration with um Telegram, Brad? I can show. Yeah. Yeah, let's do that. simple one that we've got right now that I use
19:19
every day is I have a um an RSS feed
19:25
reader called Miniflux that sits out on a server for me and goes through about a
19:31
hundred different RSS feeds in real time all the time and and so its job is to
19:38
write into this swarm called Chuck Minifilelex swarm. The server out there in the cloud also knows how to talk to
19:45
Kilroy swarms and it pushes the data in through a web hook that Kilroy makes available to it. And there's a swarm
19:52
that's receiving a constant stream of RSS stories, right? Those feed into an
19:57
AI agent whose prompt is this. So system prompt says
20:03
you're an agent that transforms RSS stories into markdown messages for Telegram. and it goes into some more detail about how to construct the
20:10
telegram uh markdown message. But if you look at the end result, what I end up
20:15
with is uh a an unending stream of
20:21
scrolling RSS messages showing up in Telegram, which means even when I'm away from my desktop, I can pull out my
20:27
phone. I can look at the current news that has been pulled in from the RSS feeds and pushed over to Telegram by my
20:34
agents on my desktop. I'm not looking at some thirdparty news feed. I'm not looking at some uh you know online news
20:41
site like a you know a CNN or a Reuters or whatever. This is all my news being
20:47
picked out and selected and summarized by my AI and pushed out to my private Telegram channel.
20:52
And you can you can take it even further if you want. For example, my version of that um pulls in a bunch of feeds from a
20:58
different lot of different places and then I actually have it summarized into tweet size stories. So, it goes in, it
21:05
reads the article, it summarizes it, and then I get a feed of of tweetsiz
21:11
messages with the links in it and an engaging headline so that if I want to
21:16
repost something and comment on it, I can. So, you can take this any even further. You could have another agent in
21:22
the swarm that's actually um discerning which feeds, which stories, you know,
21:28
have some component to them that you want to be emphasized and have only those come in. So, um, there's a million
21:35
ways to expand on this, um, and interact with it. So, so let's look at let's look at one
21:40
that's a little bit more elaborate. So, we're working our way up in complexity, right? We have an RSS data coming into
21:46
an agent. The agent summarizes it as a markdown message for Telegram. Telegram shows it on the screen. And, oh yeah, as
21:52
a side effect, I can also look at it inside of Kilroy in a right in a terminal window inside of Kilroy. Um,
21:58
all of this runs over on a laptop on my desktop whose job right now is to do nothing but be my my news feed. Um, so
22:05
here's one that's more complicated. I've shown it three times now because I keep clicking the wrong tab. This one is a a
22:13
fully functional Telegram bot, right? This is uh an agent that runs in
22:20
Telegram uh a Telegram bot that talks to Kilroy in real time, sends messages from the
22:26
users in Telegram into Kilroy. It's processed through these two blue AI boxes here. Uh and the results are then
22:34
sent back to uh to Telegram for the user. Now the this is this is a kind of
22:41
an advanced leapt to the endgame here on Kilroyy's AI functionality, but this is
22:47
a really advanced model because it's got the ability for the user to change on
22:52
the fly what the system prompts are and change the behavior of the LLM using telegram bot commands. So instead of
23:00
just having one persona or one system prompt and that's how this is going to work you can see that there are several
23:06
commands that are supported and if you look closely there there's a command called uh slashpersona there's one for
23:13
uh slashhelp and there's one for slashreset and those are all hooked into this big swarm of nodes to to do the
23:21
things that you would expect. So instead of instead of uh looking at that and trying to decompose it, let me just show
23:27
you how it works. Uh who am I talking to?
23:35
So it's going to go over and uh I guess I'm I'm I'm in pirate persona. I
23:41
didn't realize that. So the Let me let me uh let me reset that again and let
23:48
you see what's really going on. We'll uh hide these guys and hide this guy
23:55
and bring back the techbot and show you. This one has lots
24:01
of pieces and out of the box it just has a generic persona, but um because I was
24:08
playing with this before we got on the call, it had switched into
24:14
cap sea captain mode. And um so if we just ask it well what are the things
24:20
that I can do right there's the normal response that you get and you can see in the
24:26
background here in Kilroy with all these other windows open you can watch these lights these blinky lights go off which
24:32
will basically give you some indication of the work that's getting done by the various uh agents along in the pipeline.
24:37
I'll shrink this down a little bit so we can see them all. But if you built a pipeline to send out or distribute to
24:42
other people, which is something you can do on our network, um then you can make it so they don't see any of that. They
24:48
just see the the primary interface or see nothing and only see it in Telegram.
24:54
So you can ask it knows about itself and it knows its own command. So I can ask it uh
25:02
things about how do I do different stuff. But if I just tell it the persona is uh Karen
25:08
and uh have it switch over to that. So now Karen persona is active. You lose
25:15
her. Let me reset it so it forgets the captain stuff and um you need to leave
25:23
our store immediately. And of course she's going to yell back.
25:29
Right. So wi with just commands and telegram I've completely reconfigured
25:34
the pipeline to have a different system prompt a different persona to change uh who I'm talking to and it didn't change
25:41
the pipeline right we didn't have to change the topology we changed the configuration of the nodes in the
25:47
pipeline that's what these red lines indicate and in a future podcast we'll go into more details about how to build
25:53
something like this but basically the flow is message comes in from telegram is it a command if it's a command go
25:59
down here to the command process processor and decide what to do. Was it the reset command? Send a reset command off to the AI. Was it a help command?
26:06
Then ask for help and put that back out to the user. If it was one of the persona commands, it comes in up here
26:12
and it changes to either the pirate, the baby, or the Karen and changes the system prompt in here. And if we uh if
26:19
you looked at the system prompt in the thing as a as it's running right now, which we can do, we can go back over
26:25
here real quick and ask it to show us the system prompt,
26:30
which says, "You're an angry angry middle-aged woman who is never happy and always wants to speak to the manager, always respond to the user in
26:35
character." If I change the if I go back to Telegram and change it to be uh
26:43
the baby persona, right? And then we go back up here and
26:50
and look at the setting for the system prompt. Now it's you're a baby, always respond to the user and character. So
26:57
behind the scenes, the commands from Telegram are reconfiguring Kilroy on the fly. This could be commands to start up
27:03
new agents. Kilroy can self-modify these pipelines. It can add in new agents. It can change the cloud the swarms that
27:09
it's talking to. It can change the system prompts. It can rewire itself on the fly based on user input or input
27:16
from the outside world. Um really and truly it's sort of Skynet in a box, right, Brad?
27:21
Yeah, absolutely. I mean, I think that's a really important thing to to emphasize here is that um we're at the point with
27:28
this ability to create these pipelines where um a user can um
27:36
we you'd be able to you'd be able to structure these things so that when an agent needs more capability, it can
27:42
actually create that capability, right? So, well, and and the really cool thing is this doesn't have to all be running on
27:48
your machine, right? If you look back at this diagram, all of these different swarms, any one of these swarms could be
27:55
shared with other Kilroy users out in the wild internet and their agents could
28:01
respond, right? So, I could have a a bot that interacts with a bot that's running on Brad's desktop and gives me some
28:08
answers back and we literally don't have to do anything but coordinate on what is
28:14
the name of the swarm that we're going to use to talk through. You could have a 100 person chat room built on top of
28:19
this with uh you know AI trading strategies for cryptocurrencies that you are watching with your crypto bros,
28:27
right? And the agents can be working together and collaborating to refine and improve different trading strategies.
28:34
You could have trading contests where agents are trading against each other and whatever one wins, you know, people
28:40
can access and utilize for their own trading strategies. Um this can be on centralized exchanges, this can be in
28:46
DeFi. Um and the key the other thing I wanted to to point out to everybody is is that the the integration with
28:53
Telegram is not unique. The this interaction and integration could happen
28:58
in Telegram. It can happen in Discord. It could happen in text messages. There
29:03
there really is no working in Minecraft, right? Yeah, we have it working in Minecraft, right? So, we built a plugin of Kilroy
29:10
into Minecraft and you can actually interact and control the Minecraft
29:15
environment via Kilroy. So, you could then have AI agents that are building in
29:20
Minecraft. You can have all kinds of things happening. Um, the key is is that the interface is irrelevant and so it
29:27
can go to wherever a user wants to live. It doesn't have to live within the Kilroy interface. It doesn't have to
29:34
live within Telegram. It can be whatever your company or your team or your group
29:39
wants to utilize as a UI. And there's nothing stopping you from creating brand
29:45
new UIs that function and live inside of Kilroy. And there's nothing stopping you from integrating any API that exists in
29:52
the world into Kilroy. Um, if there's a web hook, if there's a way to interconnect to it, you can build a
29:59
module within Kilroy and you can now make it available to agents. So maybe you have a company that has services
30:05
that you want people to be able to utilize to order pizzas all over the world. Um, you can do that. You may have
30:11
games that you want people to be able to interact with in a swarm and have thousands of players automatically
30:16
playing and competing against each other. That there really is no limit to it. And that's actually one of our
30:22
biggest problems with talking about Kilroy is well and I think Brad this the the the the key thing to take away from this is
30:29
that this is a completely novel way to put together LLMs and agents and
30:35
cooperating human users with the real world services. Nobody has done this
30:42
particular swarm-based peer-to-peer completely decentralized AI model. Yeah,
30:48
you can call tools from chat GBT and you can have MCP as your tool calling interface to get to all sorts of
30:55
services and you pretty much have to be an expert programmer to get all the interfaces lined up and the glue code
31:01
written and the pipeline built and the pipeline's very static and it does what it does. In Kilroy, any user that can
31:07
draw a picture like this by dragging blocks onto the screen and connecting the lines together can make a pipeline
31:13
that does more than you can do with the current tooling that's sort of state of the practice out there in the internet.
31:19
Yeah. And I think let's carry that over into crypto, you know, kind of our originating world, right? the the really
31:27
cool one of the really powerful and the things that attracted me to crypto from the beginning was understanding
31:33
composability, understanding the ability for components of financial services and
31:38
and crypto tools to be made accessible and ava a available to other protocols,
31:44
right? So, a lending protocol might utilize a swapping protocol within their interface or whatever. But what but that
31:52
make requires you to be a solidity programmer, right? But as we drop in modules um for DeFi protocols into this
32:00
platform, now users in this AI interface or in the regular app interface can make
32:06
use of those to build new agents, new applications, new pipelines that make
32:11
use of all kinds of protocols in DeFi onchain. The other thing that's that
32:17
this solves for crypto is one of the biggest holes we have in in the goal of
32:23
decentralizing money is that all of our interfaces in DeFi are on the web. And
32:28
so there is a centralized server and there is a registered domain that are susceptible to attack. And that has
32:35
happened to the tunes of millions and millions of dollars being stolen from domains getting hacked and being taken
32:41
over um and to servers being hacked. This eliminates that problem because you
32:46
can now interact directly with a smart contract on a decentralized UI and build
32:52
a UI that's easy for a user to use without them having to visit a website ever to interact with a protocol. And
33:00
now you can have these interconnected protocols within Kilroy that allow you to do all kinds of really powerful
33:06
things um and and make it work. And and as a one more side note, um one of the
33:11
things built into Kilroy is um and the reason we're probably going to have our own um layer two or three for Kilroy is
33:19
that the apps can be stored as NFTts. And so you can actually distribute
33:25
applications in this network and licenses in this network um in NFTTS and
33:31
enable people to easily use the app store that way and access um apps
33:36
because all of the apps in in Kilroy are essentially text files. They're JSON. So
33:43
the engine recognizes and understands the commands that drive it from a JSON file, but we don't require this this
33:50
huge um installation paradigm that we're all used to on our applications on our
33:56
on our computers. There's no executable to install. Um these are really simple
34:01
and easy and fast to install applications. So all right, we've used up our 30
34:07
minutes. All right. All right. So, um, if you happen to watch this, uh, I don't think
34:13
we're going to have a lot today, but if you happen to and you're someone that doesn't mind getting your hands a little
34:19
dirty, we're looking for folks that want to experiment with Kilroy, try it out,
34:25
install it on their machines, and help us knock off some of the rough edges, build integrations into the platform,
34:32
um, um, and build start building pipelines and agents that other people can
34:38
utilize. Um there will be a revenue stream available for for these um that that um will be a part of the entire
34:45
Kilroy platform, but we're really looking for people that can see the power of what this can do and are
34:51
interested in helping to uh build it out further. And there's a lot of opportunities within what we're trying
34:57
to build and where we're going with this thing that if you're interested, um DMs are available on Farcaster, on X, um
35:05
Telegram. I'm B 05 crypto. Chuck, you're cshoten correct on all those platforms.
35:11
Um, yep. I'm B05 on Farcaster, Bz Crypto on everything else. So, if you're
35:17
interested, you'd like to play with this, you want to be an early adopter, um, you want to learn more about it,
35:22
please don't hesitate to reach out. We would love to add more people utilizing this.
35:28
And tune in next time for some details on how you really bank make these things rather than just looking at scary
35:34
pictures. We're gonna go through the build process for pipelines and for Kilroy apps and how it works under the
35:40
hood. So that by the end of this, you should be a pro. That's excellent. I love it. All right,
35:45
I'm gonna end the stream. I hope everybody has a great day. See you guys. Thanks, Brad. ing up.
ROTP: Introduction to Kilroy! Decentralizing AI/Automation/Crypto

Realm of the Possible
4 subscribers

Subscribe

2


Share

Save

Clip

31 views  Streamed live on Oct 17, 2025
Learn how you can be empowered to build anything with a powerful decentralized automation and AI platform. 

We've created the most powerful decentralized AI and crypto automation/integration platform while being completely decentralized. It comes complete with no configuration peer-to-peer/swarming capabilities, the ability to easily build integrations with any other compute, and a Turing complete visual creation tool anyone can use to build functionality in the network while giving agents on Kilroy the ability to build and deploy on their own.
Transcript
Follow along using the transcript.


Show transcript

Realm of the Possible
4 subscribers
Videos
About

All

Computers

For you

18:57
The Infinite Software Crisis – Jake Nations, Netflix
AI Engineer
117K views
•
9 days ago


18:00
The Hottest Thing Anyone Has Ever Said to Me | The Hustler
Kelsey Cook
2.8M views
•
1 year ago


14:19
What Sam Altman Doesn't Want You To Know
More Perfect Union
2.3M views
•
10 days ago
Fundraiser

33:14
Ethernet is DEAD?? Mac Studio is 100x FASTER!!
NetworkChuck
310K views
•
9 days ago

20:16
One Formula That Demystifies 3D Graphics
Tsoding
285K views
•
4 days ago
New

10:02
Can Magnus Carlsen Beat a Noob with 30 Queens?
Esports Spotlight
929K views
•
1 month ago

5:47
Chef Show - SNL
Saturday Night Live
12M views
•
2 years ago

15:30
Ex-Google CEO Warns "AI is Becoming Conscious"
AI Upload
32K views
•
13 days ago

53:44
Building the PERFECT Linux PC with Linus Torvalds
Linus Tech Tips
4.5M views
•
4 weeks ago

21:14
Why I Left Quantum Computing Research
Looking Glass Universe
1M views
•
6 months ago

11:05
Will Ferrell Refuses To Discuss The Bird On His Shoulder | CONAN on TBS
Team Coco
1.4M views
•
3 months ago

15:40
I Quit an AI Startup After 6 Months - Here's What I learned
Brian Jenney
191K views
•
2 weeks ago

24:19
NEW Walking Method 2x Better Than Running?! (Tested)
Jeremy Ethier
621K views
•
8 days ago

20:48
Watch Ukrainian Drones OBLITERATE a Russian Plane
Beyond Military
747K views
•
10 days ago

5:25
New Barista Training - SNL
Saturday Night Live
7.1M views
•
11 months ago

17:00
Downfall of the 7-Hour Coding Tutorial
Boot dev
160K views
•
1 month ago

16:19
How ChatGPT Is Weirdly Turning Into Facebook
Enrico Tartarotti
141K views
•
2 weeks ago

20:43
I Powered My House Using 500 Disposable vapes
Chris Doel
4M views
•
1 month ago

21:49
The Windows 11 Crisis
ColdFusion
1.9M views
•
2 weeks ago

14:10
How AI is Changing Manufacturing
Future in the Making
68K views
•
3 weeks ago


Show more
    
0 notifications total

Skip to search

Skip to main content

Keyboard shortcuts
Close jump menu
Search
new feed updates notifications
Home
My Network
Jobs
3
3 new messages notifications
Messaging
25
25 new notifications
Notifications
Don Hopkins
Me

For Business
Learning
Background Image
Chuck Shotton
Chuck Shotton 
  3rd degree connection3rd
CEO of Kilroy.Tech

Kilroy.Tech

The College of William and Mary
Leesburg, Virginia, United States  Contact info
500+ connections

Message

Follow

More
AboutAbout
I am the CEO of Kilroy.Tech. My team and I are creating the next generation platform for AI and agentic programming, using our low-code/no-code platform, Kilroy. As the first truly decentralized AI platform, you can build AI-powered apps for defi, fintech, personal productivity, games, or whatever else you can imagine. Kilroy is designed to be approachable by non-technical users, but powerful enough to run enterprise-grade applications created by serious software engineers. 

My personal portfolio of skills has been developed from numerous company turn-arounds that required immediate changes to technology required in a dramatically short timeframe. I have also led the engineering effort for companies poised for exit via merger and acquisition. My background includes building large, distributed systems, architectures, and mobility solutions for government, defense, commercial, medical, and financial services.

Specialties: Experienced corporate and technology leader with significant turn-around and M&A successes. Strong technology and management background in building large, distributed systems, architectures, and mobility solutions for government, defense, commercial, medical, and financial applications.I am the CEO of Kilroy.Tech. My team and I are creating the next generation platform for AI and agentic programming, using our low-code/no-code platform, Kilroy. As the first truly decentralized AI platform, you can build AI-powered apps for defi, fintech, personal productivity, games, or whatever else you can imagine. Kilroy is designed to be approachable by non-technical users, but powerful enough to run enterprise-grade applications created by serious software engineers. My personal portfolio of skills has been developed from numerous company turn-arounds that required immediate changes to technology required in a dramatically short timeframe. I have also led the engineering effort for companies poised for exit via merger and acquisition. My background includes building large, distributed systems, architectures, and mobility solutions for government, defense, commercial, medical, and financial services. Specialties: Experienced corporate and technology leader with significant turn-around and M&A successes. Strong technology and management background in building large, distributed systems, architectures, and mobility solutions for government, defense, commercial, medical, and financial applications.…see more
ActivityActivity
781 followers781 followers


Posts

Comments
Loaded 7 Posts posts
View Chuck Shotton’s  graphic link
Chuck ShottonChuck Shotton
   • 3rd+Verified • 3rd+
CEO of Kilroy.TechCEO of Kilroy.Tech
1w •  1 week ago • Visible to anyone on or off LinkedIn

In case this hasn't made it onto rotp.ai yet, here's a direct link to today's podcast. See my previous post for details.
Brad Nickel
Brad NickelBrad Nickel
   • 3rd+Verified • 3rd+
Founder: Kilroy - The usability layer for AI and crypto Advisor/Investor in DeFi. Co-Founder: Material Indicators - Algos, tools, and indicators for smart traders.Founder: Kilroy - The usability layer for AI and crypto Advisor/Investor in DeFi. Co-Founder: Material Indicators - Algos, tools, and indicators for smart traders.
Learn how you can be empowered to build anything with a powerful decentralized automation and AI platform. 

We've created the most powerful decentralized AI and crypto automation/integration platform while being completely decentralized. It comes complete with no configuration peer-to-peer/swarming capabilities, the ability to easily build integrations with any other compute, and a Turing complete visual creation tool anyone can use to build functionality in the network while giving agents on Kilroy the ability to build and deploy on their own.
…more


like
2
1 repost




View Chuck Shotton’s  graphic link
Chuck ShottonChuck Shotton
   • 3rd+Verified • 3rd+
CEO of Kilroy.TechCEO of Kilroy.Tech
1w •  1 week ago • Visible to anyone on or off LinkedIn

There's a new version of the "Realm of the Possible" podcast available now! This week, Brad Nickel and I review the latest news in the AI world, talk about some of the hidden dangers of centralized (cloud) AI use in the professional workplace, and go over a cool new feature for creating AI-driven "mini-apps" in our Kilroy platform, using completely local, desktop AI services.

You can find this episode and all of our earlier ones at https://rotp.ai
…more

Realm of the Possible
rotp.ai
like
4





Show all posts
ExperienceExperience
Managing Partner
Managing Partner
Managing Partner
Kilroy Ventures · Part-timeKilroy Ventures · Part-time
Oct 2023 - Present · 2 yrs 3 mosOct 2023 to Present · 2 yrs 3 mos
Leesburg, Virginia, United States · HybridLeesburg, Virginia, United States · Hybrid
Co-Founder and Managing Partner of a leading-edge investment group that is actively transforming fund, portfolio, and LP management into an automated, distributed ledger infrastructure. We are focused on technology driven companies in the FinTech, DeFi, and consumer-facing services sectors. Working with a pool of accredited investors, we raise and deploy capital for seed round investments in the range of $250k to $1M.Co-Founder and Managing Partner of a leading-edge investment group that is actively transforming fund, portfolio, and LP management into an automated, distributed ledger infrastructure. We are focused on technology driven companies in the FinTech, DeFi, and consumer-facing services sectors. Working with a pool of accredited investors, we raise and deploy capital for seed round investments in the range of $250k to $1M.
Kilroy.Tech logo
Chief Executive Officer
Chief Executive Officer
Kilroy.Tech · Full-timeKilroy.Tech · Full-time
Jan 2022 - Present · 4 yrsJan 2022 to Present · 4 yrs
Leesburg, Virginia, United States · On-siteLeesburg, Virginia, United States · On-site
Kilroy.Tech is the creator of the industry-first AI, DeFi, and TradFi automation platform, Kilroy. Providing fully decentralized compute and AI functionality in a peer-to-peer swarm across end user desktops is our mission.Kilroy.Tech is the creator of the industry-first AI, DeFi, and TradFi automation platform, Kilroy. Providing fully decentralized compute and AI functionality in a peer-to-peer swarm across end user desktops is our mission.
Corporate Leadership, Entrepreneurship and +3 skills
Chief Executive Officer
Chief Executive Officer
Chief Executive Officer
Concluent Systems, LLC · Full-timeConcluent Systems, LLC · Full-time
Mar 2018 - Oct 2025 · 7 yrs 8 mosMar 2018 to Oct 2025 · 7 yrs 8 mos
Founder and senior partner for this high-powered technology practice focused on solving challenging software systems engineering, architecture, and development problems for its clients.

Concluent is leveraging its expertise in blockchain and distributed ledger technologies with its strong base of intellectual property to create products that are changing the landscape of robotic process automation. We are focused on the automation of highly complex business and financial transactions that require trusted and reliable transactions backed by modern DLT platforms.

Past engagements included Chief Architect for Common Operating System software for the DARPA/USAF Joint Unmanned Combat Air System program, providing business development strategy for major defense and aerospace corporations, and architecting large distributed software systems for defense and civilian agencies. Extensive work with the Air Force Research Lab and other DoD agencies has honed Concluent's IP portfolio, creating a best in breed tool set for rapid application development, large system architecture prototyping, and edge-based computing solutions in a peer environment.Founder and senior partner for this high-powered technology practice focused on solving challenging software systems engineering, architecture, and development problems for its clients. Concluent is leveraging its expertise in blockchain and distributed ledger technologies with its strong base of intellectual property to create products that are changing the landscape of robotic process automation. We are focused on the automation of highly complex business and financial transactions that require trusted and reliable transactions backed by modern DLT platforms. Past engagements included Chief Architect for Common Operating System software for the DARPA/USAF Joint Unmanned Combat Air System program, providing business development strategy for major defense and aerospace corporations, and architecting large distributed software systems for defense and civilian agencies. Extensive work with the Air Force Research Lab and other DoD agencies has honed Concluent's IP portfolio, creating a best in breed tool set for rapid application development, large system architecture prototyping, and edge-based computing solutions in a peer environment.
President, Inveniam.io, and CTO of Inveniam Capital Partners
President, Inveniam.io, and CTO of Inveniam Capital Partners
President, Inveniam.io, and CTO of Inveniam Capital Partners
Inveniam Capital PartnersInveniam Capital Partners
Jan 2017 - Dec 2018 · 2 yrsJan 2017 to Dec 2018 · 2 yrs
Leesburg, VALeesburg, VA
I was responsible for leading the technology team that created Inveniam Capital Partner's state of the art investment platform, Inveniam.io. Utilizing blockchain and distributed ledger technologies, we created the first end-to-end platform for automating the creation bespoke investment transactions backed by the DLT. As president and CTO of Inveniam.io, I helped the Inveniam Capital Partners team define strategy, build innovative solutions for online investment banking in the 21st century, and led a team of stellar technology professionals who redefined FinTech as a service.I was responsible for leading the technology team that created Inveniam Capital Partner's state of the art investment platform, Inveniam.io. Utilizing blockchain and distributed ledger technologies, we created the first end-to-end platform for automating the creation bespoke investment transactions backed by the DLT. As president and CTO of Inveniam.io, I helped the Inveniam Capital Partners team define strategy, build innovative solutions for online investment banking in the 21st century, and led a team of stellar technology professionals who redefined FinTech as a service.
DynAgility logo
CTO
CTO
DynAgilityDynAgility
Mar 2012 - Feb 2017 · 5 yrsMar 2012 to Feb 2017 · 5 yrs
Sterling, VASterling, VA
DynAgility served the software product development needs of software and technology companies from startups to global leaders. Focused on delivering commercial software to help customers rapidly leverage the performance benefits of leading-edge technologies with the reliability of proven architectures, I provided technology leadership, managed the architecture and engineering practices and collaborated with the DynAgility management team on business development and corporate strategy. At DynAgility, my team created over 125 commercial grade applications for Fortune 500 businesses, start-ups, and everything in between, ranging from satellite based anti-piracy solutions for shipping to some of the most downloaded apps for weather to embedded systems applications and large scale machine learning solutions.
DynAgility served the software product development needs of software and technology companies from startups to global leaders. Focused on delivering commercial software to help customers rapidly leverage the performance benefits of leading-edge technologies with the reliability of proven architectures, I provided technology leadership, managed the architecture and engineering practices and collaborated with the DynAgility management team on business development and corporate strategy. At DynAgility, my team created over 125 commercial grade applications for Fortune 500 businesses, start-ups, and everything in between, ranging from satellite based anti-piracy solutions for shipping to some of the most downloaded apps for weather to embedded systems applications and large scale machine learning solutions. 
Show all 15 experiences
EducationEducation
William & Mary logo
William & Mary
William & Mary
BS magna cum laude, Computer ScienceBS magna cum laude, Computer Science
Minor in AnthropologyMinor in Anthropology
Nansemond Suffolk Academy
Nansemond Suffolk Academy
Nansemond Suffolk Academy
SkillsSkills
Corporate Leadership
Corporate Leadership
Company logo
Chief Executive Officer at Kilroy.TechChief Executive Officer at Kilroy.Tech
Entrepreneurship
Entrepreneurship
Company logo
Chief Executive Officer at Kilroy.TechChief Executive Officer at Kilroy.Tech
Show all 53 skills
RecommendationsRecommendations
ReceivedReceived
GivenGiven
Ian Andrew Bell
Ian Andrew Bell
Ian Andrew Bell
· 3rdThird degree connection
Product Management, Go-To-Market, Strategy, Technology IntegrationProduct Management, Go-To-Market, Strategy, Technology Integration
July 23, 2014, Ian Andrew worked with Chuck but they were at different companiesJuly 23, 2014, Ian Andrew worked with Chuck but they were at different companies
Chuck created MacHTTP, empowering millions of Pre-OSX users to leverage the web as publishers as early as 1993, right up until Apache shipped with every copy of OSX in 2000. In addition to his contribution in code, Chuck provided leadership and candour to the Mac web developer community, not to mention his wit and humility. 20 years on I still reach out to Chuck periodically to check his perspective on various issues, and have a great laugh.Chuck created MacHTTP, empowering millions of Pre-OSX users to leverage the web as publishers as early as 1993, right up until Apache shipped with every copy of OSX in 2000. In addition to his contribution in code, Chuck provided leadership and candour to the Mac web developer community, not to mention his wit and humility. 20 years on I still reach out to Chuck periodically to check his perspective on various issues, and have a great laugh.
Naomi Pearce
Naomi Pearce
Naomi Pearce
· 3rdThird degree connection
Owner, Pearce CommunicationsOwner, Pearce Communications
May 12, 2006, Chuck was Naomi’s clientMay 12, 2006, Chuck was Naomi’s client
Chuck Shotton is one of the smartest guys I know.... and I know some smart guys. If I had a buck for every time I had the "yeah, but he's right. he's almost always right. it's annoying" conversation ;^)... engineering chops, communication chops, market groking chops, well-earned biz chops, instincts like you woudn't believe. I loved working with Chuck. I'd do it again in a heartbeat.Chuck Shotton is one of the smartest guys I know.... and I know some smart guys. If I had a buck for every time I had the "yeah, but he's right. he's almost always right. it's annoying" conversation ;^)... engineering chops, communication chops, market groking chops, well-earned biz chops, instincts like you woudn't believe. I loved working with Chuck. I'd do it again in a heartbeat.
PatentsPatents
Data structure having internal self-references suitable for immutably representing and verifying data generated over time
Data structure having internal self-references suitable for immutably representing and verifying data generated over time
US 10873457 · Issued Nov 3, 2020US 10873457 · Issued Nov 3, 2020
See patent
Chains of trust for external documents anchored on a blockchain or across multiple blockchain records and external manifests.Chains of trust for external documents anchored on a blockchain or across multiple blockchain records and external manifests.
Computer network virus protection system and method
Computer network virus protection system and method
US 7,913,078 · Issued Mar 22, 2011US 7,913,078 · Issued Mar 22, 2011
Show all 4 patents
Honors & awardsHonors & awards
Apple Cool Tool Award
Apple Cool Tool Award
Issued by Apple, Inc.Issued by Apple, Inc.
Company logo
Associated with Quarterdeck
Associated with Quarterdeck
Product of the Year
Product of the Year
Company logo
Associated with Quarterdeck
Associated with Quarterdeck
WebSTARWebSTAR
Show all 3 honors & awards
InterestsInterests
CompaniesCompanies
GroupsGroups
SchoolsSchools
William & Mary logo
William & Mary
William & Mary
103,602 followers103,602 followers

Follow
StevenDouglas logo
StevenDouglas
StevenDouglas
305,679 followers305,679 followers

Follow
Show all companies

More profiles for youMore profiles for you
Todd Garrity
Todd Garrity
Todd Garrity
· 3rdThird degree connection
Product Leader | Designing Experiences & Driving Products from Concept to LaunchProduct Leader | Designing Experiences & Driving Products from Concept to Launch
Message
Randall Davis
Randall Davis
Randall Davis
· 3rdThird degree connection
Leading Advanced Software Product Roadmap Acceleration: Cloud | Full Stack Apps | Big Data | Analytics | Data Science | AI Leading Advanced Software Product Roadmap Acceleration: Cloud | Full Stack Apps | Big Data | Analytics | Data Science | AI 
Message
Tom Garrity
Tom Garrity
Tom Garrity
· 3rdThird degree connection
Partner at Arc 2 ConsultingPartner at Arc 2 Consulting
Message
Brad Nickel
Brad Nickel
Brad Nickel
· 3rdThird degree connection
Founder: Kilroy - The usability layer for AI and crypto
Advisor/Investor in DeFi. 
Co-Founder: Material Indicators - Algos, tools, and indicators for smart traders.Founder: Kilroy - The usability layer for AI and crypto Advisor/Investor in DeFi. Co-Founder: Material Indicators - Algos, tools, and indicators for smart traders.

Follow
Silas Matteson
Silas Matteson
Silas Matteson
· 3rdThird degree connection
Principal at Matteson Associates LLCPrincipal at Matteson Associates LLC
Message
Show all
Explore Premium profilesExplore Premium profiles
Robert van Straten
Robert van Straten
Robert van Straten
· 3rdThird degree connection
Software Engineer bij Eekels Technology B.V.Software Engineer bij Eekels Technology B.V.
Message
Nathan Bakema
Nathan Bakema
Nathan Bakema
· 3rdThird degree connection
Agile Test EngineerAgile Test Engineer
Message
Mohsen G.
Mohsen G.
Mohsen G.
· 3rdThird degree connection
Computational Mechanics Researcher,
Software Development Engineer at DIANA FEAComputational Mechanics Researcher, Software Development Engineer at DIANA FEA
Message
Jeroen Severein
Jeroen Severein
Jeroen Severein
· 3rdThird degree connection
Team Lead Software Engineering bij Adwise - Your Digital BrainTeam Lead Software Engineering bij Adwise - Your Digital Brain
Message
People you may knowPeople you may know
From Chuck's job titleFrom Chuck's job title

Alan Kay
Alan Kay
Alan Kay
President at Viewpoints Research InstitutePresident at Viewpoints Research Institute

Pending
Neil Mayle
Neil Mayle
Neil Mayle
Entrepreneur and Technology ConsultantEntrepreneur and Technology Consultant

Connect
E. Dean Tribble
E. Dean Tribble
E. Dean Tribble
CEO at AgoricCEO at Agoric

Connect
John Amuedo
John Amuedo
John Amuedo
Founder, Signal InferenceFounder, Signal Inference

Connect
Idit Harel
Idit Harel
Idit Harel
Hey, I’m an entrepreneurial grad student again, in Architecture and Urban Design Hey, I’m an entrepreneurial grad student again, in Architecture and Urban Design 

Connect
Show all
You might likeYou might like
Pages for youPages for you

Partij voor de Dieren logo
Partij voor de Dieren
Partij voor de Dieren
Political OrganizationsPolitical Organizations
41,336 followers41,336 followers

1 connection follows this page

Follow
Ministerie van Justitie en Veiligheid logo
Ministerie van Justitie en Veiligheid
Ministerie van Justitie en Veiligheid
Government AdministrationGovernment Administration
233,884 followers233,884 followers

1 connection follows this page

Follow
Show all

Don HopkinsStatus is online
MessagingYou are on the messaging overlay. Press enter to open the list of conversations.
3

Compose message
You are on the messaging overlay. Press enter to open the list of conversations.
0 notifications total

Skip to search

Skip to main content

Keyboard shortcuts
Close jump menu
Search
new feed updates notifications
Home
My Network
Jobs
3
3 new messages notifications
Messaging
25
25 new notifications
Notifications
Don Hopkins
Me

For Business
Learning
Chuck Shotton
Chuck Shotton
CEO of Kilroy.Tech

Message

Follow
All activity

Posts

Comments

Reactions
Loaded 8 Posts posts
Feed post number 1
View Chuck Shotton’s  graphic link
Chuck ShottonChuck Shotton
   • 3rd+Verified • 3rd+
CEO of Kilroy.TechCEO of Kilroy.Tech
1w •  1 week ago • Visible to anyone on or off LinkedIn

Follow

In case this hasn't made it onto rotp.ai yet, here's a direct link to today's podcast. See my previous post for details.
Brad Nickel
Brad NickelBrad Nickel
   • 3rd+Verified • 3rd+
Founder: Kilroy - The usability layer for AI and crypto Advisor/Investor in DeFi. Co-Founder: Material Indicators - Algos, tools, and indicators for smart traders.Founder: Kilroy - The usability layer for AI and crypto Advisor/Investor in DeFi. Co-Founder: Material Indicators - Algos, tools, and indicators for smart traders.
1w •  1 week ago • Visible to anyone on or off LinkedIn

Follow
Learn how you can be empowered to build anything with a powerful decentralized automation and AI platform. 

We've created the most powerful decentralized AI and crypto automation/integration platform while being completely decentralized. It comes complete with no configuration peer-to-peer/swarming capabilities, the ability to easily build integrations with any other compute, and a Turing complete visual creation tool anyone can use to build functionality in the network while giving agents on Kilroy the ability to build and deploy on their own.
…more
Previously live

Play

Back to start of video

Skip back 10 seconds

Skip ahead 10 seconds
Current time 0:00/Duration 33:50
1x

Playback speed

Unmute

Turn fullscreen on
Event ended

ROTP: Building in Kilroy Part 5
 Linkedin Live

like
2
1 repost




Feed post number 2
View Chuck Shotton’s  graphic link
Chuck ShottonChuck Shotton
   • 3rd+Verified • 3rd+
CEO of Kilroy.TechCEO of Kilroy.Tech
1w •  1 week ago • Visible to anyone on or off LinkedIn

Follow

There's a new version of the "Realm of the Possible" podcast available now! This week, Brad Nickel and I review the latest news in the AI world, talk about some of the hidden dangers of centralized (cloud) AI use in the professional workplace, and go over a cool new feature for creating AI-driven "mini-apps" in our Kilroy platform, using completely local, desktop AI services.

You can find this episode and all of our earlier ones at https://rotp.ai
…more

Realm of the Possible
rotp.ai
like
4




Feed post number 3
View Chuck Shotton’s  graphic link
Chuck ShottonChuck Shotton
   • 3rd+Verified • 3rd+
CEO of Kilroy.TechCEO of Kilroy.Tech
1mo •  1 month ago • Visible to anyone on or off LinkedIn

Follow

We've been working on our software platform, Kilroy, for quite some time now and it's ready for the first brave souls to give it a try. Kilroy is a fully decentralized, desktop platform for building everything from complex AI-driven pipelines to fintech and defi apps, to games and personal productivity tools -- using an intuitive drag-and-drop interface to build or download and run secure, portable apps.

We've been producing a video podcast about the platform for the past month or so. If you'd like to learn more about what we are up to, check out https://www.rotp.ai/ and start with episode 1. If you'd like to be one of the first to start using and building on Kilroy, drop me a note.
…more

Realm of the Possible
rotp.ai
likelovecelebrate
11
1 comment




Feed post number 4
Chuck’s profile photo
Chuck Shotton reposted this

Brad Nickel
Brad NickelBrad Nickel
   • 3rd+Verified • 3rd+
Founder: Kilroy - The usability layer for AI and crypto Advisor/Investor in DeFi. Co-Founder: Material Indicators - Algos, tools, and indicators for smart traders.Founder: Kilroy - The usability layer for AI and crypto Advisor/Investor in DeFi. Co-Founder: Material Indicators - Algos, tools, and indicators for smart traders.
1mo • Edited •  1 month ago • Edited • Visible to anyone on or off LinkedIn

Follow
How to easily build your own trading automation bot or just about anything you can imagine for your life and work on Kilroy.Tech. With Chuck Shotton and Brad Nickel.

11:30 AM eastern 👇

Kilroy is the only truly decentralized agentic automation and integration platform that anyone can use to deploy agents in swarms via peer-to-peer. 

Don't miss this one! Make sure to attend it today.
…more
Brad Nickel
Brad NickelBrad Nickel
   • 3rd+Verified • 3rd+
Founder: Kilroy - The usability layer for AI and crypto Advisor/Investor in DeFi. Co-Founder: Material Indicators - Algos, tools, and indicators for smart traders.Founder: Kilroy - The usability layer for AI and crypto Advisor/Investor in DeFi. Co-Founder: Material Indicators - Algos, tools, and indicators for smart traders.
1mo •  1 month ago • Visible to anyone on or off LinkedIn

Follow
This week we're going to continue walking you through building a trading app in Kilroy. 

Learn how you can be empowered to build anything with a powerful decentralized automation and AI platform. 

We've created the most powerful decentralized AI and crypto automation/integration platform while being completely decentralized. It comes complete with no configuration peer-to-peer/swarming capabilities, the ability to easily build integrations with any other compute, and a Turing complete visual creation tool anyone can use to build functionality in the network while giving agents on Kilroy the ability to build and deploy on their own.
…more
Event cancelled

Event cancelled

ROTP: Building an automated trading app from scratch in Kilroy Part 2
 Linkedin Live

like
6
1 repost




Feed post number 5
View Chuck Shotton’s  graphic link
Chuck ShottonChuck Shotton
   • 3rd+Verified • 3rd+
CEO of Kilroy.TechCEO of Kilroy.Tech
2mo •  2 months ago • Visible to anyone on or off LinkedIn

Follow

It’s been a while since I started my role at Kilroy.Tech as a Chief Executive Officer, but I wanted to share this update with everyone.


Starting a New Position
likecelebrate
34
12 comments




Feed post number 6
View Chuck Shotton’s  graphic link
Chuck ShottonChuck Shotton
   • 3rd+Verified • 3rd+
CEO of Kilroy.TechCEO of Kilroy.Tech
3yr • Edited •  3 years ago • Edited • Visible to anyone on or off LinkedIn

Follow

Bill Fitzgerald and I have been working on this application for the past year and it's ready for you to give it a shot! https://myvitalfew.com
View Bill Fitzgerald’s  graphic link
Bill FitzgeraldBill Fitzgerald
   • 3rd+Premium • 3rd+
Executive Retained Search and Recruiting and AuthorExecutive Retained Search and Recruiting and Author
3yr •  3 years ago • Visible to anyone on or off LinkedIn

Follow
My Vital Few redefines what we mean by networking and works with Linkedin to provide added functionality. hashtag#linkedin hashtag#networking hashtag#jobsearch
…more

My Vital Few - Networking Redefined
Bill Fitzgerald
like
9




Feed post number 7
View Chuck Shotton’s  graphic link
Chuck ShottonChuck Shotton
   • 3rd+Verified • 3rd+
CEO of Kilroy.TechCEO of Kilroy.Tech
7yr •  7 years ago • Visible to anyone on or off LinkedIn

Follow

View Patrick O'Meara’s  graphic link
Patrick O'MearaPatrick O'Meara
   • 3rd+Premium • 3rd+
Chairman & CEO, Inveniam Capital PartnersChairman & CEO, Inveniam Capital Partners
7yr • Edited •  7 years ago • Edited • Visible to anyone on or off LinkedIn

Follow
https://lnkd.in/ejth86x The World bank will be fourth, by my count, https://lnkd.in/egGqNip, but certainly the best PR. hashtag#blockchain hashtag#capitalmarkets hashtag#tokenization hashtag#tokenizeddebt hashtag#debtmarkets hashtag#inveniam Lewis Cohen Jared Klee Logan S. Brian Anderson Jerry Cuomo Keith Bear Benjamin W. chris howley
…more

The World Bank is preparing for the world's first blockchain bond
cnbc.com
like
3
1 repost




Feed post number 8
View Chuck Shotton’s  graphic link
Chuck ShottonChuck Shotton
   • 3rd+Verified • 3rd+
CEO of Kilroy.TechCEO of Kilroy.Tech
7yr •  7 years ago • Visible to anyone on or off LinkedIn

Follow


InveniamInveniam
6,006 followers6,006 followers
7yr •  7 years ago • Visible to anyone on or off LinkedIn

Follow
https://lnkd.in/ewvJmdB
Financial advisors are missing one key technology disruption

(CNBC) While bitcoin dominates headlines, financial advisors would be wise to pay attention to the underlying blockchain technology and its ability to cut out the middleman in financial transactions and cause dramatic shifts in the way the financial industry conducts business.
…more

Advisors not up to speed on blockchain technology will get left behind: Experts
tinyurl.com
like
2
1 repost





People you may knowPeople you may know
From Chuck's job titleFrom Chuck's job title

Helmig Heerink
Helmig Heerink
Helmig Heerink
Managing Director at Quades & Prequre ★ Hiring & training recruiters! ★Managing Director at Quades & Prequre ★ Hiring & training recruiters! ★

Connect
Alan Kay
Alan Kay
Alan Kay
President at Viewpoints Research InstitutePresident at Viewpoints Research Institute

Pending
E. Dean Tribble
E. Dean Tribble
E. Dean Tribble
CEO at AgoricCEO at Agoric

Connect
Idit Harel
Idit Harel
Idit Harel
Hey, I’m an entrepreneurial grad student again, in Architecture and Urban Design Hey, I’m an entrepreneurial grad student again, in Architecture and Urban Design 

Connect
Neil Mayle
Neil Mayle
Neil Mayle
Entrepreneur and Technology ConsultantEntrepreneur and Technology Consultant

Connect

Show more

About
Accessibility
Help Center

Privacy & Terms
Ad Choices
Advertising

Business Services
Get the LinkedIn app
More
 LinkedIn Corporation © 2025
Don HopkinsStatus is online
MessagingYou are on the messaging overlay. Press enter to open the list of conversations.
3

Compose message
You are on the messaging overlay. Press enter to open the list of conversations.

 Kilroy - The decentralized integration, automation & low-code dev environment for DeFi, AI, & Web2
Home
 
About The Kilroy Team
Search …
About Us
Our team has over 50 years experience in tech,
marketing, strategy, operations, startups, and development.

Our Founders
The Founders of Kilroy have dedicated their professional lives to building innovative platforms that change how we interact as humans with technology. From building the first MacOS web server eventually acquired by Apple to building one of the first SaaS companies for small businesses, they are always ahead of the curve in seeing where the tech can take us and building to what it can be. 

Combined they have decades of experience building teams, marketing, partnerships, and operations in technology startups and understand how best to position the companies they run for maximum growth.  


Chuck Shotton
30 ys tech VC & start-ups
DARPA, NASA, academia, Fortune 500 
Venture partner – Safeguard Scientifics funds
Has raised and funded millions in tech startups
Director, Academic Computing Dept, Univ of Texas Health Science Center
Built the first MacOS web server that enabled thousands of the first websites on the Internet.
Built Kilroy to facilitate over $200 million in bond products in Europe on Ethereum

Brad Nickel
26 years in tech product management, marketing, & business development
In crypto / DeFi since 2015 investing & advising
Host of popular Mission: DeFi & DeFi Lunch podcasts
40,000 plus followers on Twitter and LinkedIn
Founder of successful algo trading tools company
15 yrs experience advising startups
Conceived of and built one of the first web-based SaaS platforms
Conceived of and built with his team one of the first tabbed browsers
Conceived of, built, and signed on the largest printer and copier manufacturers to the network printing standard
Contact Us
 

[forminator_form id=”211″]

Archives
June 2024
Categories
Uncategorized
   Designed by Elegant Themes | Powered by WordPress
   To view keyboard shortcuts, press question mark
View keyboard shortcuts

Chuck Shotton
2,008 posts

See new posts
Chuck Shotton
@cshotton
Technology magpie, MacHTTP, UAVs, and solder
Leesburg, VAshotton.com
Joined October 2007
150 Following
226 Followers
Not followed by anyone you’re following
Posts
Replies
Media
Chuck Shotton’s posts
Chuck Shotton reposted
Brad
@b05crypto
·
Nov 7
ROTP: How to easily build agentic swarms in Kilroy Pt. 3 https://x.com/i/broadcasts/1MYxNlYlnnpGw
Chuck Shotton reposted
Brad
@b05crypto
·
Oct 31
ROTP: How to easily build agentic swarms in Kilroy Pt. 2

Brad
@b05crypto
ROTP: How to easily build agentic swarms in Kilroy Pt. 2
Chuck Shotton reposted
Brad
@b05crypto
·
Sep 11, 2024
Onboard the next 100 million in DeFi - Live Kilroy Demo https://x.com/i/broadcasts/1ZkKzRlMMygKv
Chuck Shotton reposted
Brad
@b05crypto
·
Jun 19, 2024
Realm of the Possible 6/19/24 https://x.com/i/broadcasts/1kvJpvveLDwKE
Chuck Shotton reposted
Brad
@b05crypto
·
May 21, 2024
ROTP - 5/21/2024 https://x.com/i/broadcasts/1RDGllQzZdkGL
Who to follow

John McCrea
@johnmccrea
Entrepreneur since 1994. Doing brand-safe, non-hallucinatory Conversational AI since 2018. Trying to save democracy along the way.

sontek
@sontek
Python, Golang, Kubernetes, and DevOps!

Evin Grano
@TechYeoman
Technologist & LP; Product & Tech, #Blockchain & #FinTech, #MachineLearning & #AI. Thoughts 💭 are my own. RT 🚫 endorsement
Show more
Chuck Shotton
@cshotton
·
Jul 18, 2023
Wow! I had a legit #MacHTTP support question in email today. (That's about 22 years after the last official version was released. I'm impressed!) http://machttp.org if your curious...
machttp.org
MacHTTP.org
MacHTTP.org is the open source organization supporting the MacHTTP family of web servers.
Chuck Shotton reposted
Brad
@b05crypto
·
Jun 8, 2023
Content Warning! This event contains language not suitable for those under 3, Evangelical Christians, Gary Gensler, No Coiners, Liz Warren, Frauds pretending to be Satoshi Nakamoto  or Jim Cramer!

To be read in booming ringside announcer voice.

BTC vs DeFi - The Showdown of the
Show more
You might like
John McCrea
@johnmccrea
sontek
@sontek
Evin Grano
@TechYeoman
Show more
Trending now
What’s happening
Trending
Star Trek
3,280 posts
Trending
Lucy Connolly
21.2K posts
Trending in Netherlands
Privacy
181K posts
Trending in Netherlands
Europeaan
Show more
Terms of Service
 |
Privacy Policy
 |
Cookie Policy
 |
Accessibility
 |
Ads info
 |

More
© 2025 X Corp.
Realm of the Possible Realm of the Possible
Home
Leave a Review
Episodes 
Sponsors
Realm of the Possible
Latest Episode
ROTP: Build an automated trading pipeline pt. 1
This week we're going to start walking you through building a trading app in Kilroy. Learn how you can be empowered to build anything with a powerful decentralized automation and AI platform. We've created the most powerful d...


Featured Episodes
ROTP: Build an automated trading pipeline pt. 1
Nov. 15, 2025
Recent Episodes
View all 
ROTP: Build an automated trading pipeline pt. 1
Nov. 15, 2025
ROTP: Build an automated trading pipeline pt. 1
This week we're going to start walking you through building a trading app in Kilroy. Learn how you can be empowered to build anything with a powerful decentralized automation and AI platform. We've created the most powerful d...

Listen to the Episode
How to easily build agentic swarms in Kilroy Part 3
Nov. 10, 2025
How to easily build agentic swarms in Kilroy Part 3
Last week we dove a little deeper into the mechanics of building agentic pipelines with Kilroy and this week we're going to get into integrations with Telegram and Mac Shortcuts. Learn how you can be empowered to build anythi...

Listen to the Episode
How to easily build agentic swarms in Kilroy Part 2
Nov. 10, 2025
How to easily build agentic swarms in Kilroy Part 2
Last week we covered the basics of building AI swarms. This week we get more detailed and functional and give an example of building swarms that can make you more productive with your team. Learn how you can be empowered to b...

Listen to the Episode
How to easily build agentic swarms in Kilroy Part 1
Nov. 10, 2025
How to easily build agentic swarms in Kilroy Part 1
Last week we did the high level overview! Today it's time to get into specifcally how to build easily in Kilroy. Learn how you can be empowered to build anything with a powerful decentralized automation and AI platform. We've...

Listen to the Episode
Introduction to Kilroy - Truly Decentralized Agentic AI that integrates anything
Nov. 10, 2025
Introduction to Kilroy - Truly Decentralized Agentic AI that integrat…
Learn how you can be empowered to build anything with a powerful decentralized automation and AI platform. We've created the most powerful decentralized AI and crypto automation/integration platform while being completely dec...

Listen to the Episode
Trailer for Realm Of The Possible Show
Dec. 29, 2024
Trailer for Realm Of The Possible Show
Coming soon! the Realm of The Possible show with Brad Nickel and Chuck Shotton, covering open source AI, AI innovations, AI agents, AI development, Decentralized Finance, cryptocurrencies, and much much more. This is your go ...

Listen to the Episode
Join the Email List
Enter your first name...
Enter your email here...
By signing up, you agree to receive email from this podcast.
Building in Kilroy Episodes
ROTP: Build an automated trading pipeline pt. 1
How to easily build agentic swarms in Kilroy Part 3
See all →
Kilroy Episodes
How to easily build agentic swarms in Kilroy Part 2
How to easily build agentic swarms in Kilroy Part 1
Pipelines Episodes
How to easily build agentic swarms in Kilroy Part 3
How to easily build agentic swarms in Kilroy Part 2
Agents Episodes
How to easily build agentic swarms in Kilroy Part 2
Agentic AI Episodes
How to easily build agentic swarms in Kilroy Part 2
Swarms Episodes
How to easily build agentic swarms in Kilroy Part 2
trading Episodes
ROTP: Build an automated trading pipeline pt. 1
agentic trading Episodes
ROTP: Build an automated trading pipeline pt. 1
Browse episodes by category
Building in Kilroy 5
Kilroy 3
Pipelines 3
Agents 1
Agentic AI 1
Swarms 1
trading 1
agentic trading 1
ai trading 1
Realm of the Possible
Brad Nickel and Chuck Shotton of Kilroy cover the latest in AI, DeFi, and tech with a focus on open source and decentralized technologies.

Contact
Leave a Review
Episodes
Sponsors
© 2025 Realm of the Possible Podcast Website by Podpage

